{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82285a92",
   "metadata": {},
   "source": [
    "# Explainability-Aware Active Learning for Recommender Systems\n",
    "\n",
    "**Authors and Contact Information:**\n",
    "\n",
    "** Anonymous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cfbf1-e067-4866-9329-6bf224e7d055",
   "metadata": {},
   "source": [
    "# Adding Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d210430-af10-4148-a12c-185bb69322c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard & third-party imports ==========================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import shutil\n",
    "import logging\n",
    "import tempfile\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# === Logging setup ============================================================\n",
    "# Log to both console and a file; concise, timestamped format.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"run_logs.log\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Keep noisy Numba/CUDA internals quiet unless explicitly needed.\n",
    "for name in (\n",
    "    \"numba\",\n",
    "    \"numba.cuda\",\n",
    "    \"numba.cuda.cudadrv\",\n",
    "    \"numba.cuda.cudadrv.driver\",\n",
    "    \"numba.cuda.cudadrv.memory\",\n",
    "):\n",
    "    _lg = logging.getLogger(name)\n",
    "    _lg.setLevel(logging.WARNING)\n",
    "    _lg.propagate = False\n",
    "\n",
    "\n",
    "# === CUDA detection ===========================================================\n",
    "# Try importing Numba's CUDA; fall back to CPU-only if unavailable.\n",
    "try:\n",
    "    from numba import cuda\n",
    "    USE_CUDA = cuda.is_available()\n",
    "except Exception:\n",
    "    USE_CUDA = False\n",
    "\n",
    "logger.info(\n",
    "    \"[GPU] CUDA detected — GPU path will be used where available.\"\n",
    "    if USE_CUDA else\n",
    "    \"[CPU] No CUDA detected — running CPU-only path.\"\n",
    ")\n",
    "\n",
    "# === Hardware info (robust) ===================================================\n",
    "try:\n",
    "    import platform\n",
    "    np_ver = np.__version__\n",
    "    nb_ver = numba.__version__\n",
    "\n",
    "    if USE_CUDA:\n",
    "        # Device identity\n",
    "        dev = cuda.get_current_device()\n",
    "        name = getattr(dev, \"name\", \"Unknown GPU\")\n",
    "        cc   = getattr(dev, \"compute_capability\", (None, None))\n",
    "        try:\n",
    "            driver_ver  = \".\".join(map(str, cuda.runtime.get_driver_version()))\n",
    "        except Exception:\n",
    "            driver_ver = \"unknown\"\n",
    "        try:\n",
    "            runtime_ver = \".\".join(map(str, cuda.runtime.get_version()))\n",
    "        except Exception:\n",
    "            runtime_ver = \"unknown\"\n",
    "\n",
    "        # Memory via context (portable across Numba versions)\n",
    "        try:\n",
    "            free_b, total_b = cuda.current_context().get_memory_info()\n",
    "            total_gib = total_b / (1 << 30)\n",
    "            free_gib  = free_b  / (1 << 30)\n",
    "            mem_str   = f\"VRAM total={total_gib:.2f} GiB, free={free_gib:.2f} GiB\"\n",
    "        except Exception:\n",
    "            mem_str = \"VRAM: (unavailable)\"\n",
    "\n",
    "        logger.info(\n",
    "            \"[GPU] Device=%s | CC=%s.%s | %s | Driver=%s | Runtime=%s | NumPy=%s | Numba=%s\",\n",
    "            name, *(cc if isinstance(cc, tuple) else (cc, \"\")), mem_str, driver_ver, runtime_ver, np_ver, nb_ver\n",
    "        )\n",
    "    else:\n",
    "        # CPU identity (best-effort across OSes)\n",
    "        cpu_name = (platform.processor()\n",
    "                    or getattr(platform.uname(), \"processor\", \"\")\n",
    "                    or getattr(platform.uname(), \"machine\", \"\")\n",
    "                    or \"unknown CPU\")\n",
    "        logger.info(\n",
    "            \"[CPU] Processor=%s | Cores=%s | OS=%s %s | Python=%s | NumPy=%s | Numba=%s\",\n",
    "            cpu_name, os.cpu_count(),\n",
    "            platform.system(), platform.release(),\n",
    "            platform.python_version(), np_ver, nb_ver\n",
    "        )\n",
    "except Exception as e:\n",
    "    logger.warning(\"Could not query hardware info: %s\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365a25e",
   "metadata": {},
   "source": [
    "# Global Hyperparameters for Explainable Active Learning (ExAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950692a-be41-434f-b209-688656c463bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Active Learning settings\n",
    "# =========================\n",
    "num_iter   = 10   # AL iterations\n",
    "SWITCH     = 5    # EXAL Max→Min switch iteration (inclusive logic handled in code)\n",
    "\n",
    "# =========================\n",
    "# Optimization (SGD)\n",
    "# =========================\n",
    "ALPHA_INIT    = 0.01   # LR for initial EMF pretrain\n",
    "ALPHA_RETRAIN = 0.001  # LR for per-iteration online updates\n",
    "\n",
    "# =========================\n",
    "# Explainability (λ, W)\n",
    "# =========================\n",
    "# Decouple training vs selection effects:\n",
    "# - LAMBDA_TRAIN: applies W in EMF training\n",
    "# - LAMBDA_SELECT: applies W in EXAL selection (Min/Max/Min–Max)\n",
    "# This enables the 2×2: (train λ ∈ {0,>0}) × (select λ ∈ {0,>0})\n",
    "LAMBDA_TRAIN  = 0.0  # λ used in EMF training (0 , 0.005  are good candidates  for study)\n",
    "LAMBDA_SELECT = 0.5    # λ used in EXAL selection\n",
    "\n",
    "# W construction\n",
    "theta   = 0.0   # threshold on W_uj (0 → keep all)\n",
    "NEIGHBOR= 20    # k-NN used to compute W\n",
    "\n",
    "# =========================\n",
    "# Training schedule\n",
    "# =========================\n",
    "INIT_STEPS  = 1000  # EMF pretrain epochs before AL\n",
    "ONLINE_STEP = 5     # SGD steps/user per AL iteration\n",
    "BETA        = 0.15  # L2 weight (β)\n",
    "\n",
    "# =========================\n",
    "# Model size\n",
    "# =========================\n",
    "K = 10  # latent dimension\n",
    "\n",
    "# =========================\n",
    "# Evaluation\n",
    "# =========================\n",
    "TopN = 10  # cutoff for MAP/NDCG/xP/xR\n",
    "\n",
    "# =========================\n",
    "# CUDA safety\n",
    "# =========================\n",
    "MAX_K = 128  # must be ≥ K (CUDA kernel local buffer)\n",
    "if USE_CUDA and K > MAX_K:\n",
    "    raise ValueError(f\"K={K} exceeds MAX_K={MAX_K}; increase MAX_K or lower K.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6347acb-60a1-48ab-8ba6-d544f6068ca3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fedf8-4d2e-4395-a512-bbec420d6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_and_extract(\n",
    "    url: str,\n",
    "    dest_dir: str,\n",
    "    expect_files: Iterable[str] = (),\n",
    "    timeout: Tuple[float, float] = (15.0, 60.0),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Stream-download a ZIP file from `url`, verify HTTP status, extract safely\n",
    "    into the current working directory (preserving the archive's structure),\n",
    "    and assert that `dest_dir/expect_files` exist afterwards.\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    with requests.get(url, stream=True, timeout=timeout) as resp:\n",
    "        resp.raise_for_status()\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp:\n",
    "            for chunk in resp.iter_content(chunk_size=1 << 20):  # 1 MiB\n",
    "                if chunk:\n",
    "                    tmp.write(chunk)\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(tmp_path) as zf:\n",
    "            # Safe extraction rooted at CWD, with Zip-Slip protection.\n",
    "            cwd_abs = os.path.abspath(\".\")\n",
    "            for member in zf.infolist():\n",
    "                # Normalize and ensure the path stays under CWD\n",
    "                out_path = os.path.abspath(os.path.join(cwd_abs, member.filename))\n",
    "                if not out_path.startswith(cwd_abs + os.sep) and out_path != cwd_abs:\n",
    "                    raise RuntimeError(f\"Unsafe zip path: {member.filename}\")\n",
    "                if member.is_dir():\n",
    "                    os.makedirs(out_path, exist_ok=True)\n",
    "                else:\n",
    "                    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "                    with zf.open(member) as src, open(out_path, \"wb\") as dst:\n",
    "                        shutil.copyfileobj(src, dst)\n",
    "    except zipfile.BadZipFile as e:\n",
    "        raise RuntimeError(f\"Corrupt zip from {url}\") from e\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    # Verify the expected files are present under dest_dir\n",
    "    for relpath in expect_files:\n",
    "        full = os.path.join(dest_dir, relpath)\n",
    "        if not os.path.exists(full):\n",
    "            raise RuntimeError(f\"Expected file not found after extract: {full}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_movielens(dataset: str = \"100k\") -> Tuple[pd.DataFrame, pd.DataFrame | None]:\n",
    "    \"\"\"\n",
    "    Load MovieLens (100k or 1M) into a dense user×item ratings DataFrame (zeros = unrated).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_M : DataFrame [n_users × n_items]\n",
    "        Ratings matrix with unrated cells filled with 0. dtype=float32 on GPU, float64 on CPU.\n",
    "    movies : DataFrame | None\n",
    "        For 100k: (movieID, movie name, genre) with one multi-genre string per row.\n",
    "        For 1M: None (no item metadata packaged like 100k’s u.item).\n",
    "    \"\"\"\n",
    "    DT = np.float32 if USE_CUDA else np.float64\n",
    "\n",
    "    if dataset == \"100k\":\n",
    "        if not os.path.exists(\"ml-100k\"):\n",
    "            _download_and_extract(\n",
    "                url=\"https://files.grouplens.org/datasets/movielens/ml-100k.zip\",\n",
    "                dest_dir=\"ml-100k\",\n",
    "                expect_files=(\"u.data\", \"u.item\"),\n",
    "                timeout=(15, 60),\n",
    "            )\n",
    "\n",
    "        # Ratings (tab-separated): UserID, movieID, Rating, Timestamp\n",
    "        data = pd.read_csv(\n",
    "            \"ml-100k/u.data\",\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"UserID\", \"movieID\", \"Rating\", \"Timestamp\"],\n",
    "            dtype={\"UserID\": np.int32, \"movieID\": np.int32, \"Rating\": DT, \"Timestamp\": np.int64},\n",
    "        )\n",
    "        data_M = (\n",
    "            data.pivot(index=\"UserID\", columns=\"movieID\", values=\"Rating\")\n",
    "                .fillna(0.0)\n",
    "                .astype(DT, copy=False)\n",
    "        )\n",
    "\n",
    "        # Item metadata: build a concise genre string per movie\n",
    "        genre_cols = [\n",
    "            \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "            \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "            \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\",\n",
    "        ]\n",
    "        movies = pd.read_csv(\n",
    "            \"ml-100k/u.item\",\n",
    "            sep=\"|\",\n",
    "            header=None,\n",
    "            encoding=\"ISO-8859-1\",\n",
    "            usecols=[0, 1, *range(5, 24)],\n",
    "            names=[\"movieID\", \"movie name\"] + genre_cols,\n",
    "        )\n",
    "        movies[\"genre\"] = movies[genre_cols].dot(pd.Index(genre_cols) + \",\").str.rstrip(\",\")\n",
    "        movies = movies[[\"movieID\", \"movie name\", \"genre\"]]\n",
    "\n",
    "    elif dataset == \"1m\":\n",
    "        if not os.path.exists(\"ml-1m\"):\n",
    "            _download_and_extract(\n",
    "                url=\"https://files.grouplens.org/datasets/movielens/ml-1m.zip\",\n",
    "                dest_dir=\"ml-1m\",\n",
    "                expect_files=(\"ratings.dat\",),\n",
    "                timeout=(15, 60),\n",
    "            )\n",
    "\n",
    "        # Ratings ('::'-separated): UserID::movieID::Rating::Timestamp\n",
    "        data = pd.read_csv(\n",
    "            \"ml-1m/ratings.dat\",\n",
    "            sep=\"::\",\n",
    "            engine=\"python\",   # needed for '::' separator\n",
    "            header=None,\n",
    "            names=[\"UserID\", \"movieID\", \"Rating\", \"Timestamp\"],\n",
    "            dtype={\"UserID\": np.int32, \"movieID\": np.int32, \"Rating\": DT, \"Timestamp\": np.int64},\n",
    "        )\n",
    "        data_M = (\n",
    "            data.pivot(index=\"UserID\", columns=\"movieID\", values=\"Rating\")\n",
    "                .fillna(0.0)\n",
    "                .astype(DT, copy=False)\n",
    "        )\n",
    "        movies = None  # 1M doesn’t ship an easy-to-join u.item equivalent\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset. Use '100k' or '1m'.\")\n",
    "\n",
    "    # Reindex to consecutive integers (0..U-1, 0..I-1)\n",
    "    data_M = data_M.reset_index(drop=True)\n",
    "    data_M.columns = range(data_M.shape[1])\n",
    "\n",
    "    logger.info(\n",
    "        f\"[Data] Loaded MovieLens-{dataset}: {data_M.shape[0]} users × {data_M.shape[1]} items | \"\n",
    "        f\"dtype={data_M.values.dtype} (GPU={USE_CUDA})\"\n",
    "    )\n",
    "    if movies is not None:\n",
    "        logger.info(f\"[Data] Movies metadata rows: {len(movies)}\")\n",
    "\n",
    "    return data_M, movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13ceca-8566-4784-b131-b07ab6c9e511",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382f4c7-b244-4547-9733-50f443a9cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(\n",
    "    data_M: pd.DataFrame,\n",
    "    num_test_users: int | None = None,\n",
    "    num_train_ratings: int = 3,\n",
    "    num_test_ratings: int = 20,\n",
    "    min_pool_size: int = 10,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create cold-start splits for AL:\n",
    "      - train: tiny warm-start for test users; full data for non-test users\n",
    "      - test:  held-out items for test users\n",
    "      - pool:  remaining rated items for test users (candidates)\n",
    "    Returns float32 on GPU, float64 on CPU; arrays are C-contiguous.\n",
    "    \"\"\"\n",
    "    X = data_M.values\n",
    "    U, I = X.shape\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # --- dtypes: float32 on GPU, else keep/upgrade to float64 ---\n",
    "    if np.issubdtype(X.dtype, np.floating):\n",
    "        out_dtype = X.dtype\n",
    "    else:\n",
    "        out_dtype = np.float32 if USE_CUDA else np.float64\n",
    "\n",
    "    # --- Sanity checks ---\n",
    "    if any(v < 0 for v in (num_train_ratings, num_test_ratings, min_pool_size)):\n",
    "        raise ValueError(\"Split counts must be non-negative.\")\n",
    "    req = num_train_ratings + num_test_ratings + min_pool_size\n",
    "    if req == 0:\n",
    "        logger.warning(\"Split sizes sum to 0; eligible users get empty pool.\")\n",
    "\n",
    "    # --- Eligible users (enough ratings for train+test+pool) ---\n",
    "    user_nz = np.count_nonzero(X, axis=1)  # ratings per user\n",
    "    candidates = np.where(user_nz >= req)[0]\n",
    "    n_cand = candidates.size\n",
    "\n",
    "    # #test users (default: 10% of eligible, at least 1)\n",
    "    if num_test_users is None:\n",
    "        num_test_users = max(1, int(0.1 * n_cand))\n",
    "    num_test_users = min(num_test_users, n_cand)\n",
    "\n",
    "    # Pick test-user cohort (rng-stable)\n",
    "    rng.shuffle(candidates)\n",
    "    test_users = np.sort(candidates[:num_test_users])  # sort for reproducibility\n",
    "    test_set = set(test_users.tolist())\n",
    "\n",
    "    logger.info(\"=== SPLIT ===\")\n",
    "    logger.info(f\"Dataset: {U} users × {I} items | dtype={out_dtype} | GPU={USE_CUDA}\")\n",
    "    logger.info(f\"Per test user -> train={num_train_ratings}, test={num_test_ratings}, min_pool={min_pool_size}\")\n",
    "    logger.info(f\"Eligible users (≥{req} ratings): {n_cand}\")\n",
    "    logger.info(f\"Selected test users: {len(test_users)}\")\n",
    "\n",
    "    # --- Allocate outputs (C-contiguous) ---\n",
    "    train = np.zeros((U, I), dtype=out_dtype)\n",
    "    test  = np.zeros((U, I), dtype=out_dtype)\n",
    "    pool  = np.zeros((U, I), dtype=out_dtype)\n",
    "\n",
    "    split_stats = {\"train\": 0, \"test\": 0, \"pool\": 0}\n",
    "\n",
    "    # --- Per-user assignment ---\n",
    "    for u in range(U):\n",
    "        items = np.flatnonzero(X[u])  # rated item ids\n",
    "        n_items = items.size\n",
    "\n",
    "        if u in test_set and n_items >= req:\n",
    "            # Choose items for cold-start train+test\n",
    "            n_select = num_train_ratings + num_test_ratings\n",
    "            if n_select > n_items:                 # defensive clip\n",
    "                n_select = n_items\n",
    "                logger.debug(f\"User {u}: n_select clipped to {n_select} (had {n_items}).\")\n",
    "\n",
    "            sel = rng.choice(items, size=n_select, replace=False)\n",
    "\n",
    "            # First part -> train\n",
    "            if num_train_ratings:\n",
    "                train_idx = sel[:num_train_ratings]\n",
    "                train[u, train_idx] = X[u, train_idx]\n",
    "                split_stats[\"train\"] += train_idx.size\n",
    "\n",
    "            # Rest -> test\n",
    "            if num_test_ratings:\n",
    "                test_idx = sel[num_train_ratings:]\n",
    "                test[u, test_idx] = X[u, test_idx]\n",
    "                split_stats[\"test\"] += test_idx.size\n",
    "\n",
    "            # Remaining rated items -> pool\n",
    "            if n_items > n_select:\n",
    "                sel_set = set(sel.tolist())\n",
    "                pool_items = [i for i in items if i not in sel_set]\n",
    "                if pool_items:\n",
    "                    pool[u, pool_items] = X[u, pool_items]\n",
    "                    split_stats[\"pool\"] += len(pool_items)\n",
    "\n",
    "        else:\n",
    "            # Non-AL users: all ratings stay in train\n",
    "            if n_items:\n",
    "                train[u, items] = X[u, items]\n",
    "                split_stats[\"train\"] += n_items\n",
    "\n",
    "    logger.info(f\"Split stats: {split_stats}\")\n",
    "    logger.info(\"=============\")\n",
    "\n",
    "    # Ensure contiguous arrays for Numba/CUDA safety\n",
    "    train = np.ascontiguousarray(train)\n",
    "    test  = np.ascontiguousarray(test)\n",
    "    pool  = np.ascontiguousarray(pool)\n",
    "\n",
    "    return train, test, pool, test_users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237317d-c1e4-4a5c-bdae-9eb8d9000773",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd30b7-2c7f-4e5b-b022-5c7a4b15706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(\n",
    "    train: np.ndarray,\n",
    "    test:  np.ndarray,\n",
    "    rng: np.random.Generator,\n",
    "    lamda: float,\n",
    "    steps: int = INIT_STEPS,\n",
    "    alpha: float = ALPHA_INIT,\n",
    "    beta:  float = BETA,\n",
    "    K: int = K,\n",
    "    neighbor: int = NEIGHBOR,\n",
    "    theta: float = 0.0,\n",
    "    precomputed_W: np.ndarray | None = None):\n",
    "    \"\"\"\n",
    "    Initialize Explainable MF (EMF) for ExAL.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1) Build W where W[u,i] = fraction of u’s k-NN (binary space) that rated i\n",
    "       (optionally thresholded by `theta`).\n",
    "    2) Randomly init P, Q.\n",
    "    3) Pretrain with EMF_with_explainability.\n",
    "    4) Compute initial test MAE (diagnostic).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    • W is built regardless of λ so MEP/MER can be computed later.\n",
    "    • dtypes: float32 on GPU, float64 on CPU.\n",
    "    \"\"\"\n",
    "    U, I = train.shape\n",
    "    DT = np.float32 if USE_CUDA else np.float64\n",
    "\n",
    "    # Ensure consistent dtype and contiguity for Numba/CUDA\n",
    "    train_dt = np.ascontiguousarray(train.astype(DT, copy=False))\n",
    "    test_dt  = np.ascontiguousarray(test.astype(DT,  copy=False))\n",
    "\n",
    "    P = rng.random((U, K), dtype=DT)\n",
    "    Q = rng.random((I, K), dtype=DT)\n",
    "    P = np.ascontiguousarray(P)\n",
    "    Q = np.ascontiguousarray(Q)\n",
    "\n",
    "    if precomputed_W is None:\n",
    "        W = calc_exp(train_dt, neighbor=neighbor, theta=theta)\n",
    "    else:\n",
    "        W = precomputed_W\n",
    "    # Match W dtype to compute path (important for GPU to avoid implicit casts)\n",
    "    W = np.ascontiguousarray(W.astype(DT, copy=False))\n",
    "\n",
    "    # Pretrain EMF\n",
    "    P, Q, train_mae = EMF_with_explainability(train_dt, P, Q, K, W, lamda, steps, alpha, beta)\n",
    "\n",
    "    # Initial cold-start test MAE (info only)\n",
    "    pred = P.dot(Q.T)\n",
    "    mask = (test_dt != 0)\n",
    "    test_mae = np.abs(pred[mask] - test_dt[mask]).mean() if np.any(mask) else np.nan\n",
    "\n",
    "    return P, Q, train_mae, test_mae, W\n",
    "\n",
    "\n",
    "def _rated_mask(rate: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Users with ≥1 rating.\"\"\"\n",
    "    return np.count_nonzero(rate, axis=1) > 0\n",
    "\n",
    "\n",
    "def calc_exp(rate: np.ndarray, neighbor: int = 50, theta: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build full explainability matrix W (U×I).\n",
    "\n",
    "    Definition\n",
    "    ----------\n",
    "    W[u,i] = (# of u’s k nearest neighbors who rated i) / k,\n",
    "    neighbors by cosine distance on binary user–item space.\n",
    "\n",
    "    Edge cases\n",
    "    ----------\n",
    "    • If <2 rated users or k==0 → zeros.\n",
    "    • If theta>0 → hard-threshold values below theta to 0.\n",
    "\n",
    "    Cost\n",
    "    ----\n",
    "    O(M^2 I) for pairwise distances among M rated users.\n",
    "    \"\"\"\n",
    "    U, I = rate.shape\n",
    "    DT = np.float32 if USE_CUDA else np.float64\n",
    "    W = np.zeros((U, I), dtype=DT)\n",
    "\n",
    "    mask_users = _rated_mask(rate)\n",
    "    M = int(mask_users.sum())\n",
    "    if M <= 1:\n",
    "        return W\n",
    "\n",
    "    k = min(neighbor, M - 1)\n",
    "    if k <= 0:\n",
    "        return W\n",
    "\n",
    "    # Binary view for distances/counts; keep float for sklearn\n",
    "    bin_rate = (rate > 0).astype(np.float64, copy=False)\n",
    "\n",
    "    # Distances among rated users only\n",
    "    sub = bin_rate[mask_users]                 # [M, I]\n",
    "    dist = pairwise_distances(sub, metric='cosine')  # [M, M]\n",
    "\n",
    "    # k-NN indices (skip self)\n",
    "    nn = np.argsort(dist, axis=1)[:, 1:k+1]    # [M, k]\n",
    "\n",
    "    # Count neighbor ratings per item, normalize by k (in float64 to avoid tiny drift)\n",
    "    expl_sub = sub[nn, :].sum(axis=1) / float(k)   # [M, I]\n",
    "\n",
    "    # Scatter back to full W\n",
    "    idx_users = np.flatnonzero(mask_users)\n",
    "    W[idx_users, :] = expl_sub.astype(DT, copy=False)\n",
    "\n",
    "    if theta > 0.0:\n",
    "        W[W < theta] = DT(0.0)\n",
    "    return np.ascontiguousarray(W)\n",
    "\n",
    "\n",
    "def calc_exp_row(rate: np.ndarray, u: int, neighbor: int = 50, theta: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    One user’s explainability row W[u,:].\n",
    "\n",
    "    Build neighbors for user u among users with ≥1 rating and\n",
    "    return fraction of those neighbors who rated each item.\n",
    "\n",
    "    Edge cases\n",
    "    ----------\n",
    "    • If u has no ratings or k==0 → zeros.\n",
    "    \"\"\"\n",
    "    U, I = rate.shape\n",
    "    DT = np.float32 if USE_CUDA else np.float64\n",
    "    mask_users = _rated_mask(rate)\n",
    "    if not mask_users[u]:\n",
    "        return np.zeros(I, dtype=DT)\n",
    "\n",
    "    idx_users = np.flatnonzero(mask_users)\n",
    "    M = int(mask_users.sum())\n",
    "    k = min(neighbor, M - 1)\n",
    "    if k <= 0:\n",
    "        return np.zeros(I, dtype=DT)\n",
    "\n",
    "    # Position of u in compacted matrix\n",
    "    pos = int(np.where(idx_users == u)[0][0])\n",
    "\n",
    "    bin_rate = (rate > 0).astype(np.float64, copy=False)\n",
    "    sub = bin_rate[mask_users]  # [M, I]\n",
    "\n",
    "    # Distances from u to others\n",
    "    dist_u = pairwise_distances(sub[pos][None, :], sub, metric='cosine')[0]\n",
    "    nn_idx = np.argsort(dist_u)[1:k+1]  # skip self\n",
    "\n",
    "    expl_u = sub[nn_idx, :].sum(axis=0) / float(k)\n",
    "    if theta > 0.0:\n",
    "        expl_u[expl_u < theta] = 0.0\n",
    "    return np.ascontiguousarray(expl_u.astype(DT, copy=False))\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def EMF_with_explainability(\n",
    "    R, P, Q, K, W, lamda, steps, alpha, beta\n",
    "):\n",
    "    \"\"\"\n",
    "    Explainable MF via SGD (Numba).\n",
    "\n",
    "    Objective\n",
    "    ---------\n",
    "    Squared error + β||·||² + λ W[u,i] ||P_u − Q_i||² over observed entries.\n",
    "\n",
    "    Impl notes\n",
    "    ----------\n",
    "    • Use Q^T internally for cache-friendly access.\n",
    "    • Iterate only over nonzeros.\n",
    "    • Return Q transposed back.\n",
    "    \"\"\"\n",
    "    # Work with column-major access for items\n",
    "    Q = Q.T\n",
    "    U, I = R.shape\n",
    "\n",
    "    # Gather coordinates of observed ratings\n",
    "    nz = 0\n",
    "    for u in range(U):\n",
    "        for i in range(I):\n",
    "            if R[u, i] != 0:\n",
    "                nz += 1\n",
    "\n",
    "    nnz_u = np.empty(nz, dtype=np.int64)\n",
    "    nnz_i = np.empty(nz, dtype=np.int64)\n",
    "\n",
    "    idx = 0\n",
    "    for u in range(U):\n",
    "        for i in range(I):\n",
    "            if R[u, i] != 0:\n",
    "                nnz_u[idx] = u\n",
    "                nnz_i[idx] = i\n",
    "                idx += 1\n",
    "\n",
    "    # SGD\n",
    "    for _ in range(steps):\n",
    "        for t in range(nz):\n",
    "            u = nnz_u[t]\n",
    "            i = nnz_i[t]\n",
    "            r = R[u, i]\n",
    "\n",
    "            # prediction and error\n",
    "            s = 0.0\n",
    "            for f in range(K):\n",
    "                s += P[u, f] * Q[f, i]\n",
    "            e = r - s\n",
    "\n",
    "            Wi = W[u, i]\n",
    "            for f in range(K):\n",
    "                diff = P[u, f] - Q[f, i]\n",
    "                grad_p = 2.0 * e * Q[f, i] - beta * P[u, f] - lamda * Wi * diff\n",
    "                grad_q = 2.0 * e * P[u, f] - beta * Q[f, i] + lamda * Wi * diff\n",
    "                P[u, f] += alpha * grad_p\n",
    "                Q[f, i]  += alpha * grad_q\n",
    "\n",
    "    # MAE over observed R (diagnostic)\n",
    "    if nz == 0:\n",
    "        train_mae = 0.0\n",
    "    else:\n",
    "        total_err = 0.0\n",
    "        for t in range(nz):\n",
    "            u = nnz_u[t]\n",
    "            i = nnz_i[t]\n",
    "            r = R[u, i]\n",
    "            s = 0.0\n",
    "            for f in range(K):\n",
    "                s += P[u, f] * Q[f, i]\n",
    "            total_err += abs(r - s)\n",
    "        train_mae = total_err / nz\n",
    "\n",
    "    return P, Q.T, train_mae\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def retrain_online_exp(u, train, P_init, Q, W, alpha, beta, K, steps, lamda):\n",
    "    \"\"\"\n",
    "    Online update for a single user u.\n",
    "\n",
    "    Same objective as EMF_with_explainability, but only updates P[u]\n",
    "    with Q fixed after adding new ratings to `train`.\n",
    "    \"\"\"\n",
    "    P_u = P_init[u].copy()\n",
    "    Q_t = Q.T\n",
    "    I = train.shape[1]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        for i in range(I):\n",
    "            r = train[u, i]\n",
    "            if r != 0:\n",
    "                e = r - np.dot(P_u, Q_t[:, i])\n",
    "                Wi = W[u, i]\n",
    "                for f in range(K):\n",
    "                    diff = P_u[f] - Q_t[f, i]\n",
    "                    grad = 2.0 * e * Q_t[f, i] - beta * P_u[f] - lamda * Wi * diff\n",
    "                    P_u[f] += alpha * grad\n",
    "    return P_u\n",
    "\n",
    "\n",
    "def calc_avg(train: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Per-item mean rating (ignore zeros).\n",
    "    Cold items fall back to global mean.\n",
    "    Returns 1D array of length I.\n",
    "    \"\"\"\n",
    "    sums   = train.sum(axis=0)\n",
    "    counts = (train != 0).sum(axis=0)\n",
    "    global_mean = float(sums.sum()) / max(int(counts.sum()), 1)\n",
    "    avg = np.full_like(sums, fill_value=global_mean, dtype=float)\n",
    "    np.divide(sums, counts, out=avg, where=counts > 0)\n",
    "    return avg\n",
    "\n",
    "\n",
    "# --- CUDA path: row-wise transfer variant (keep Q persistent per iteration) ---\n",
    "if 'cuda' in globals():\n",
    "    @cuda.jit\n",
    "    def _retrain_one_user_row_kernel(train_u, P_u, Q, W_u, alpha, beta, K, steps, lamda):\n",
    "        \"\"\"\n",
    "        Single-thread kernel for updating one user vector P_u.\n",
    "        Reads only this user’s train_u and W_u; Q is shared (I×K).\n",
    "        Launch with [1,1]. Requires K ≤ MAX_K.\n",
    "        \"\"\"\n",
    "        if cuda.threadIdx.x != 0 or cuda.blockIdx.x != 0:\n",
    "            return\n",
    "\n",
    "        I = train_u.shape[0]\n",
    "\n",
    "        # Local copy of P_u (registers/local mem)\n",
    "        P_loc = cuda.local.array(MAX_K, numba.float32)\n",
    "        for f in range(K):\n",
    "            P_loc[f] = P_u[f]\n",
    "\n",
    "        # SGD over rated items in this row\n",
    "        for _ in range(steps):\n",
    "            for i in range(I):\n",
    "                r = train_u[i]\n",
    "                if r != 0.0:\n",
    "                    # dot(P_loc, Q[i])\n",
    "                    s = 0.0\n",
    "                    for f in range(K):\n",
    "                        s += P_loc[f] * Q[i, f]\n",
    "                    e = r - s\n",
    "                    Wi = W_u[i]\n",
    "                    for f in range(K):\n",
    "                        diff = P_loc[f] - Q[i, f]\n",
    "                        grad = 2.0 * e * Q[i, f] - beta * P_loc[f] - lamda * Wi * diff\n",
    "                        P_loc[f] += alpha * grad\n",
    "\n",
    "        # Write back\n",
    "        for f in range(K):\n",
    "            P_u[f] = P_loc[f]\n",
    "\n",
    "\n",
    "def retrain_online_exp_gpu(\n",
    "    u: int,\n",
    "    train: np.ndarray,\n",
    "    P_init: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    alpha: float,\n",
    "    beta: float,\n",
    "    K: int,\n",
    "    steps: int,\n",
    "    lamda: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPU drop-in for `retrain_online_exp` using **row-wise transfers**.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    • Copy only train[u,:] and W[u,:] per call.\n",
    "    • Keep d_Q persistent; refresh if Q object/shape changes.\n",
    "    • Return updated P[u] (original dtype).\n",
    "    \"\"\"\n",
    "    if not USE_CUDA:\n",
    "        raise RuntimeError(\"CUDA not available. Use CPU retrain_online_exp instead.\")\n",
    "    if K > MAX_K:\n",
    "        raise ValueError(f\"K={K} exceeds MAX_K={MAX_K}; increase MAX_K or lower K.\")\n",
    "\n",
    "    # Persistent device buffers\n",
    "    cache = retrain_online_exp_gpu.__dict__.setdefault(\"_cache\", {})\n",
    "    I = int(Q.shape[0])\n",
    "\n",
    "    DT32 = np.float32\n",
    "\n",
    "    # Refresh d_Q when needed\n",
    "    q_obj_id = id(Q)\n",
    "    shapes_changed = (cache.get(\"I\") != I) or (cache.get(\"K\") != K)\n",
    "\n",
    "    if (\"d_Q\" not in cache) or shapes_changed or (cache.get(\"q_obj_id\") != q_obj_id):\n",
    "        cache[\"I\"] = I\n",
    "        cache[\"K\"] = K\n",
    "        cache[\"q_obj_id\"] = q_obj_id\n",
    "        cache[\"d_Q\"] = cuda.to_device(np.asarray(Q, dtype=DT32))  # (I,K)\n",
    "\n",
    "    # Ensure row buffers exist\n",
    "    if (\"d_train_u\" not in cache) or shapes_changed:\n",
    "        cache[\"d_train_u\"] = cuda.device_array((I,), dtype=DT32)\n",
    "        cache[\"d_W_u\"]     = cuda.device_array((I,), dtype=DT32)\n",
    "        cache[\"d_P_u\"]     = cuda.device_array((K,), dtype=DT32)\n",
    "\n",
    "    d_Q       = cache[\"d_Q\"]\n",
    "    d_train_u = cache[\"d_train_u\"]\n",
    "    d_W_u     = cache[\"d_W_u\"]\n",
    "    d_P_u     = cache[\"d_P_u\"]\n",
    "\n",
    "    # Host→device for this user only\n",
    "    train_u32 = np.asarray(train[u], dtype=DT32)\n",
    "    W_u32     = np.asarray(W[u],     dtype=DT32)\n",
    "    P_u32     = np.asarray(P_init[u], dtype=DT32)\n",
    "\n",
    "    d_train_u.copy_to_device(train_u32)\n",
    "    d_W_u.copy_to_device(W_u32)\n",
    "    d_P_u.copy_to_device(P_u32)\n",
    "\n",
    "    # Launch kernel (single thread; suppress perf warnings)\n",
    "    import warnings\n",
    "    from numba.core.errors import NumbaPerformanceWarning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", NumbaPerformanceWarning)\n",
    "        _retrain_one_user_row_kernel[1, 1](\n",
    "            d_train_u, d_P_u, d_Q, d_W_u,\n",
    "            DT32(alpha), DT32(beta), np.int32(K), np.int32(steps), DT32(lamda)\n",
    "        )\n",
    "\n",
    "    # Device→host for P[u]\n",
    "    P_u_out = d_P_u.copy_to_host()\n",
    "    return P_u_out.astype(P_init.dtype, copy=False)\n",
    "\n",
    "\n",
    "def retrain_online_exp_gpu_clear_cache() -> None:\n",
    "    \"\"\"\n",
    "    Free persistent device buffers used by retrain_online_exp_gpu.\n",
    "    \"\"\"\n",
    "    cache = retrain_online_exp_gpu.__dict__.get(\"_cache\")\n",
    "    if cache:\n",
    "        retrain_online_exp_gpu.__dict__.pop(\"_cache\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0c7cf-32d9-4220-ab3b-2228cc685096",
   "metadata": {},
   "source": [
    "# ExAL selections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac6f3b-5808-4061-beb3-9fdafc1434dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def active_selection_exal_min(\n",
    "    u: int,\n",
    "    test_items: np.ndarray,   # indices of u’s held-out items\n",
    "    pool: np.ndarray,         # nonzero ⇒ candidate in u’s pool\n",
    "    eR: np.ndarray,           # predictions = P @ Q^T\n",
    "    lR: np.ndarray,           # per-item running mean\n",
    "    Q_dot: np.ndarray,        # optional item–item dots (I×I) or empty (0×0)\n",
    "    Q: np.ndarray,            # item factors (I×K)\n",
    "    expl: np.ndarray,         # W explainability weights (U×I)\n",
    "    alpha: float,             # online LR used in the bound\n",
    "    lamda: float,             # explainability λ (selection term)\n",
    "    K: int,                   # latent dim\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    EXAL-Min: for each candidate m in u’s pool, sum the absolute EXAL bound\n",
    "    over u’s test items and pick the *smallest* sum.\n",
    "\n",
    "    Inside term (aligned with SGD):\n",
    "      Δr_uj ≈ α * [ 2(R_um − lR[m])(Q_m·Q_j) + λ W_{u,m}(r̂_uj − Q_m·Q_j) ]\n",
    "    \"\"\"\n",
    "    if test_items.shape[0] == 0:\n",
    "        return -1\n",
    "\n",
    "    use_Qdot = Q_dot.size != 0\n",
    "    best_score = np.inf\n",
    "    best_m = -1\n",
    "    I = pool.shape[1]\n",
    "\n",
    "    for m in range(I):\n",
    "        if pool[u, m] == 0:\n",
    "            continue\n",
    "\n",
    "        Rum = eR[u, m]\n",
    "        Qm = Q[m]\n",
    "        s = 0.0\n",
    "\n",
    "        for t in range(test_items.shape[0]):\n",
    "            j = test_items[t]\n",
    "\n",
    "            if use_Qdot:\n",
    "                dp = Q_dot[m, j]\n",
    "            else:\n",
    "                sdp = 0.0\n",
    "                Qj = Q[j]\n",
    "                for f in range(K):\n",
    "                    sdp += Qm[f] * Qj[f]\n",
    "                dp = sdp\n",
    "\n",
    "            ruj_pred = eR[u, j]\n",
    "            inside = 1.0 - ruj_pred + 2.0 * alpha * (\n",
    "                (Rum - lR[m]) * dp\n",
    "                + lamda * expl[u, m] * (ruj_pred - dp)\n",
    "            )\n",
    "            s += abs(inside)\n",
    "\n",
    "        if s < best_score:\n",
    "            best_score = s\n",
    "            best_m = m\n",
    "\n",
    "    return best_m\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_exal_max(\n",
    "    u: int,\n",
    "    test_items: np.ndarray,\n",
    "    pool: np.ndarray,\n",
    "    eR: np.ndarray,\n",
    "    lR: np.ndarray,\n",
    "    Q_dot: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    expl: np.ndarray,\n",
    "    alpha: float,\n",
    "    lamda: float,\n",
    "    K: int,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    EXAL-Max: identical score as EXAL-Min but choose the *largest* sum.\n",
    "    Uses the same corrected inside term.\n",
    "    \"\"\"\n",
    "    if test_items.shape[0] == 0:\n",
    "        return -1\n",
    "\n",
    "    use_Qdot = Q_dot.size != 0\n",
    "    best_score = -np.inf\n",
    "    best_m = -1\n",
    "    I = pool.shape[1]\n",
    "\n",
    "    for m in range(I):\n",
    "        if pool[u, m] == 0:\n",
    "            continue\n",
    "\n",
    "        Rum = eR[u, m]\n",
    "        Qm = Q[m]\n",
    "        s = 0.0\n",
    "\n",
    "        for t in range(test_items.shape[0]):\n",
    "            j = test_items[t]\n",
    "\n",
    "            if use_Qdot:\n",
    "                dp = Q_dot[m, j]\n",
    "            else:\n",
    "                sdp = 0.0\n",
    "                Qj = Q[j]\n",
    "                for f in range(K):\n",
    "                    sdp += Qm[f] * Qj[f]\n",
    "                dp = sdp\n",
    "\n",
    "            ruj_pred = eR[u, j]\n",
    "            inside = 1.0 - ruj_pred + 2.0 * alpha * (\n",
    "                (Rum - lR[m]) * dp\n",
    "                + lamda * expl[u, m] * (ruj_pred - dp)\n",
    "            )\n",
    "            s += abs(inside)\n",
    "\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_m = m\n",
    "\n",
    "    return best_m\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_exal_max_min(\n",
    "    u: int,\n",
    "    test_items: np.ndarray,\n",
    "    pool: np.ndarray,\n",
    "    eR: np.ndarray,\n",
    "    lR: np.ndarray,\n",
    "    Q_dot: np.ndarray,\n",
    "    Q: np.ndarray,\n",
    "    expl: np.ndarray,\n",
    "    alpha: float,\n",
    "    lamda: float,\n",
    "    iteration: int,\n",
    "    switch_point: int,\n",
    "    K: int,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    EXAL Max→Min: use EXAL-Max while iteration < (switch_point−1),\n",
    "    then EXAL-Min. \n",
    "    \"\"\"\n",
    "    if iteration < switch_point:\n",
    "        return active_selection_exal_max(u, test_items, pool, eR, lR, Q_dot, Q, expl, alpha, lamda, K)\n",
    "    else:\n",
    "        return active_selection_exal_min(u, test_items, pool, eR, lR, Q_dot, Q, expl, alpha, lamda, K)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_karimi(\n",
    "   u: int,\n",
    "   test_items: np.ndarray,\n",
    "   pool: np.ndarray,\n",
    "   eR: np.ndarray,\n",
    "   lR: np.ndarray,\n",
    "   Q_dot: np.ndarray,\n",
    "   Q: np.ndarray,\n",
    "   alpha: float,\n",
    "   K: int,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Karimi baseline (λ=0 analogue):\n",
    "    EXAL without the explainability term.\n",
    "\n",
    "    Inside term:\n",
    "      1 − r̂_uj + 2α (R_um − lR[m]) (Q_m·Q_j)\n",
    "    Choose candidate with *smallest* summed absolute term.\n",
    "    \"\"\"\n",
    "    if test_items.shape[0] == 0:\n",
    "        return -1\n",
    "\n",
    "    use_Qdot = Q_dot.size != 0\n",
    "    best_m = -1\n",
    "    best_score = np.inf\n",
    "    I = pool.shape[1]\n",
    "\n",
    "    for m in range(I):\n",
    "        if pool[u, m] == 0:\n",
    "            continue\n",
    "\n",
    "        Rum = eR[u, m]\n",
    "        Qm = Q[m]\n",
    "        s = 0.0\n",
    "\n",
    "        for t in range(test_items.shape[0]):\n",
    "            j = test_items[t]\n",
    "\n",
    "            if use_Qdot:\n",
    "                dp = Q_dot[m, j]\n",
    "            else:\n",
    "                sdp = 0.0\n",
    "                Qj = Q[j]\n",
    "                for f in range(K):\n",
    "                    sdp += Qm[f] * Qj[f]\n",
    "                dp = sdp\n",
    "\n",
    "            ruj_pred = eR[u, j]\n",
    "            inside = 1.0 - ruj_pred + 2.0 * alpha * ((Rum - lR[m]) * dp)\n",
    "            s += abs(inside)\n",
    "\n",
    "        if s < best_score:\n",
    "            best_score = s\n",
    "            best_m = m\n",
    "\n",
    "    return best_m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738efd38-0289-4b02-8bb2-bd5406abe419",
   "metadata": {},
   "source": [
    "# Active Learning Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2350a7-7037-4a2d-b6be-ad2856173bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def select_random(u, pool, rand_val):\n",
    "    \"\"\"\n",
    "    Random pick from user u’s pool (nonzero entries).\n",
    "    Deterministic for a given rand_val∈[0,1):\n",
    "      - count candidates\n",
    "      - pick k-th valid where k=floor(rand_val * count)\n",
    "    Returns -1 if u has no candidates.\n",
    "    \"\"\"\n",
    "    pool_u = pool[u]\n",
    "    I = pool_u.shape[0]\n",
    "\n",
    "    # count candidates\n",
    "    cnt = 0\n",
    "    for i in range(I):\n",
    "        if pool_u[i] != 0:\n",
    "            cnt += 1\n",
    "    if cnt == 0:\n",
    "        return -1\n",
    "\n",
    "    # map rand_val → index [0, cnt-1]\n",
    "    k = int(rand_val * cnt)\n",
    "    if k >= cnt:\n",
    "        k = cnt - 1  # guard rand_val==1.0\n",
    "\n",
    "    # return k-th valid index\n",
    "    seen = 0\n",
    "    for i in range(I):\n",
    "        if pool_u[i] != 0:\n",
    "            if seen == k:\n",
    "                return i\n",
    "            seen += 1\n",
    "\n",
    "    return -1  # unreachable fallback\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_uncertainty(u, pool, eR, midpoint=3.0):\n",
    "    \"\"\"\n",
    "    Uncertainty sampling:\n",
    "    choose the pool item whose prediction for u is closest to midpoint (≈3.0).\n",
    "    Returns -1 if no candidates.\n",
    "    \"\"\"\n",
    "    pool_u = pool[u]\n",
    "    eR_u = eR[u]\n",
    "    I = pool_u.shape[0]\n",
    "\n",
    "    best_idx = -1\n",
    "    best_dist = np.inf\n",
    "\n",
    "    for i in range(I):\n",
    "        if pool_u[i] != 0:\n",
    "            d = abs(eR_u[i] - midpoint)\n",
    "            if d < best_dist:\n",
    "                best_dist = d\n",
    "                best_idx = i\n",
    "\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_highest_pred(u, pool, eR):\n",
    "    \"\"\"\n",
    "    Greedy exploitation:\n",
    "    pick the candidate with the highest predicted rating for user u.\n",
    "    \"\"\"\n",
    "    best_idx = -1\n",
    "    best_score = -np.inf\n",
    "    I = pool.shape[1]\n",
    "    for i in range(I):\n",
    "        if pool[u, i] != 0:\n",
    "            s = eR[u, i]\n",
    "            if s > best_score:\n",
    "                best_score = s\n",
    "                best_idx = i\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_highest_confidence(u, pool, eR, midpoint=3.0):\n",
    "    \"\"\"\n",
    "    Most confident prediction:\n",
    "    pick the candidate furthest from midpoint (absolute distance).\n",
    "    \"\"\"\n",
    "    best_idx = -1\n",
    "    best_confidence = -1.0\n",
    "    \n",
    "    for i in range(pool.shape[1]):\n",
    "        if pool[u, i] != 0:\n",
    "            conf = abs(eR[u, i] - midpoint)\n",
    "            if conf > best_confidence:\n",
    "                best_confidence = conf\n",
    "                best_idx = i\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_highest_variance(u, pool, eR):\n",
    "    \"\"\"\n",
    "    Highest global variance:\n",
    "      1) compute per-item variance across users\n",
    "      2) among u’s pool, pick item with max variance\n",
    "    Returns -1 if no candidates.\n",
    "    \"\"\"\n",
    "    U, I = eR.shape\n",
    "\n",
    "    # per-item mean and mean-of-squares\n",
    "    item_mean = np.empty(I)\n",
    "    item_msq  = np.empty(I)\n",
    "\n",
    "    for i in range(I):\n",
    "        s = 0.0\n",
    "        ss = 0.0\n",
    "        for uu in range(U):\n",
    "            x = eR[uu, i]\n",
    "            s  += x\n",
    "            ss += x * x\n",
    "        invU = 1.0 / U\n",
    "        item_mean[i] = s * invU\n",
    "        item_msq[i]  = ss * invU\n",
    "\n",
    "    # scan u’s pool\n",
    "    pool_u = pool[u]\n",
    "    best_idx = -1\n",
    "    best_var = -1.0\n",
    "\n",
    "    for i in range(I):\n",
    "        if pool_u[i] == 0:\n",
    "            continue\n",
    "        mu = item_mean[i]\n",
    "        var = item_msq[i] - mu * mu  # Var = E[X^2] − (E[X])^2\n",
    "        if var > best_var:\n",
    "            best_var = var\n",
    "            best_idx = i\n",
    "\n",
    "    return best_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9ebb7-1188-44ff-8657-919bb3dc36db",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bd976-23b8-453c-b67e-b8536f6ddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Top-N helper (Numba-safe)\n",
    "# -----------------------------\n",
    "@numba.njit\n",
    "def topn(eR, n, u):\n",
    "    row = eR[u]\n",
    "    finite = np.isfinite(row)\n",
    "    idx = np.arange(row.shape[0])[finite]\n",
    "    if idx.size == 0:\n",
    "        return idx\n",
    "    scores = row[idx]\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    k = n if n < order.size else order.size\n",
    "    return idx[order[:k]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Explainability @N\n",
    "# -----------------------------\n",
    "@numba.njit\n",
    "def calculate_MER(eR, W, users, n):\n",
    "    \"\"\"\n",
    "    MER@N (Mean Explainable Recall):\n",
    "      For each user, recall = (# explainable *candidates* appearing in top-N)\n",
    "                              / (# explainable *candidates* overall).\n",
    "      A candidate is an unseen item with a finite prediction after masking.\n",
    "      Users with no explainable candidates OR with no finite candidates are skipped.\n",
    "    Returns:\n",
    "      scalar MER in [0,1].\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    counted = 0\n",
    "\n",
    "    I = W.shape[1]\n",
    "    Uq = users.shape[0]\n",
    "\n",
    "    for ui in range(Uq):\n",
    "        u = users[ui]\n",
    "        row = eR[u]\n",
    "\n",
    "        # skip if user has no finite (unseen) candidates\n",
    "        has_finite = False\n",
    "        for j in range(row.shape[0]):\n",
    "            if np.isfinite(row[j]):\n",
    "                has_finite = True\n",
    "                break\n",
    "        if not has_finite:\n",
    "            continue\n",
    "\n",
    "        # total explainable *candidates* for user u\n",
    "        expl_total = 0\n",
    "        for j in range(I):\n",
    "            if (W[u, j] > 0.0) and np.isfinite(row[j]):\n",
    "                expl_total += 1\n",
    "        if expl_total == 0:\n",
    "            continue\n",
    "\n",
    "        # explainable candidates in top-N\n",
    "        top = topn(eR, n, u)\n",
    "        cnt = 0\n",
    "        for k_i in range(top.shape[0]):\n",
    "            k = top[k_i]\n",
    "            if (W[u, k] > 0.0) and np.isfinite(row[k]):\n",
    "                cnt += 1\n",
    "\n",
    "        total += (cnt / float(expl_total))\n",
    "        counted += 1\n",
    "\n",
    "    return total / counted if counted > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def calculate_MEP(eR, W, users, n):\n",
    "    \"\"\"\n",
    "    MEP@N (Mean Explainable Precision):\n",
    "      For each user, precision = (# explainable in top-L) / L,\n",
    "      where L = min(N, #finite candidates for that user).\n",
    "      Users with L==0 are skipped.\n",
    "    Returns:\n",
    "      (MEP, total_explainable_found, total_positions_L)\n",
    "    \"\"\"\n",
    "    MEP_sum = 0.0\n",
    "    total_expl = 0\n",
    "    total_L = 0\n",
    "    counted = 0\n",
    "\n",
    "    Uq = users.shape[0]\n",
    "    J = eR.shape[1]\n",
    "\n",
    "    for ui in range(Uq):\n",
    "        u = users[ui]\n",
    "        row = eR[u]\n",
    "\n",
    "        # count finite candidates\n",
    "        finite_cnt = 0\n",
    "        for j in range(J):\n",
    "            if np.isfinite(row[j]):\n",
    "                finite_cnt += 1\n",
    "        if finite_cnt == 0:\n",
    "            continue\n",
    "\n",
    "        top = topn(eR, n, u)\n",
    "        top_len = n if finite_cnt >= n else finite_cnt\n",
    "\n",
    "        cnt = 0\n",
    "        for r in range(top_len):\n",
    "            k = top[r]\n",
    "            # be explicit: only count explainable *candidates*\n",
    "            if (W[u, k] > 0.0) and np.isfinite(row[k]):\n",
    "                cnt += 1\n",
    "\n",
    "        MEP_sum += cnt / float(top_len)\n",
    "        total_expl += cnt\n",
    "        total_L += top_len\n",
    "        counted += 1\n",
    "\n",
    "    return (MEP_sum / counted if counted > 0 else 0.0), total_expl, total_L\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Ranking metrics\n",
    "# -----------------------------\n",
    "@numba.njit\n",
    "def calculate_MAP(eR, test, users, n):\n",
    "    \"\"\"\n",
    "    MAP@N:\n",
    "      For each user:\n",
    "        - Let R = #relevant in test[u].\n",
    "        - Let L = min(N, #finite candidates).\n",
    "        - AP = sum_{rank<=L} (precision@rank when item is relevant) / min(R, L).\n",
    "      Users with R==0 or L==0 are skipped.\n",
    "    Returns:\n",
    "      mean(AP) over valid users.\n",
    "    \"\"\"\n",
    "    total_ap = 0.0\n",
    "    valid = 0\n",
    "\n",
    "    Uq = users.shape[0]\n",
    "    I = test.shape[1]\n",
    "\n",
    "    for ui in range(Uq):\n",
    "        u = users[ui]\n",
    "\n",
    "        # count relevant\n",
    "        rel = 0\n",
    "        for j in range(I):\n",
    "            if test[u, j] != 0:\n",
    "                rel += 1\n",
    "        if rel == 0:\n",
    "            continue\n",
    "\n",
    "        # count finite candidates\n",
    "        row = eR[u]\n",
    "        finite_cnt = 0\n",
    "        for j in range(row.shape[0]):\n",
    "            if np.isfinite(row[j]):\n",
    "                finite_cnt += 1\n",
    "        if finite_cnt == 0:\n",
    "            continue\n",
    "\n",
    "        top = topn(eR, n, u)\n",
    "        top_len = n if finite_cnt >= n else finite_cnt\n",
    "\n",
    "        hits = 0.0\n",
    "        sum_prec = 0.0\n",
    "        for rank in range(top_len):\n",
    "            j = top[rank]\n",
    "            if test[u, j] != 0:\n",
    "                hits += 1.0\n",
    "                sum_prec += hits / float(rank + 1)\n",
    "\n",
    "        denom = rel if rel < top_len else top_len\n",
    "        if denom > 0:\n",
    "            total_ap += (sum_prec / float(denom))\n",
    "            valid += 1\n",
    "\n",
    "    return total_ap / valid if valid > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ndcg(eR, test, users, n, graded=False):\n",
    "    ndcg_total = 0.0\n",
    "    valid_users = 0\n",
    "\n",
    "    for k in range(users.shape[0]):\n",
    "        u = users[k]\n",
    "        row = eR[u]\n",
    "        rel = test[u]\n",
    "\n",
    "        # keep only finite candidates\n",
    "        finite = np.isfinite(row)\n",
    "        cand = np.where(finite)[0]\n",
    "        if cand.size == 0:\n",
    "            continue\n",
    "\n",
    "        scores = row[cand]\n",
    "        order = np.argsort(scores)[::-1]\n",
    "        top_idx = cand[order[:n]]\n",
    "\n",
    "        # DCG\n",
    "        dcg = 0.0\n",
    "        for i, j in enumerate(top_idx):\n",
    "            g = rel[j] if graded else (1.0 if rel[j] > 0 else 0.0)\n",
    "            if g > 0.0:\n",
    "                dcg += g / np.log2(i + 2.0)\n",
    "\n",
    "        # IDCG (best possible among the same candidate set)\n",
    "        rel_cand = rel[cand]\n",
    "        ideal = np.sort(rel_cand)[::-1][:n]\n",
    "        idcg = 0.0\n",
    "        for i in range(ideal.shape[0]):\n",
    "            g = ideal[i] if graded else (1.0 if ideal[i] > 0 else 0.0)\n",
    "            if g > 0.0:\n",
    "                idcg += g / np.log2(i + 2.0)\n",
    "\n",
    "        if idcg > 0.0:\n",
    "            ndcg_total += dcg / idcg\n",
    "            valid_users += 1\n",
    "\n",
    "    return (ndcg_total / valid_users) if valid_users > 0 else 0.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Popularity / Novelty / Diversity (CPU-side)\n",
    "# -----------------------------\n",
    "def calculate_item_coverage(topN_items_all_users, num_items):\n",
    "    \"\"\"\n",
    "    Item Coverage (IC):\n",
    "      Fraction of catalog items that appear at least once in any user's top-N list.\n",
    "    \"\"\"\n",
    "    if num_items <= 0:\n",
    "        return 0.0\n",
    "    unique_items = set()\n",
    "    for user_items in topN_items_all_users:\n",
    "        unique_items.update(user_items)\n",
    "    return len(unique_items) / float(num_items)\n",
    "\n",
    "\n",
    "def calculate_gini_index(topN_items_all_users, num_items):\n",
    "    \"\"\"\n",
    "    Gini index of recommendation exposure across items (0=uniform, 1=concentrated).\n",
    "    Includes items with zero exposure.\n",
    "    \"\"\"\n",
    "    if num_items <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    counts = np.zeros(num_items, dtype=np.float64)\n",
    "    for user_items in topN_items_all_users:\n",
    "        for it in user_items:\n",
    "            if 0 <= it < num_items:\n",
    "                counts[it] += 1.0\n",
    "\n",
    "    total = counts.sum()\n",
    "    if total == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    counts.sort()\n",
    "    n = float(num_items)\n",
    "    index = np.arange(1, num_items + 1, dtype=np.float64)\n",
    "    gini = (2.0 * (index * counts).sum()) / (n * total) - (n + 1.0) / n\n",
    "    # Numerical safety\n",
    "    if gini < 0.0:\n",
    "        gini = 0.0\n",
    "    elif gini > 1.0:\n",
    "        gini = 1.0\n",
    "    return float(gini)\n",
    "\n",
    "\n",
    "def calculate_ARP(topN_items_all_users, item_popularity):\n",
    "    \"\"\"\n",
    "    ARP (Average Recommendation Popularity):\n",
    "      Mean popularity (count of raters) of the items that appear in top-N lists.\n",
    "      Lower is better (more long-tail exposure).\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    cnt = 0\n",
    "    for user_items in topN_items_all_users:\n",
    "        for it in user_items:\n",
    "            total += float(item_popularity[it])\n",
    "            cnt += 1\n",
    "    return total / cnt if cnt > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_novelty_log2(topN_items_per_user, item_popularity, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Novelty (dataset-specific):\n",
    "      Mean of -log2(popularity + eps) over all recommended items.\n",
    "    \"\"\"\n",
    "    logp = np.log2(item_popularity + eps)\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for items in topN_items_per_user:\n",
    "        s += float(-np.sum(logp[items]))\n",
    "        c += len(items)\n",
    "    return s / c if c else 0.0\n",
    "\n",
    "\n",
    "def calculate_novelty_IDF(topN_items_per_user, item_popularity, num_users, eps=1e-6):\n",
    "    \"\"\"\n",
    "    IDF-style Novelty (dataset-comparable):\n",
    "      novelty(i) = -log2( (pop(i)+eps) / num_users )\n",
    "    \"\"\"\n",
    "    if num_users <= 0:\n",
    "        return 0.0\n",
    "    p = (item_popularity + eps) / float(num_users)\n",
    "    inv_info = -np.log2(p)\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for items in topN_items_per_user:\n",
    "        s += float(np.sum(inv_info[items]))\n",
    "        c += len(items)\n",
    "    return s / c if c else 0.0\n",
    "\n",
    "\n",
    "def calculate_novelty_EFD(topN_items_per_user, item_popularity, num_users, eps=1e-6):\n",
    "    \"\"\"\n",
    "    EFD (Expected Free Discovery):\n",
    "      Mean of 1 / freq(i), where freq(i) = (pop(i)+eps)/num_users.\n",
    "      Larger values suggest rarer items overall.\n",
    "    \"\"\"\n",
    "    if num_users <= 0:\n",
    "        return 0.0\n",
    "    freq = (item_popularity + eps) / float(num_users)  # (0, 1]\n",
    "    inv = 1.0 / freq\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for items in topN_items_per_user:\n",
    "        s += float(np.sum(inv[items]))\n",
    "        c += len(items)\n",
    "    return s / c if c else 0.0\n",
    "\n",
    "\n",
    "def calculate_novelty_EPC(topN_items_per_user, item_popularity):\n",
    "    \"\"\"\n",
    "    EPC (Expected Popularity Complement) in [0,1]:\n",
    "      Mean of (1 - pop(i)/max_pop) across all recommended items.\n",
    "    \"\"\"\n",
    "    max_pop = max(1, int(item_popularity.max()))\n",
    "    s = 0.0\n",
    "    c = 0\n",
    "    for items in topN_items_per_user:\n",
    "        s += float(np.sum(1.0 - (item_popularity[items] / max_pop)))\n",
    "        c += len(items)\n",
    "    return s / c if c else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Popularity exposure / buckets\n",
    "# -----------------------------\n",
    "def compute_item_popularity(rating_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Popularity per item = #users with a nonzero rating.\n",
    "    Args:\n",
    "        rating_matrix: shape (num_users, num_items), zeros = unrated.\n",
    "    Returns:\n",
    "        1D int64 array of length num_items.\n",
    "    \"\"\"\n",
    "    # np.count_nonzero along axis=0 returns int64 on NumPy; be explicit.\n",
    "    return np.count_nonzero(rating_matrix, axis=0).astype(np.int64)\n",
    "\n",
    "\n",
    "def assign_popularity_buckets(item_popularity: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Assign each item to a popularity bucket: 0=low, 1=medium, 2=high using tertiles.\n",
    "    Thresholds are the 33.33% and 66.66% percentiles of item_popularity.\n",
    "\n",
    "    Notes:\n",
    "      - Items == q33 → low; q33 < pop ≤ q66 → medium; pop > q66 → high.\n",
    "      - If all items have identical popularity, everything ends up in bucket 0 (low),\n",
    "        which is fine and makes the split explicit.\n",
    "\n",
    "    Args:\n",
    "        item_popularity: 1D array length num_items.\n",
    "    Returns:\n",
    "        1D int array length num_items with values in {0,1,2}.\n",
    "    \"\"\"\n",
    "    if item_popularity.size == 0:\n",
    "        return item_popularity.astype(int)\n",
    "\n",
    "    q33, q66 = np.percentile(item_popularity, [33.33, 66.66])\n",
    "    buckets = np.zeros_like(item_popularity, dtype=int)\n",
    "\n",
    "    # (q33, q66] -> 1 ; (> q66) -> 2 ; else -> 0\n",
    "    mid_mask  = (item_popularity > q33) & (item_popularity <= q66)\n",
    "    high_mask = (item_popularity > q66)\n",
    "\n",
    "    buckets[mid_mask]  = 1\n",
    "    buckets[high_mask] = 2\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def fraction_by_popularity_bucket(topN_items: np.ndarray,\n",
    "                                  popularity_buckets: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the fraction of (low, medium, high) items within a single top-N list.\n",
    "\n",
    "    Args:\n",
    "        topN_items: 1D array of item indices for one user (length N).\n",
    "        popularity_buckets: 1D array (len=num_items) with values {0,1,2}.\n",
    "    Returns:\n",
    "        1D float array of length 3 with fractions summing to 1 (or zeros if empty).\n",
    "    \"\"\"\n",
    "    n = int(len(topN_items))\n",
    "    if n == 0:\n",
    "        return np.zeros(3, dtype=float)\n",
    "\n",
    "    # Map items -> buckets, then bincount over {0,1,2}\n",
    "    b = popularity_buckets[topN_items]\n",
    "    counts = np.bincount(b, minlength=3).astype(float)\n",
    "    return counts / float(n)\n",
    "\n",
    "\n",
    "def popularity_exposure_gap(topN_items_all_users, popularity_buckets: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Exposure gap across all users’ recommendations:\n",
    "        gap = share(low) - share(high)\n",
    "      where share(x) = (#items in bucket x) / (total #recommended items)\n",
    "\n",
    "    Positive gap => more long-tail exposure (more low-pop items recommended overall).\n",
    "\n",
    "    Args:\n",
    "        topN_items_all_users: iterable of 1D arrays (one per user) of item indices.\n",
    "        popularity_buckets: 1D array (len=num_items) with values {0,1,2}.\n",
    "    Returns:\n",
    "        float in [-1, 1]. Returns 0.0 if there are no recommended items at all.\n",
    "    \"\"\"\n",
    "    low = 0.0\n",
    "    high = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    for items in topN_items_all_users:\n",
    "        if len(items) == 0:\n",
    "            continue\n",
    "        b = popularity_buckets[items]\n",
    "        # Count low (0) and high (2); medium (1) is ignored for the gap.\n",
    "        low  += float(np.sum(b == 0))\n",
    "        high += float(np.sum(b == 2))\n",
    "        total += float(len(items))\n",
    "\n",
    "    if total == 0.0:\n",
    "        return 0.0\n",
    "    return (low / total) - (high / total)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Q·Q^T precomputation with memory-awareness and caching\n",
    "# -----------------------------\n",
    "def maybe_precompute_Q_dot(\n",
    "    Q: np.ndarray,\n",
    "    max_frac_avail: float = 0.25,   # use up to 25% of currently available RAM\n",
    "    abs_cap_gib: float = 2.0        # but never exceed this hard cap (GiB)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return Q_dot = Q @ Q.T if affordable under memory limits; otherwise, return\n",
    "    an empty (0,0) sentinel. Caches the result per-`id(Q)` to avoid recomputing\n",
    "    across iterations when Q hasn't changed.\n",
    "\n",
    "    Rules:\n",
    "      • bytes_needed = I*I*Q.dtype.itemsize\n",
    "      • allowed = min(abs_cap_gib, max_frac_avail * psutil.virtual_memory().available)\n",
    "      • If bytes_needed <= allowed -> compute; else -> return empty.\n",
    "    \"\"\"\n",
    "    I = int(Q.shape[0])\n",
    "    if I == 0:\n",
    "        return np.empty((0, 0), dtype=Q.dtype)\n",
    "\n",
    "    # --- simple cache keyed by object identity + shape + dtype ---\n",
    "    cache = maybe_precompute_Q_dot.__dict__.setdefault(\"_cache\", {})\n",
    "    key = (id(Q), I, Q.dtype.str)\n",
    "    hit = cache.get(\"key\") == key\n",
    "    if hit and \"Q_dot\" in cache:\n",
    "        return cache[\"Q_dot\"]\n",
    "\n",
    "    # --- how much can we use? ---\n",
    "    GIB = float(1 << 30)\n",
    "    allowed = abs_cap_gib * GIB\n",
    "    try:\n",
    "        import psutil  # optional\n",
    "        avail = float(psutil.virtual_memory().available)\n",
    "        allowed = min(allowed, max_frac_avail * avail)\n",
    "    except Exception:\n",
    "        # psutil not available; fall back to hard cap only\n",
    "        pass\n",
    "\n",
    "    bytes_needed = float(I) * float(I) * float(Q.dtype.itemsize)\n",
    "\n",
    "    if bytes_needed <= allowed:\n",
    "        Q_dot = Q.dot(Q.T)   # dtype preserved\n",
    "        cache[\"key\"] = key\n",
    "        cache[\"Q_dot\"] = Q_dot\n",
    "        try:\n",
    "            import math\n",
    "            logger.info(f\"[Q_dot] precomputed {I}×{I} ({bytes_needed/ (1<<20):.1f} MiB).\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        return Q_dot\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"[Q_dot] skipped: need {bytes_needed/(1<<20):.1f} MiB, \"\n",
    "            f\"allowing ≤ {allowed/(1<<20):.1f} MiB.\"\n",
    "        )\n",
    "        cache[\"key\"] = key\n",
    "        cache[\"Q_dot\"] = np.empty((0, 0), dtype=Q.dtype)  # sentinel\n",
    "        return cache[\"Q_dot\"]\n",
    "\n",
    "def clear_qdot_cache():\n",
    "    maybe_precompute_Q_dot.__dict__.pop(\"_cache\", None)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DataFrame utility\n",
    "# -----------------------------\n",
    "def safe_concat(df_list, ignore_index: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate DataFrames while dropping columns that are all-NaN in each input\n",
    "    (helps when some per-iteration frames have sparse diagnostics).\n",
    "\n",
    "    Args:\n",
    "        df_list: iterable of DataFrames.\n",
    "        ignore_index: passed to pd.concat.\n",
    "    Returns:\n",
    "        Concatenated DataFrame. If df_list is empty, returns an empty DataFrame.\n",
    "    \"\"\"\n",
    "    if not df_list:\n",
    "        return pd.DataFrame()\n",
    "    cleaned = [df.dropna(axis=1, how='all') for df in df_list]\n",
    "    return pd.concat(cleaned, ignore_index=ignore_index)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure LAMBDA_SELECT is defined before use\n",
    "try:\n",
    "    LAMBDA_SELECT\n",
    "except NameError:\n",
    "    LAMBDA_SELECT = None  # means “use lambda_value”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fffbd28-67ff-4a98-89f7-4a3efb5bbd65",
   "metadata": {},
   "source": [
    "# Main experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033749f-bc0e-467d-9df6-ffccec63dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lambda_value, strategy_input=None, return_results=False, seed=None, n_iter=num_iter,\n",
    "         results_folder=None, dataset='100k', freeze_Q=False, theta=0.0, neighbor=NEIGHBOR,\n",
    "         RECOMPUTE_W_EACH_ITER=True):\n",
    "    global logger\n",
    "    # Selection λ source: prefer global LAMBDA_SELECT; else fall back to training λ\n",
    "    sel_lambda_source = (LAMBDA_SELECT if ('LAMBDA_SELECT' in globals() and LAMBDA_SELECT is not None)\n",
    "                         else lambda_value)\n",
    "\n",
    "    # 1) Output dirs\n",
    "    if results_folder is None:\n",
    "        results_folder = f\"Results_{neighbor}/seeds_results\"\n",
    "    pop_folder = f\"Results_{neighbor}/Popularity_Buckets\"\n",
    "\n",
    "    # 2) RNG\n",
    "    rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n",
    "    logger.info(f\"[seed={seed}] first rng draw = {rng.random():.6f}\")\n",
    "\n",
    "    # 3) FS setup\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    os.makedirs(pop_folder, exist_ok=True)\n",
    "\n",
    "    # 4) Data\n",
    "    data_M, _ = load_movielens(dataset)\n",
    "    rate = data_M  # already dense DataFrame\n",
    "\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"STARTING EXAL EXPERIMENT with freeze_Q=\"\n",
    "                f\"{freeze_Q}, lambda_train={lambda_value}, lambda_select={sel_lambda_source}, \"\n",
    "                f\"strategy={strategy_input}, dataset={dataset}, seed={seed}, n_iter={n_iter}, \"\n",
    "                f\"neighbor={neighbor}, theta={theta}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"{'fixed Q during AL (ablation)' if freeze_Q else 'EMF updates per iteration (paper-faithful)'}\")\n",
    "\n",
    "    fixed_test = None\n",
    "\n",
    "    # Split for cold-start AL: tiny train, held-out test, remaining pool\n",
    "    train_init, test_init, pool_init, test_user = split(\n",
    "        rate,\n",
    "        num_test_users=fixed_test,\n",
    "        num_train_ratings=3,\n",
    "        num_test_ratings=20,\n",
    "        min_pool_size=10,\n",
    "        rng=rng\n",
    "    )\n",
    "\n",
    "    # Quick sanity for first 5 users\n",
    "    logger.info(\"==== Data Split Check for First 5 Test Users ====\")\n",
    "    for u in test_user[:5]:\n",
    "        train_idx = np.where(train_init[u] != 0)[0]\n",
    "        test_idx = np.where(test_init[u] != 0)[0]\n",
    "        pool_idx = np.where(pool_init[u] != 0)[0]\n",
    "        logger.info(f\"User {u:3d}: train={len(train_idx)}, test={len(test_idx)}, pool={len(pool_idx)}\")\n",
    "    logger.info(\"==============================================\")\n",
    "\n",
    "    # Item stats for selection/metrics\n",
    "    lR = calc_avg(train_init)\n",
    "    item_popularity = compute_item_popularity(train_init)\n",
    "    popularity_buckets = assign_popularity_buckets(item_popularity)\n",
    "\n",
    "    # Strategy list\n",
    "    if strategy_input:\n",
    "        strategies = [strategy_input]\n",
    "    else:\n",
    "        strategies = [\n",
    "            'EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max', 'KARIMI',\n",
    "            'Uncertainty', 'Random', 'HighestPred', 'HighestConfidence', 'HighestVar'\n",
    "        ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 9) Run each strategy\n",
    "    for strategy in strategies:\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"STRATEGY: {strategy}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "\n",
    "        # Reset state per strategy for fair comparison\n",
    "        rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n",
    "        train = np.copy(train_init)\n",
    "        test = np.copy(test_init)\n",
    "        pool = np.copy(pool_init)\n",
    "\n",
    "        # Per-iteration metrics table\n",
    "        evolution = pd.DataFrame(columns=[\"Iteration\", \"MAE\", \"MEP\", \"MER\", \"F-Score\", \"MAP\"])\n",
    "\n",
    "        # Decoupled λ: training vs. selection\n",
    "        lamda_train  = float(lambda_value)\n",
    "        lamda_select = float(sel_lambda_source)\n",
    "        logger.info(f\"Running {strategy} with lamda_train={lamda_train}, lamda_select={lamda_select}\")\n",
    "\n",
    "        # Init EMF + explainability\n",
    "        W0 = calc_exp(train_init, neighbor=neighbor, theta=theta)\n",
    "        P_init, Q_init, _, _, expl = initialize_model(\n",
    "            train_init, test_init, rng,\n",
    "            steps=INIT_STEPS, alpha=ALPHA_INIT, beta=BETA,\n",
    "            K=K, neighbor=neighbor, theta=theta, lamda=lamda_train,\n",
    "            precomputed_W=W0\n",
    "        )\n",
    "\n",
    "        # W sparsity snapshot\n",
    "        if (lamda_train > 0) or (lamda_select > 0):\n",
    "            logger.info(\"\\n[Explainability Matrix Sparsity Check]\")\n",
    "            logger.info(\"Initial explainable items per user (first 10 users):\")\n",
    "            for u in range(min(10, expl.shape[0])):\n",
    "                num_expl = int(np.sum(expl[u] > 0))\n",
    "                total_items = expl.shape[1]\n",
    "                perc = 100.0 * num_expl / total_items\n",
    "                logger.info(f\"  User {u:2d}: {num_expl:4d} / {total_items} items explainable ({perc:.2f}%)\")\n",
    "            logger.info(\"-\" * 55)\n",
    "        else:\n",
    "            logger.info(\"[Explainability] λ_train==0 and λ_select==0 → W only used for metrics.\")\n",
    "\n",
    "        # Working copies\n",
    "        P, Q = np.copy(P_init), np.copy(Q_init)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            train = train.astype(np.float32, copy=False)\n",
    "            P     = P.astype(np.float32, copy=False)\n",
    "            Q     = Q.astype(np.float32, copy=False)\n",
    "            expl  = expl.astype(np.float32, copy=False)\n",
    "\n",
    "        train_mae_list, test_mae_list = [], []\n",
    "\n",
    "        # 11) AL loop\n",
    "        for iteration in tqdm.tqdm(range(n_iter), desc=f\"[{strategy}] AL Iter\"):\n",
    "            logger.info(f\"\\n--- Iteration {iteration} ---\")\n",
    "\n",
    "            # Exact lR this iter\n",
    "            item_sums   = train.sum(axis=0)\n",
    "            item_counts = (train != 0).sum(axis=0)\n",
    "            global_sum  = float(item_sums.sum())\n",
    "            global_cnt  = int(item_counts.sum())\n",
    "            global_mean = global_sum / max(global_cnt, 1)\n",
    "\n",
    "            lR = np.full(train.shape[1], global_mean, dtype=train.dtype)\n",
    "            mask_pos = item_counts > 0\n",
    "            lR[mask_pos] = item_sums[mask_pos] / item_counts[mask_pos]\n",
    "\n",
    "            # Recompute W if needed\n",
    "            if RECOMPUTE_W_EACH_ITER:\n",
    "                expl = calc_exp(train, neighbor=neighbor, theta=theta)\n",
    "\n",
    "            # Predictions (+ optional Q·Qᵀ cache)\n",
    "            eR = P.dot(Q.T)\n",
    "            Q_dot = maybe_precompute_Q_dot(Q)\n",
    "\n",
    "            # Selection + online user update\n",
    "            for u in tqdm.tqdm(test_user, desc=\" Users\", leave=False):\n",
    "                test_items = np.where(test[u, :] != 0)[0]\n",
    "\n",
    "                if strategy == 'EXAL-Min':\n",
    "                    j = active_selection_exal_min(\n",
    "                        u, test_items, pool, eR, lR, Q_dot, Q, expl, ALPHA_RETRAIN, lamda_select, K\n",
    "                    )\n",
    "                elif strategy == 'EXAL-Max':\n",
    "                    j = active_selection_exal_max(\n",
    "                        u, test_items, pool, eR, lR, Q_dot, Q, expl, ALPHA_RETRAIN, lamda_select, K\n",
    "                    )\n",
    "                elif strategy == 'EXAL-Min-Max':\n",
    "                    j = active_selection_exal_max_min(\n",
    "                        u, test_items, pool, eR, lR, Q_dot, Q, expl,\n",
    "                        ALPHA_RETRAIN, lamda_select, iteration, switch_point=SWITCH, K=K\n",
    "                    )\n",
    "\n",
    "                elif strategy == 'KARIMI':\n",
    "                    j = active_selection_karimi(\n",
    "                        u, test_items, pool, eR, lR, Q_dot, Q, ALPHA_RETRAIN, K\n",
    "                    )\n",
    "                elif strategy == 'Random':\n",
    "                    j = select_random(u, pool, rng.random())\n",
    "                elif strategy == 'Uncertainty':\n",
    "                    j = active_selection_uncertainty(u, pool, eR, midpoint=3.0)\n",
    "                elif strategy == 'HighestPred':\n",
    "                    j = active_selection_highest_pred(u, pool, eR)\n",
    "                elif strategy == 'HighestConfidence':\n",
    "                    j = active_selection_highest_confidence(u, pool, eR, midpoint=3.0)\n",
    "                elif strategy == 'HighestVar':\n",
    "                    j = active_selection_highest_variance(u, pool, eR)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "                if j >= 0:\n",
    "                    # Move pick from pool→train\n",
    "                    r = pool[u, j]\n",
    "                    train[u, j] = r\n",
    "                    pool[u, j]  = 0\n",
    "\n",
    "                    # O(1) updates for lR/global stats\n",
    "                    item_sums[j]   += r\n",
    "                    item_counts[j] += 1\n",
    "                    global_sum     += r\n",
    "                    global_cnt     += 1\n",
    "                    lR[j] = item_sums[j] / item_counts[j]\n",
    "\n",
    "                    # Fast per-user W refresh if not rebuilding full W\n",
    "                    if (lamda_train > 0 or lamda_select > 0) and not RECOMPUTE_W_EACH_ITER:\n",
    "                        expl[u, :] = calc_exp_row(train, u, neighbor=neighbor, theta=theta)\n",
    "\n",
    "                    # Online update for user u\n",
    "                    P[u] = (\n",
    "                        retrain_online_exp_gpu(u, train, P, Q, expl, ALPHA_RETRAIN, BETA, K, ONLINE_STEP, lamda_train)\n",
    "                        if USE_CUDA else\n",
    "                        retrain_online_exp(u, train, P, Q, expl, ALPHA_RETRAIN, BETA, K, ONLINE_STEP, lamda_train)\n",
    "                    )\n",
    "\n",
    "            # Optional EMF pass (updates Q and P)\n",
    "            if not freeze_Q:\n",
    "                P, Q, _ = EMF_with_explainability(train, P, Q, K, expl, lamda_train, ONLINE_STEP, ALPHA_RETRAIN, BETA)\n",
    "\n",
    "            # Metrics (after updates)\n",
    "            eR = P.dot(Q.T)\n",
    "\n",
    "            # MAE\n",
    "            mae_train = np.nanmean(np.abs(eR[train != 0] - train[train != 0]))\n",
    "            mae_test  = np.nanmean(np.abs(eR[test  != 0] - test [test  != 0]))\n",
    "            train_mae_list.append(mae_train)\n",
    "            test_mae_list.append(mae_test)\n",
    "            logger.info(f\"[{strategy}] Iter {iteration}: TRAIN MAE={mae_train:.4f}, TEST MAE={mae_test:.4f}\")\n",
    "\n",
    "            # W diagnostics\n",
    "            num_expl_nonzero = np.sum(expl > 0)\n",
    "            total_entries    = expl.shape[0] * expl.shape[1]\n",
    "            percent_nonzero  = 100 * num_expl_nonzero / total_entries\n",
    "            logger.info(f\"[W Sparsity] Non-zero W entries: {num_expl_nonzero}/{total_entries} ({percent_nonzero:.4f}%)\")\n",
    "\n",
    "            percent_exp_user = np.sum(expl > 0, axis=1) / expl.shape[1]\n",
    "            mean_cov = np.mean(percent_exp_user) * 100\n",
    "            top5     = np.sort(percent_exp_user)[-5:] * 100\n",
    "            bot5     = np.sort(percent_exp_user)[:5] * 100\n",
    "            logger.info(f\"[W Coverage] Mean explainable items/user: {mean_cov:.2f}%\")\n",
    "            logger.info(f\"Top 5 users w/ most explainable items: {top5}\")\n",
    "            logger.info(f\"Bottom 5 users w/ least explainable items: {bot5}\")\n",
    "\n",
    "            # Mask seen items for ranking metrics\n",
    "            mask = (train != 0) | (pool != 0)\n",
    "            eR_masked = eR.copy()\n",
    "            eR_masked[mask] = np.float32(-np.inf) if eR_masked.dtype == np.float32 else -np.inf\n",
    "\n",
    "            # Explainability@N\n",
    "            MEP, total_expl, total_n = calculate_MEP(eR_masked, expl, test_user, TopN)\n",
    "            MER = calculate_MER(eR_masked, expl, test_user, TopN)\n",
    "            F   = 2*(MEP*MER)/(MEP+MER) if (MEP+MER) > 0 else 0.0\n",
    "\n",
    "            # Ranking\n",
    "            MAPv = calculate_MAP(eR_masked, test, test_user, TopN)\n",
    "            ndcg = calculate_ndcg(eR_masked, test, test_user, TopN, graded=True)\n",
    "\n",
    "            # Build top-N per user for exposure/novelty metrics\n",
    "            topN_items, all_top = [], []\n",
    "            for u in test_user:\n",
    "                s = eR[u].copy()\n",
    "                s[mask[u]] = -np.inf\n",
    "                valid = ~np.isneginf(s)\n",
    "                if np.any(valid):\n",
    "                    v_idx = np.where(valid)[0]\n",
    "                    top_local = v_idx[np.argsort(s[v_idx])[-TopN:]][::-1]\n",
    "                else:\n",
    "                    top_local = np.empty(0, dtype=np.int64)\n",
    "                topN_items.append(top_local)\n",
    "                all_top.extend(top_local)\n",
    "\n",
    "            # Beyond-accuracy\n",
    "            num_items      = train.shape[1]\n",
    "            ic             = calculate_item_coverage(topN_items, num_items)\n",
    "            gini_conc      = calculate_gini_index(topN_items, num_items)\n",
    "            diversity_1mG  = 1.0 - gini_conc\n",
    "            arp            = calculate_ARP(topN_items, item_popularity)\n",
    "            novelty_log2   = float(calculate_novelty_log2(topN_items, item_popularity))\n",
    "            novelty_efd    = calculate_novelty_EFD(topN_items, item_popularity, num_users=train.shape[0])\n",
    "            novelty_epc    = calculate_novelty_EPC(topN_items, item_popularity)\n",
    "\n",
    "            # Popularity mix / exposure\n",
    "            frac_pop       = fraction_by_popularity_bucket(np.array(all_top), popularity_buckets)\n",
    "            frac_bias      = float(frac_pop[0] - frac_pop[2])\n",
    "            exposure_gap   = popularity_exposure_gap(topN_items, popularity_buckets)\n",
    "\n",
    "            # MAE by popularity bucket (diagnostic)\n",
    "            mae_high, mae_low = [], []\n",
    "            for u, recs in zip(test_user, topN_items):\n",
    "                high = [i for i in recs if popularity_buckets[i] == 2 and test[u, i] != 0]\n",
    "                low  = [i for i in recs if popularity_buckets[i] == 0 and test[u, i] != 0]\n",
    "                if high:\n",
    "                    mae_high.append(np.mean(np.abs(eR[u, high] - test[u, high])))\n",
    "                if low:\n",
    "                    mae_low.append(np.mean(np.abs(eR[u, low] - test[u, low])))\n",
    "            mean_high = np.nan if not mae_high else float(np.mean(mae_high))\n",
    "            mean_low  = np.nan if not mae_low  else float(np.mean(mae_low))\n",
    "            mae_bias  = mean_low - mean_high\n",
    "\n",
    "            # Persist popularity fractions (per strategy/λs)\n",
    "            pop_df = pd.DataFrame({\n",
    "                \"Iteration\":[iteration],\n",
    "                \"Frac_Low\": [frac_pop[0]],\n",
    "                \"Frac_Med\": [frac_pop[1]],\n",
    "                \"Frac_High\":[frac_pop[2]],\n",
    "            })\n",
    "            pop_key = f\"{strategy}_lambdaTrain_{lambda_value}_lambdaSel_{sel_lambda_source}_{dataset}\"\n",
    "            pop_path= os.path.join(pop_folder, f\"{pop_key}{'_seed_'+str(seed) if seed else ''}.csv\")\n",
    "            if os.path.exists(pop_path):\n",
    "                old = pd.read_csv(pop_path)\n",
    "                pop_df = pd.concat([old, pop_df], ignore_index=True)\n",
    "            pop_df.to_csv(pop_path, index=False)\n",
    "\n",
    "            # Row for main CSV\n",
    "            iteration_df = pd.DataFrame({\n",
    "                \"Iteration\":          [iteration],\n",
    "                \"MAE\":                [mae_test],\n",
    "                \"Train_MAE\":          [mae_train],\n",
    "                \"Overfit_Gap\":        [mae_test - mae_train],\n",
    "                \"MEP\":                [MEP],\n",
    "                \"MER\":                [MER],\n",
    "                \"F-Score\":            [F],\n",
    "                \"MAP\":                [MAPv],\n",
    "                \"NDCG\":               [ndcg],\n",
    "                \"IC_ItemCoverage\":    [ic],\n",
    "                \"Gini_Concentration\": [gini_conc],\n",
    "                \"Diversity_1mGini\":   [diversity_1mG],\n",
    "                \"ARP\":                [arp],\n",
    "                \"Novelty_Log2\":       [novelty_log2],\n",
    "                \"Novelty_EFD\":        [novelty_efd],\n",
    "                \"Novelty_EPC\":        [novelty_epc],\n",
    "                \"ExposureGap_LminusH\":[exposure_gap],\n",
    "                \"Frac_Bias\":          [frac_bias],\n",
    "                \"MAE_HighPop\":        [mean_high],\n",
    "                \"MAE_LowPop\":         [mean_low],\n",
    "                \"MAE_Pop_Bias\":       [mae_bias],\n",
    "                \"Total_Explained\":    [total_expl],\n",
    "                \"Total_Candidates\":   [total_n]\n",
    "            })\n",
    "            evolution = safe_concat([evolution, iteration_df], ignore_index=True)\n",
    "            logger.info(\"Iteration stats:\\n\" + str(iteration_df))\n",
    "\n",
    "        # Save per-strategy results\n",
    "        key = f\"{strategy}_lambdaTrain_{lambda_value}_lambdaSel_{sel_lambda_source}_{dataset}\"\n",
    "        results[key] = evolution\n",
    "        if not return_results:\n",
    "            path = os.path.join(results_folder, f\"{key}{'_seed_'+str(seed) if seed else ''}.csv\")\n",
    "            evolution.to_csv(path, index=False)\n",
    "            logger.info(f\"Saved results to {path}\")\n",
    "\n",
    "            # Final iter log snapshot\n",
    "            logger.info(f\"Iteration {iteration} complete:\")\n",
    "            logger.info(f\"  - Train MAE: {mae_train:.4f}\")\n",
    "            logger.info(f\"  - Test MAE: {mae_test:.4f}\")\n",
    "            logger.info(f\"  - MEP: {MEP:.4f}, MER: {MER:.4f}, F-Score: {F:.4f}\")\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"EXPERIMENT COMPLETE\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "    return results if return_results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da710b-55c3-4b08-8d4e-2bc2527045ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multi_seed_experiment(lambda_value, strategy_input=None, seeds=None, n_iter=num_iter,\n",
    "                          results_folder=None, dataset='100k', freeze_Q=False,\n",
    "                          theta=0.0, neighbor=NEIGHBOR, recompute_w_each_iter=True):    \n",
    "    \"\"\"\n",
    "    Run active learning experiments for multiple seeds and average results.\n",
    "    \"\"\"\n",
    "    # NEW: selection λ source (global override falls back to lambda_value)\n",
    "    sel_lambda_source = (LAMBDA_SELECT if ('LAMBDA_SELECT' in globals() and LAMBDA_SELECT is not None)\n",
    "                         else lambda_value)\n",
    "\n",
    "    # 1. Set default folders for results\n",
    "    if results_folder is None:\n",
    "        results_folder = f\"Results_{neighbor}\"\n",
    "    seeds_folder = os.path.join(results_folder, \"seeds_results\")\n",
    "    pop_folder = os.path.join(results_folder, \"Popularity_Buckets\")\n",
    "\n",
    "    # 2. Default seeds if not provided\n",
    "    if seeds is None:\n",
    "        seeds = [42, 101, 202, 303, 404, 505, 606, 707, 808, 909]\n",
    "\n",
    "    # 3. Ensure output directories exist\n",
    "    os.makedirs(seeds_folder, exist_ok=True)\n",
    "    os.makedirs(pop_folder, exist_ok=True)\n",
    "    logger.info(f\"Running multi-seed experiment with seeds: {seeds}\")\n",
    "\n",
    "    # 4. Prepare result storage\n",
    "    results_all = {}  \n",
    "\n",
    "    # 5. Run experiment for each seed\n",
    "    for seed in seeds:\n",
    "        logger.info(f\"Seed {seed}: first rng draw = {np.random.default_rng(seed).random():.6f}\")\n",
    "\n",
    "        # ---- GPU/Cache hygiene between seeds ----\n",
    "        try:\n",
    "            retrain_online_exp_gpu_clear_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            clear_qdot_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Run single-seed experiment\n",
    "        results = main(\n",
    "            lambda_value,\n",
    "            strategy_input=strategy_input,\n",
    "            return_results=True,\n",
    "            seed=seed,\n",
    "            n_iter=n_iter,\n",
    "            results_folder=seeds_folder,\n",
    "            dataset=dataset,\n",
    "            theta=theta,\n",
    "            neighbor=neighbor,\n",
    "            freeze_Q=freeze_Q,\n",
    "            RECOMPUTE_W_EACH_ITER=recompute_w_each_iter   \n",
    "        )\n",
    "\n",
    "        # Save and collect results\n",
    "        for strategy_key, df in results.items():\n",
    "            out_path = os.path.join(seeds_folder, f\"{strategy_key}_seed_{seed}.csv\")\n",
    "            df.to_csv(out_path, index=False)\n",
    "            logger.info(f\"Saved: {out_path}\")\n",
    "\n",
    "            if strategy_key not in results_all:\n",
    "                results_all[strategy_key] = []\n",
    "            results_all[strategy_key].append(df.copy())\n",
    "\n",
    "    # 6. Average results across seeds for each strategy\n",
    "    for strategy_key, dfs in results_all.items():\n",
    "        concat_df = pd.concat(dfs, keys=range(len(dfs)), names=['Seed', 'Row'])\n",
    "        avg_df = concat_df.groupby('Iteration').mean(numeric_only=True).reset_index()\n",
    "        avg_csv = os.path.join(seeds_folder, f\"AVG_{strategy_key}.csv\")\n",
    "        avg_df.to_csv(avg_csv, index=False)\n",
    "        logger.info(f\"Averaged results for {strategy_key} saved to {avg_csv}\")\n",
    "\n",
    "    logger.info(f\"Multi-seed experiment completed. Averaged results in {seeds_folder}/.\")\n",
    "\n",
    "    # 7. Post-process and average popularity results for each strategy\n",
    "    strategies = [\n",
    "        'EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max',  'KARIMI',\n",
    "        'Uncertainty', 'Random', 'HighestPred', 'HighestConfidence', 'HighestVar'\n",
    "    ]\n",
    "\n",
    "    for strat in strategies:\n",
    "        # UPDATED pattern to include both lambdas (train/select)\n",
    "        pattern = os.path.join(\n",
    "            pop_folder,\n",
    "            f\"{strat}_lambdaTrain_{lambda_value}_lambdaSel_{sel_lambda_source}_{dataset}_seed_*.csv\"\n",
    "        )\n",
    "        files = [f for f in glob.glob(pattern) if not os.path.basename(f).startswith('AVG_')]\n",
    "\n",
    "        avg_file = os.path.join(\n",
    "            pop_folder,\n",
    "            f\"AVG_{strat}_lambdaTrain_{lambda_value}_lambdaSel_{sel_lambda_source}_{dataset}.csv\"\n",
    "        )\n",
    "\n",
    "        if len(files) == 1:\n",
    "            shutil.copyfile(files[0], avg_file)\n",
    "            logger.info(f\"[INFO] Only one Popularity file for {strat}: copied {files[0]} → {avg_file}\")\n",
    "        elif len(files) > 1:\n",
    "            dfs = [pd.read_csv(f) for f in files]\n",
    "            concat = pd.concat(dfs, keys=range(len(dfs)), names=['Seed', 'Row'])\n",
    "            avg_df = concat.groupby('Iteration').mean(numeric_only=True).reset_index()\n",
    "            avg_df.to_csv(avg_file, index=False)\n",
    "            logger.info(f\"[INFO] Averaged Popularity results for {strat} saved to {avg_file}\")\n",
    "        else:\n",
    "            logger.warning(f\"[WARN] No Popularity files found for {strat}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c4d36-729d-4aec-af6e-bf750edd6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    import argparse\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        force=True\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Jupyter safety: drop argv noise\n",
    "    if 'ipykernel' in sys.argv[0]:\n",
    "        sys.argv = [sys.argv[0]]\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Active Learning Experiment for MovieLens'\n",
    "    )\n",
    "\n",
    "    # λ for EMF training/updates (default = LAMBDA_TRAIN)\n",
    "    parser.add_argument('--lambda_value', type=float, default=LAMBDA_TRAIN,\n",
    "                        help='Lambda regularization parameter (λ) for EMF training/updates.')\n",
    "    # Optional λ for selection; falls back to --lambda_value\n",
    "    parser.add_argument('--lambda_select', type=float, default=None,\n",
    "                        help='Lambda used by EXAL selection rules (defaults to --lambda_value if not set).')\n",
    "\n",
    "    # Strategy choice (or \"all\")\n",
    "    _valid_strategies = [\n",
    "        'EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max', 'KARIMI',\n",
    "        'Uncertainty', 'Random', 'HighestPred', 'HighestConfidence', 'HighestVar',\n",
    "        'all'\n",
    "    ]\n",
    "    parser.add_argument('--strategy', type=str, default=None, choices=_valid_strategies,\n",
    "                        help='AL strategy (e.g., \"EXAL-Min\"). Use \"all\" to run every strategy.')\n",
    "\n",
    "    # Dataset + W parameters\n",
    "    parser.add_argument('--dataset', type=str, default='100k',\n",
    "                        choices=['100k', '1m'],\n",
    "                        help='MovieLens dataset.')\n",
    "    parser.add_argument('--theta', type=float, default=theta,\n",
    "                        help='Explainability threshold θ for W_{ui}.')\n",
    "    parser.add_argument('--neighbor', type=int, default=NEIGHBOR,\n",
    "                        help='Number of neighbors (k) for explainability matrix W.')\n",
    "\n",
    "    # Freeze Q during AL (ablation)\n",
    "    parser.add_argument('--freeze_Q', dest='freeze_Q', action='store_true',\n",
    "                        help='Freeze item factors Q during AL iterations.')\n",
    "    parser.add_argument('--no-freeze_Q', dest='freeze_Q', action='store_false',\n",
    "                        help='Allow iteration-end EMF updates to Q (default).')\n",
    "    parser.set_defaults(freeze_Q=False)\n",
    "\n",
    "    # Recompute W each iteration? (tri-state: auto / true / false)\n",
    "    group = parser.add_mutually_exclusive_group()\n",
    "    group.add_argument('--recompute_w_each_iter', dest='recompute_w_each_iter',\n",
    "                       action='store_true',\n",
    "                       help='Recompute W every iteration.')\n",
    "    group.add_argument('--no-recompute_w_each_iter', dest='recompute_w_each_iter',\n",
    "                       action='store_false',\n",
    "                       help='Do NOT recompute W every iteration.')\n",
    "    parser.set_defaults(recompute_w_each_iter=None)\n",
    "\n",
    "    # Parse CLI\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    lambda_v  = args.lambda_value\n",
    "    strategy  = args.strategy\n",
    "    dataset   = args.dataset\n",
    "    theta     = args.theta\n",
    "    freeze_Q  = args.freeze_Q\n",
    "    neighbor  = args.neighbor\n",
    "    \n",
    "    # Stash selection λ globally (keeps function signatures unchanged)\n",
    "    if args.lambda_select is not None:\n",
    "        LAMBDA_SELECT = args.lambda_select\n",
    "\n",
    "    # Normalize \"all\"/\"*\" -> None (run all strategies)\n",
    "    if strategy is not None and strategy.lower() in ('all', '*'):\n",
    "        strategy = None\n",
    "\n",
    "    # Auto default for recompute_w_each_iter: true if selection λ>0 (else false)\n",
    "    _sel_src = (LAMBDA_SELECT if LAMBDA_SELECT is not None else lambda_v)\n",
    "    if args.recompute_w_each_iter is None:\n",
    "        recompute_w_each_iter = (_sel_src > 0)\n",
    "        logger.info(f\"[Explainability] RECOMPUTE_W_EACH_ITER auto-set to {recompute_w_each_iter} \"\n",
    "                    f\"(λ_train={lambda_v}, λ_select={_sel_src}, θ={theta})\")\n",
    "    else:\n",
    "        recompute_w_each_iter = args.recompute_w_each_iter\n",
    "        logger.info(f\"[Explainability] RECOMPUTE_W_EACH_ITER explicitly set to {recompute_w_each_iter} \"\n",
    "                    f\"(λ_train={lambda_v}, λ_select={_sel_src}, θ={theta})\")\n",
    "\n",
    "    # Run\n",
    "    multi_seed_experiment(\n",
    "        lambda_v,\n",
    "        strategy_input=strategy,  \n",
    "        seeds=None,\n",
    "        n_iter=num_iter,\n",
    "        results_folder=None,\n",
    "        dataset=dataset,\n",
    "        freeze_Q=freeze_Q,\n",
    "        theta=theta,\n",
    "        neighbor=neighbor,\n",
    "        recompute_w_each_iter=recompute_w_each_iter\n",
    "    )\n",
    "\n",
    "    # Final cache cleanup\n",
    "    try:\n",
    "        retrain_online_exp_gpu_clear_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        clear_qdot_cache()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0827762-6716-41cd-9bd8-88fac49bc5cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921c016-97ab-499c-844f-a5bf9f2417ae",
   "metadata": {},
   "source": [
    "# PLOT the Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ff188",
   "metadata": {},
   "source": [
    "### Metrics Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc89483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# ---------- Logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------- Experiment selectors (EDIT THESE TO MATCH THE RUN YOU WANT TO PLOT) ----------\n",
    "LAMBDA_TRAIN  = 0.005     # 0.0 for MF; 0.005 for EMF\n",
    "LAMBDA_SELECT = 0.5\n",
    "dataset       = '100k'\n",
    "NEIGHBOR      = 20\n",
    "TopN          = 10       # your experiment used 10; change if needed\n",
    "SWITCH        = 5        # Min→Max switch guide if you want to show it\n",
    "\n",
    "neighbor = NEIGHBOR\n",
    "logger.info(f\"Generating enhanced plots for NEIGHBOR = {neighbor} | \"\n",
    "            f\"λ_train={LAMBDA_TRAIN}, λ_select={LAMBDA_SELECT}, dataset={dataset}, TopN={TopN}\")\n",
    "\n",
    "# ---------- Styles ----------\n",
    "styles = {\n",
    "    'KARIMI':               {'color': 'blue',      'marker': 'o', 'linestyle': '-'},\n",
    "    'Random':               {'color': 'green',     'marker': 's', 'linestyle': '-'},\n",
    "    'HighestVar':           {'color': 'cyan',      'marker': '^', 'linestyle': '-'},\n",
    "    'HighestPred':          {'color': 'magenta',   'marker': 'v', 'linestyle': '-'},\n",
    "    'HighestConfidence':    {'color': 'purple',    'marker': '>', 'linestyle': '-'},\n",
    "    'Uncertainty':          {'color': 'orange',    'marker': 'x', 'linestyle': '-'},\n",
    "    'EXAL-Min':             {'color': 'gold',      'marker': 'D', 'linestyle': '-'},\n",
    "    'EXAL-Max':             {'color': 'black',     'marker': '*', 'linestyle': '--'},\n",
    "    'EXAL-Min-Max':         {'color': 'red',       'marker': 'X', 'linestyle': '--'}\n",
    "}\n",
    "\n",
    "method_categories = {\n",
    "    'Baselines': ['KARIMI', 'Random', 'HighestVar', 'HighestPred','HighestConfidence','Uncertainty'],\n",
    "    'Original ExAL': ['EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max']\n",
    "}\n",
    "\n",
    "# ---------- Metrics to plot (must match iteration_df columns) ----------\n",
    "labels = {\n",
    "    'MAP':                 'Mean Average Precision (MAP) ↑ better',\n",
    "    'MEP':                 'Explainable Precision (MEP) ↑ better',\n",
    "    'MER':                 'Explainable Recall (MER) ↑ better',\n",
    "    'F-Score':             'Explainable F1 ↑ better',\n",
    "    'MAE':                 'Mean Absolute Error (MAE) ↓ better',\n",
    "}\n",
    "\n",
    "direction_info = {\n",
    "    'MAP':                 {'better': 'higher', 'arrow': '↑'},\n",
    "    'MEP':                 {'better': 'higher', 'arrow': '↑'},\n",
    "    'MER':                 {'better': 'higher', 'arrow': '↑'},\n",
    "    'F-Score':             {'better': 'higher', 'arrow': '↑'},\n",
    "    'MAE':                 {'better': 'lower',  'arrow': '↓'},\n",
    "}\n",
    "\n",
    "# ---------- Folders / files ----------\n",
    "results_folder = f\"Results_{neighbor}/seeds_results\"\n",
    "plot_folder    = f\"Results_{neighbor}/Plots_Enhanced_Metrics\"\n",
    "summary_folder = f\"Results_{neighbor}/summaries\"\n",
    "os.makedirs(plot_folder, exist_ok=True)\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "csv_files = {\n",
    "    method: os.path.join(\n",
    "        results_folder,\n",
    "        f\"AVG_{method}_lambdaTrain_{LAMBDA_TRAIN}_lambdaSel_{LAMBDA_SELECT}_{dataset}.csv\"\n",
    "    )\n",
    "    for method in styles\n",
    "}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def ensure_zero_row_generic(df, metric):\n",
    "    \"\"\"Guarantee Iteration==0 exists by duplicating first row if absent.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.sort_values('Iteration').reset_index(drop=True)\n",
    "    if (df['Iteration'] == 0).any():\n",
    "        return df\n",
    "    first = df.iloc[0].copy()\n",
    "    first['Iteration'] = 0\n",
    "    return (pd.DataFrame([first]).append(df, ignore_index=True)\n",
    "            .sort_values('Iteration').reset_index(drop=True))\n",
    "\n",
    "def ensure_zero_row(df, metric):\n",
    "    \"\"\"Disable zero-row synthesis for MAP/NDCG to avoid skewing start.\"\"\"\n",
    "    if metric in ('MAP', 'NDCG'):\n",
    "        return df.sort_values('Iteration').reset_index(drop=True)\n",
    "    return ensure_zero_row_generic(df, metric)\n",
    "\n",
    "def create_organized_legend(ax, dfs):\n",
    "    legend_elements = []\n",
    "    for category, methods in method_categories.items():\n",
    "        category_methods = [m for m in methods if m in dfs and m in styles]\n",
    "        if category_methods:\n",
    "            legend_elements.append(plt.Line2D([0], [0], color='none', label=f'─── {category} ───'))\n",
    "            for method in category_methods:\n",
    "                style = styles[method]\n",
    "                legend_elements.append(\n",
    "                    plt.Line2D([0], [0],\n",
    "                               color=style['color'],\n",
    "                               marker=style['marker'],\n",
    "                               linestyle=style['linestyle'],\n",
    "                               linewidth=2,\n",
    "                               markersize=7,\n",
    "                               label=method)\n",
    "                )\n",
    "    return ax.legend(handles=legend_elements, fontsize=22,\n",
    "                     loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "def highlight_best_performers(ax, dfs, metric):\n",
    "    if not dfs:\n",
    "        return None, None\n",
    "    finals = {m: df[metric].iloc[-1] for m, df in dfs.items() if not df.empty}\n",
    "    if not finals:\n",
    "        return None, None\n",
    "    is_lower_better = direction_info[metric]['better'] == 'lower'\n",
    "    best_m = min(finals, key=finals.get) if is_lower_better else max(finals, key=finals.get)\n",
    "    best_v = finals[best_m]\n",
    "    if best_m in dfs:\n",
    "        df = dfs[best_m]\n",
    "        ax.scatter(df['Iteration'].iloc[-1], best_v,\n",
    "                   s=150, facecolors='none', edgecolors='red', linewidths=4,\n",
    "                   label=f'Best: {best_m}')\n",
    "    return best_m, best_v\n",
    "\n",
    "def annotate_final_points(ax, dfs, metric, fontsize=22):\n",
    "    for m, df in dfs.items():\n",
    "        x = df['Iteration'].iloc[-1]\n",
    "        y = df[metric].iloc[-1]\n",
    "        ax.annotate(f\"{y:.4f}\", (x, y), xytext=(5, 0), textcoords='offset points', fontsize=fontsize)\n",
    "\n",
    "def analyze_trends(dfs, metric):\n",
    "    trends = {}\n",
    "    is_lower_better = direction_info[metric]['better'] == 'lower'\n",
    "    for method, df in dfs.items():\n",
    "        if len(df) < 2:\n",
    "            continue\n",
    "        df = df.sort_values('Iteration')\n",
    "        first_val = df[metric].iloc[0]\n",
    "        last_val  = df[metric].iloc[-1]\n",
    "        improvement = (first_val - last_val) if is_lower_better else (last_val - first_val)\n",
    "        trend = 'improving' if improvement > 0 else 'declining'\n",
    "        trends[method] = {'improvement': improvement, 'trend': trend,\n",
    "                          'first': first_val, 'last': last_val}\n",
    "    return trends\n",
    "\n",
    "def sanity_check_metric(metric='MAP'):\n",
    "    found = []\n",
    "    for method, path in csv_files.items():\n",
    "        if os.path.isfile(path):\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                if metric in df.columns and 'Iteration' in df.columns and not df.empty:\n",
    "                    df = df[['Iteration', metric]].dropna()\n",
    "                    if not df.empty:\n",
    "                        v = float(df[metric].iloc[-1])\n",
    "                        found.append((method, v, path))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed reading {method} from {path}: {e}\")\n",
    "    if not found:\n",
    "        logger.warning(f\"No files with metric '{metric}' found for current selectors.\")\n",
    "        return\n",
    "    hb = direction_info[metric]['better'] == 'higher'\n",
    "    found.sort(key=lambda x: x[1], reverse=hb)\n",
    "    logger.info(f\"=== sanity_check_metric({metric}) ===\")\n",
    "    for m, v, p in found:\n",
    "        logger.info(f\"{m:20s}  final={v:.5f}   {p}\")\n",
    "\n",
    "# ---------- Plot all metrics ----------\n",
    "best_rows = []\n",
    "\n",
    "for metric, ylabel in labels.items():\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    dfs = {}\n",
    "    min_val, max_val = np.inf, -np.inf\n",
    "\n",
    "    # Load\n",
    "    for method, path in csv_files.items():\n",
    "        if not os.path.isfile(path):\n",
    "            logger.debug(f\"Missing file for {method}: {path}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if metric not in df.columns or 'Iteration' not in df.columns:\n",
    "                logger.debug(f\"Metric '{metric}' not in file for {method}: {path}\")\n",
    "                continue\n",
    "            df = df[['Iteration', metric]].dropna()\n",
    "            df = df[np.isfinite(df[metric])]\n",
    "            if df.empty:\n",
    "                continue\n",
    "            df = ensure_zero_row(df, metric)\n",
    "            dfs[method] = df\n",
    "            min_val = min(min_val, df[metric].min())\n",
    "            max_val = max(max_val, df[metric].max())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading {method} from {path}: {e}\")\n",
    "\n",
    "    # Log what actually loaded + finals\n",
    "    loaded = list(dfs.keys())\n",
    "    logger.info(f\"[{metric}] loaded methods: {loaded}\")\n",
    "    if loaded:\n",
    "        finals = {m: float(dfs[m][metric].iloc[-1]) for m in loaded}\n",
    "        higher_better = (direction_info[metric]['better'] == 'higher')\n",
    "        finals_sorted = sorted(finals.items(), key=lambda kv: kv[1], reverse=higher_better)\n",
    "        logger.info(f\"[{metric}] final values:\")\n",
    "        for m, v in finals_sorted:\n",
    "            logger.info(f\"  {m:20s} {v:.5f}\")\n",
    "    else:\n",
    "        logger.warning(f\"No data for metric '{metric}' at NEIGHBOR={neighbor}\")\n",
    "        plt.close()\n",
    "        continue\n",
    "\n",
    "    # Axis range with a small margin\n",
    "    margin = 0.05 * (max_val - min_val) if max_val > min_val else 0.01\n",
    "    ylow, yhigh = min_val - margin, max_val + margin\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Optional vertical guide at the Min-Max switch\n",
    "    if SWITCH is not None:\n",
    "        try:\n",
    "            ax.axvline(SWITCH, linestyle=':', linewidth=2, alpha=0.6)\n",
    "            ax.text(SWITCH, yhigh, f\"  switch={SWITCH}\", va='top', ha='left', fontsize=10)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Draw curves\n",
    "    for method, df in dfs.items():\n",
    "        style = styles[method]\n",
    "        lw = 3.5 if 'EXAL' in method else 3.0\n",
    "        ms = 7\n",
    "        alpha = 1.0 if 'EXAL' in method else 0.9\n",
    "        ax.plot(df['Iteration'], df[metric],\n",
    "                label=method,\n",
    "                linewidth=lw,\n",
    "                marker=style['marker'],\n",
    "                color=style['color'],\n",
    "                linestyle=style['linestyle'],\n",
    "                markersize=ms,\n",
    "                alpha=alpha)\n",
    "\n",
    "    best_m, best_v = highlight_best_performers(ax, dfs, metric)\n",
    "    annotate_final_points(ax, dfs, metric, fontsize=10)\n",
    "\n",
    "    # Labels / title\n",
    "    plt.xlabel('Active Learning Iteration', fontsize=20, fontweight='bold')\n",
    "    plt.ylabel(ylabel, fontsize=20, fontweight='bold')\n",
    "\n",
    "    title_metric = ylabel  # already has (abbr) in labels dict\n",
    "    plt.title(\n",
    "        f\"{title_metric}\\n\"\n",
    "        f\"Dataset: MovieLens-{dataset.upper()} | λ_train={LAMBDA_TRAIN}, λ_select={LAMBDA_SELECT} \"\n",
    "        f\"| Top-N={TopN} | Neighbors={neighbor}\",\n",
    "        fontsize=17, fontweight='bold', pad=25\n",
    "    )\n",
    "\n",
    "    plt.grid(True, alpha=0.7, linestyle='-', linewidth=0.9)\n",
    "    plt.ylim(ylow, yhigh)\n",
    "    max_iter = max(int(df['Iteration'].max()) for df in dfs.values())\n",
    "    plt.xticks(range(0, max_iter + 1), fontsize=16)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    create_organized_legend(ax, dfs)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    base = os.path.join(\n",
    "        plot_folder,\n",
    "        f\"Enhanced_{metric}_lambdaTrain_{LAMBDA_TRAIN}_lambdaSelect_{LAMBDA_SELECT}_{dataset}_k{neighbor}\"\n",
    "    )\n",
    "    plt.savefig(base + \".png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.savefig(base + \".pdf\", bbox_inches='tight', facecolor='white')\n",
    "    plt.savefig(base + \".svg\", bbox_inches='tight', facecolor='white')\n",
    "    logger.info(f\"Saved plots: {base}.(png|pdf|svg)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Trend logging\n",
    "    trends = analyze_trends(dfs, metric)\n",
    "    logger.info(f\"\\n=== TRENDS: {metric} ===\")\n",
    "    for method, t in sorted(trends.items(), key=lambda x: x[1]['improvement'], reverse=True):\n",
    "        logger.info(f\"{method:20} {t['trend']:10} \"\n",
    "                    f\"({t['first']:.4f} → {t['last']:.4f}, Δ={t['improvement']:+.4f})\")\n",
    "\n",
    "    # Record best-per-metric row\n",
    "    if best_m is not None and best_v is not None:\n",
    "        best_rows.append({\n",
    "            'Metric': metric,\n",
    "            'Better': direction_info[metric]['better'],\n",
    "            'Best_Method': best_m,\n",
    "            'Best_FinalValue': float(best_v)\n",
    "        })\n",
    "\n",
    "# ---------- Save a compact “best methods” summary ----------\n",
    "if best_rows:\n",
    "    best_df = pd.DataFrame(best_rows)\n",
    "    out_csv = os.path.join(\n",
    "        summary_folder,\n",
    "        f\"best_methods_lambdaTrain_{LAMBDA_TRAIN}_lambdaSelect_{LAMBDA_SELECT}_{dataset}_k{neighbor}.csv\"\n",
    "    )\n",
    "    best_df.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"Saved best-per-metric summary: {out_csv}\")\n",
    "else:\n",
    "    logger.info(\"No best-per-metric summary generated (no data).\")\n",
    "\n",
    "# ---------- Quick one-shot check (run once if you like) ----------\n",
    "sanity_check_metric('MAP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33236916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Pareto plots (final iteration means): MAP vs MEP\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Where to save figures\n",
    "out_dir = Path(\"stat_results/dual\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Compute per-(Method,Condition) means from df_last\n",
    "def _final_means(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"Condition\",\"Method\",\"MAP\",\"MEP\"])\n",
    "    # drop inf/nan and group\n",
    "    d = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"MAP\",\"MEP\"]).copy()\n",
    "    g = d.groupby([\"Condition\",\"Method\"], as_index=False).agg(MAP=(\"MAP\",\"mean\"), MEP=(\"MEP\",\"mean\"))\n",
    "    # keep only methods in DISPLAY_ORDER and preserve that order\n",
    "    g[\"Method\"] = pd.Categorical(g[\"Method\"], categories=DISPLAY_ORDER, ordered=True)\n",
    "    g = g.sort_values([\"Condition\",\"Method\"]).reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "means = _final_means(df_last)\n",
    "\n",
    "# Pareto-efficiency (maximize both MAP and MEP)\n",
    "def pareto_mask(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return boolean mask of Pareto-efficient points for 2 objectives (maximize X and Y).\n",
    "    A point is efficient if no other point has X>= and Y>=, with at least one strict.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    keep = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "        dominated = ( (X >= X[i]) & (Y >= Y[i]) & ((X > X[i]) | (Y > Y[i])) )\n",
    "        dominated[i] = False\n",
    "        if dominated.any():\n",
    "            keep[i] = False\n",
    "    return keep\n",
    "\n",
    "def _plot_one_pareto(df_cond: pd.DataFrame, cond_label: str, filename: Path):\n",
    "    if df_cond.empty:\n",
    "        print(f\"[PARETO] No data for {cond_label}; skipped.\")\n",
    "        return\n",
    "\n",
    "    X = df_cond[\"MAP\"].to_numpy()\n",
    "    Y = df_cond[\"MEP\"].to_numpy()\n",
    "    mask = pareto_mask(X, Y)\n",
    "    frontier = df_cond[mask].sort_values([\"MAP\",\"MEP\"])  # tidy line\n",
    "\n",
    "    # Aesthetics: highlight ExALs\n",
    "    exal_set = {\"EXAL-Min\",\"EXAL-Max\",\"EXAL-Min-Max\"}\n",
    "\n",
    "    plt.figure(figsize=(7.6, 6.6))\n",
    "    for map_v, mep_v, method in zip(df_cond[\"MAP\"], df_cond[\"MEP\"], df_cond[\"Method\"]):\n",
    "        c  = \"#1f77b4\" if method in exal_set else \"#7f7f7f\"\n",
    "        mk = \"s\"       if method in exal_set else \"o\"\n",
    "        plt.scatter(map_v, mep_v, s=90, c=c, marker=mk, edgecolor=\"black\", linewidth=1.0, zorder=3, label=method)\n",
    "\n",
    "    # draw Pareto frontier\n",
    "    plt.plot(frontier[\"MAP\"], frontier[\"MEP\"], linestyle=\"-\", linewidth=2.2,\n",
    "             color=\"#d62728\", zorder=2, label=\"Pareto frontier\")\n",
    "\n",
    "    # labels near points (no duplicate legend clutter)\n",
    "    for map_v, mep_v, method in zip(df_cond[\"MAP\"], df_cond[\"MEP\"], df_cond[\"Method\"]):\n",
    "        plt.annotate(str(method), (map_v, mep_v), xytext=(5, 5), textcoords=\"offset points\", fontsize=11)\n",
    "\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.6, alpha=0.6)\n",
    "    plt.xlabel(\"MAP (final-iteration mean)\")\n",
    "    plt.ylabel(\"MEP (final-iteration mean)\")\n",
    "    plt.title(f\"Pareto: MAP vs MEP  —  {cond_label}\")\n",
    "\n",
    "    # Legend UNDER the plot\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    # deduplicate labels (scatter added many)\n",
    "    uniq = dict(zip(labels, handles))\n",
    "    plt.legend(uniq.values(), uniq.keys(),\n",
    "               loc=\"upper center\", bbox_to_anchor=(0.5, -0.18),\n",
    "               ncol=4, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.gcf().subplots_adjust(bottom=0.25)\n",
    "\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[PARETO] Saved: {filename}\")\n",
    "\n",
    "# Split and plot MF / EMF\n",
    "mf_means  = means[means[\"Condition\"] == \"MF\"].copy()\n",
    "emf_means = means[means[\"Condition\"] == \"EMF\"].copy()\n",
    "\n",
    "_plot_one_pareto(mf_means,  \"MF (λ_train=0, λ_select=0.5)\", out_dir / \"pareto_MF.png\")\n",
    "_plot_one_pareto(emf_means, \"EMF (λ_train=0.005, λ_select=0.5)\", out_dir / \"pareto_EMF.png\")\n",
    "\n",
    "# Optional: both side-by-side with legend below the whole figure\n",
    "def _side_by_side(mf_df, emf_df, filename: Path):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14.2, 6.6), sharex=False, sharey=False)\n",
    "    exal_set = {\"EXAL-Min\",\"EXAL-Max\",\"EXAL-Min-Max\"}\n",
    "\n",
    "    for ax, dfc, title in zip(\n",
    "        axes,\n",
    "        [mf_df, emf_df],\n",
    "        [\"MF (λ_train=0, λ_select=0.5)\", \"EMF (λ_train=0.005, λ_select=0.5)\"]\n",
    "    ):\n",
    "        if dfc.empty:\n",
    "            ax.axis(\"off\"); continue\n",
    "\n",
    "        X = dfc[\"MAP\"].to_numpy()\n",
    "        Y = dfc[\"MEP\"].to_numpy()\n",
    "        mask = pareto_mask(X, Y)\n",
    "        front = dfc[mask].sort_values([\"MAP\",\"MEP\"])\n",
    "\n",
    "        handles = []\n",
    "        labels  = []\n",
    "\n",
    "        for map_v, mep_v, method in zip(dfc[\"MAP\"], dfc[\"MEP\"], dfc[\"Method\"]):\n",
    "            c  = \"#1f77b4\" if method in exal_set else \"#7f7f7f\"\n",
    "            mk = \"s\"       if method in exal_set else \"o\"\n",
    "            h = ax.scatter(map_v, mep_v, s=90, c=c, marker=mk,\n",
    "                           edgecolor=\"black\", linewidth=1.0, zorder=3, label=method)\n",
    "            handles.append(h); labels.append(method)\n",
    "            ax.annotate(str(method), (map_v, mep_v), xytext=(5, 5),\n",
    "                        textcoords=\"offset points\", fontsize=11)\n",
    "\n",
    "        ax.plot(front[\"MAP\"], front[\"MEP\"], linestyle=\"-\", linewidth=2.2,\n",
    "                color=\"#d62728\", zorder=2, label=\"Pareto frontier\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.6, alpha=0.6)\n",
    "        ax.set_xlabel(\"MAP (final mean)\")\n",
    "        ax.set_ylabel(\"MEP (final mean)\")\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # One combined legend under the figure (dedup)\n",
    "    h1, l1 = axes[0].get_legend_handles_labels()\n",
    "    h2, l2 = axes[1].get_legend_handles_labels()\n",
    "    uniq = dict(zip(l1 + l2, h1 + h2))\n",
    "    fig.legend(uniq.values(), uniq.keys(),\n",
    "               loc=\"upper center\", bbox_to_anchor=(0.5, -0.06),\n",
    "               ncol=5, frameon=True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.18, wspace=0.10)\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[PARETO] Saved: {filename}\")\n",
    "\n",
    "_side_by_side(mf_means, emf_means, out_dir / \"pareto_MF_EMF_side_by_side.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef13e2",
   "metadata": {},
   "source": [
    "## t-test significance vs. Random baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d87306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Dual-condition stats (MF vs EMF) across ALL active-selection baselines\n",
    "# =========================\n",
    "import os, re, glob, warnings\n",
    "from typing import Dict, List, Optional, Tuple, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import t as student_t  # for one-/two-sided p and CI\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# Configuration (edit here)\n",
    "# =========================\n",
    "MF_EXPECTED  = (0.0,   0.5)   # (lambda_train, lambda_select)\n",
    "EMF_EXPECTED = (0.005, 0.5)\n",
    "LAMBDA_EPS   = 1e-9\n",
    "\n",
    "DEFAULT_DATASET = \"100k\"\n",
    "NEIGHBOR        = 20\n",
    "ALPHA           = 0.05\n",
    "MIN_SAMPLE_SIZE = 3\n",
    "\n",
    "# test tails and CI level\n",
    "# TAIL options: \"two-sided\" | \"less\" | \"greater\"\n",
    "# - For vs Random, keep two-sided (neutral)\n",
    "# - For DiD, set to \"less\" to test EMF > MF on higher-is-better metrics (MEP/MAP)\n",
    "TAIL_VSR  = \"two-sided\"\n",
    "TAIL_DID  = \"greater\"\n",
    "CI_LEVEL  = 0.95   # used for reporting two-sided 95% CI around the mean difference\n",
    "\n",
    "# Reduce serial-correlation inflation in multi-iteration windows by collapsing\n",
    "# within each seed first (avg of (method - random) within the window), then t-test across seeds.\n",
    "CLUSTER_BY_SEED = True\n",
    "\n",
    "METHODS_UNIVERSE = [\n",
    "    \"EXAL-Min\", \"EXAL-Max\", \"EXAL-Min-Max\",\n",
    "    \"KARIMI\",\n",
    "    \"Uncertainty\",\n",
    "    \"HighestPred\", \"HighestConfidence\", \"HighestVar\",\n",
    "    \"Random\",  # kept for discovery/alignments; plotted as last column\n",
    "]\n",
    "REFERENCE_METHOD = \"Random\"\n",
    "\n",
    "ACTIVE_METRICS   = [\"MEP\", \"MAP\"]  # higher is better\n",
    "\n",
    "# Heatmap color mode: \"cohen_d\" | \"percent\" | \"p\"\n",
    "HEATMAP_MODE = \"cohen_d\"\n",
    "D_CAP        = 1.5   # cap |d| for coloring\n",
    "PCT_CAP      = 25.0  # cap |%Δ| for coloring\n",
    "\n",
    "# Multiple-comparison correction\n",
    "#   FAMILY_SCOPE: \"by_row\" (each heatmap row is its family) or \"global\"\n",
    "#   METHOD: \"holm\" (Holm step-down) or \"bonferroni\"\n",
    "FAMILY_SCOPE  = \"by_row\"\n",
    "ADJUST_METHOD = \"bonferroni\"\n",
    "\n",
    "OUT_DIR = os.path.join(\"stat_results\", \"dual\")\n",
    "\n",
    "STRATEGY_LABELS = {\n",
    "    \"final_only\": \"Final iteration\",\n",
    "    \"last_n\":     \"Last 5 iterations >=5 (near-convergence stability)\",\n",
    "    \"first_only\": \"First iteration\",\n",
    "    # we'll create a custom label for iteration 5 at call time\n",
    "    \"iter5\":      \"Iteration 5\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Filename parsing helpers\n",
    "# =========================\n",
    "def _float_equal(a: float, b: float, eps=LAMBDA_EPS) -> bool:\n",
    "    return abs(a - b) <= max(eps, eps * max(1.0, abs(a), abs(b)))\n",
    "\n",
    "SEED_RX = re.compile(\n",
    "    r\"^(?P<method>[^_]+)\"\n",
    "    r\"_lambdaTrain_(?P<lt>[-+]?(?:\\d+\\.?\\d*|\\.\\d+)(?:e[-+]?\\d+)?)\"\n",
    "    r\"_lambdaSel_(?P<ls>[-+]?(?:\\d+\\.?\\d*|\\.\\d+)(?:e[-+]?\\d+)?)\"\n",
    "    r\"_(?P<dataset>[^_]+)\"\n",
    "    r\"_seed_(?P<seed>\\d+)\\.csv$\", re.IGNORECASE\n",
    ")\n",
    "AVG_RX = re.compile(\n",
    "    r\"^AVG_(?P<method>[^_]+)\"\n",
    "    r\"_lambdaTrain_(?P<lt>[-+]?(?:\\d+\\.?\\d*|\\.\\d+)(?:e[-+]?\\d+)?)\"\n",
    "    r\"_lambdaSel_(?P<ls>[-+]?(?:\\d+\\.?\\d*|\\.\\d+)(?:e[-+]?\\d+)?)\"\n",
    "    r\"_(?P<dataset>[^_]+)\\.csv$\", re.IGNORECASE\n",
    ")\n",
    "\n",
    "def parse_meta(basename: str):\n",
    "    m = SEED_RX.match(basename)\n",
    "    if m:\n",
    "        return dict(method=m[\"method\"], lt=float(m[\"lt\"]), ls=float(m[\"ls\"]),\n",
    "                    dataset=m[\"dataset\"], seed=int(m[\"seed\"]), is_avg=False)\n",
    "    m = AVG_RX.match(basename)\n",
    "    if m:\n",
    "        return dict(method=m[\"method\"], lt=float(m[\"lt\"]), ls=float(m[\"ls\"]),\n",
    "                    dataset=m[\"dataset\"], seed=None, is_avg=True)\n",
    "    return None\n",
    "\n",
    "def list_seeds_dirs(base_dirs: Iterable[str]) -> List[str]:\n",
    "    \"\"\"Find .../Results_*/seeds_results directories under provided roots.\"\"\"\n",
    "    out = []\n",
    "    for base in base_dirs:\n",
    "        base = os.path.abspath(base)\n",
    "        if not os.path.isdir(base):\n",
    "            continue\n",
    "        for root, _, _ in os.walk(base):\n",
    "            parts = os.path.normpath(root).split(os.sep)\n",
    "            if len(parts) >= 2 and parts[-1] == \"seeds_results\" and parts[-2].startswith(\"Results_\"):\n",
    "                out.append(root)\n",
    "    return sorted(set(out))\n",
    "\n",
    "def discover_files(roots: Iterable[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in roots:\n",
    "        for f in glob.glob(os.path.join(r, \"*.csv\")):\n",
    "            meta = parse_meta(os.path.basename(f))\n",
    "            if not meta:\n",
    "                rows.append(dict(root=r, file=f, parsed=False))\n",
    "                continue\n",
    "            rows.append(dict(root=r, file=f, parsed=True, **meta))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def classify(df: pd.DataFrame, dataset_filter: Optional[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Return lists of seeded CSV paths for MF and EMF (dataset matched).\"\"\"\n",
    "    mf, emf = [], []\n",
    "    if df.empty:\n",
    "        return mf, emf\n",
    "    use = df[df[\"parsed\"] == True].copy()\n",
    "    if dataset_filter is not None:\n",
    "        use = use[use[\"dataset\"].astype(str) == str(dataset_filter)]\n",
    "    for _, r in use.iterrows():\n",
    "        if _float_equal(r[\"lt\"], MF_EXPECTED[0]) and _float_equal(r[\"ls\"], MF_EXPECTED[1]):\n",
    "            mf.append(r[\"file\"])\n",
    "        elif _float_equal(r[\"lt\"], EMF_EXPECTED[0]) and _float_equal(r[\"ls\"], EMF_EXPECTED[1]):\n",
    "            emf.append(r[\"file\"])\n",
    "    return sorted(mf), sorted(emf)\n",
    "\n",
    "def load_seed_csvs(files: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load seeded CSVs (skip AVG_) and attach Method, Seed, DatasetTag. Returns method -> DF.\"\"\"\n",
    "    out: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for f in files:\n",
    "        meta = parse_meta(os.path.basename(f))\n",
    "        if not meta or meta[\"is_avg\"]:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            if \"Iteration\" not in df.columns:\n",
    "                continue\n",
    "            df = df.copy()\n",
    "            df.insert(0, \"Method\", meta[\"method\"])\n",
    "            df.insert(1, \"Seed\", meta[\"seed\"])\n",
    "            df.insert(0, \"DatasetTag\", meta[\"dataset\"])\n",
    "            out.setdefault(meta[\"method\"], []).append(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "    out2 = {}\n",
    "    for m, lst in out.items():\n",
    "        if m in METHODS_UNIVERSE:\n",
    "            out2[m] = pd.concat(lst, ignore_index=True)\n",
    "    return out2\n",
    "\n",
    "def to_long(per_method: Dict[str, pd.DataFrame], cond: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for _, df in per_method.items():\n",
    "        d = df.copy()\n",
    "        d.insert(0, \"Condition\", cond)\n",
    "        frames.append(d)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# Row selection (time windows)\n",
    "# =========================\n",
    "def select_rows(df: pd.DataFrame, strategy: str, num_iterations=5, at_iter: Optional[int]=None) -> pd.DataFrame:\n",
    "    \"\"\"Return rows per (Method, Seed) matching the requested window.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    d = df.sort_values([\"Method\", \"Seed\", \"Iteration\"]).copy()\n",
    "\n",
    "    if strategy == \"final_only\":\n",
    "        idx = d.groupby([\"Method\",\"Seed\"])[\"Iteration\"].idxmax()\n",
    "        return d.loc[idx].copy()\n",
    "    if strategy == \"last_n\":\n",
    "        d[\"r\"] = d.groupby([\"Method\",\"Seed\"])[\"Iteration\"].rank(ascending=False, method=\"first\")\n",
    "        return d[d[\"r\"] <= num_iterations].drop(columns=\"r\")\n",
    "    if strategy == \"first_only\":\n",
    "        idx = d.groupby([\"Method\",\"Seed\"])[\"Iteration\"].idxmin()\n",
    "        return d.loc[idx].copy()\n",
    "    if strategy == \"first_n\":\n",
    "        d[\"r\"] = d.groupby([\"Method\",\"Seed\"])[\"Iteration\"].rank(ascending=True, method=\"first\")\n",
    "        return d[d[\"r\"] <= num_iterations].drop(columns=\"r\")\n",
    "    if strategy == \"exact_iter\":\n",
    "        if at_iter is None:\n",
    "            raise ValueError(\"exact_iter requires at_iter=<int>\")\n",
    "        return d[d[\"Iteration\"] == at_iter].copy()\n",
    "    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "# =========================\n",
    "# Stats helpers (tails + CI)\n",
    "# =========================\n",
    "def _t_and_ci(sample: np.ndarray, mu0: float = 0.0,\n",
    "              tail: str = \"two-sided\",\n",
    "              ci_level: float = 0.95) -> Tuple[float, float, Tuple[float,float]]:\n",
    "    \"\"\"\n",
    "    Compute t-stat, p-value (one- or two-sided), and a two-sided CI around mean(sample)-mu0.\n",
    "    sample: vector of paired differences (already aligned), tested vs mu0.\n",
    "    tail: \"two-sided\" | \"less\" | \"greater\"\n",
    "    Returns: (t_stat, p_value, (ci_lo, ci_hi)) where CI is two-sided at ci_level.\n",
    "    \"\"\"\n",
    "    x = np.asarray(sample, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = x.size\n",
    "    if n < 2:\n",
    "        return np.nan, 1.0, (np.nan, np.nan)\n",
    "    mean = float(np.mean(x))\n",
    "    sd   = float(np.std(x, ddof=1))\n",
    "    if sd == 0:\n",
    "        # Degenerate: all diffs identical\n",
    "        t_stat = np.inf if mean - mu0 > 0 else (-np.inf if mean - mu0 < 0 else 0.0)\n",
    "        # p-value under degeneracy\n",
    "        if tail == \"two-sided\":\n",
    "            p_val = 0.0 if (mean - mu0) != 0 else 1.0\n",
    "        elif tail == \"less\":\n",
    "            p_val = 0.0 if (mean - mu0) < 0 else 1.0\n",
    "        else:  # \"greater\"\n",
    "            p_val = 0.0 if (mean - mu0) > 0 else 1.0\n",
    "        return t_stat, p_val, (mean - mu0, mean - mu0)\n",
    "\n",
    "    se = sd / np.sqrt(n)\n",
    "    t_stat = (mean - mu0) / se\n",
    "    df = n - 1\n",
    "\n",
    "    # p-value by tail\n",
    "    if tail == \"two-sided\":\n",
    "        p_val = 2.0 * min(student_t.cdf(t_stat, df), student_t.sf(t_stat, df))\n",
    "    elif tail == \"less\":\n",
    "        p_val = student_t.cdf(t_stat, df)\n",
    "    elif tail == \"greater\":\n",
    "        p_val = student_t.sf(t_stat, df)\n",
    "    else:\n",
    "        raise ValueError(\"tail must be one of {'two-sided','less','greater'}\")\n",
    "\n",
    "    # two-sided CI at ci_level\n",
    "    alpha = 1.0 - ci_level\n",
    "    t_crit = student_t.ppf(1 - alpha/2, df)\n",
    "    ci_lo = (mean - mu0) - t_crit * se\n",
    "    ci_hi = (mean - mu0) + t_crit * se\n",
    "    return float(t_stat), float(p_val), (float(ci_lo), float(ci_hi))\n",
    "\n",
    "def _keyed_df(vals, seeds, iters):\n",
    "    return pd.DataFrame({'seed': np.asarray(seeds, int),\n",
    "                         'iter': np.asarray(iters, int),\n",
    "                         'val':  np.asarray(vals,  float)})\n",
    "\n",
    "def _delta_table(df_slice: pd.DataFrame, method: str, metric: str) -> pd.DataFrame:\n",
    "    \"\"\"Build per-(Seed, Iteration) deltas and reference means.\"\"\"\n",
    "    dm = df_slice[df_slice[\"Method\"] == method]\n",
    "    dr = df_slice[df_slice[\"Method\"] == REFERENCE_METHOD]\n",
    "    if metric not in dm.columns or metric not in dr.columns:\n",
    "        return pd.DataFrame(columns=[\"Seed\",\"Iteration\",\"Delta\",\"RefMean\"])\n",
    "    m = _keyed_df(dm[metric].astype(float).to_numpy(), dm[\"Seed\"], dm[\"Iteration\"])\n",
    "    r = _keyed_df(dr[metric].astype(float).to_numpy(), dr[\"Seed\"], dr[\"Iteration\"])\n",
    "    pr = pd.merge(m, r, on=[\"seed\",\"iter\"], how=\"inner\", suffixes=(\"_m\",\"_r\"))\n",
    "    pr = pr.replace([np.inf,-np.inf], np.nan).dropna(subset=[\"val_m\",\"val_r\"])\n",
    "    pr.rename(columns={\"seed\":\"Seed\",\"iter\":\"Iteration\"}, inplace=True)\n",
    "    pr[\"Delta\"] = pr[\"val_m\"] - pr[\"val_r\"]\n",
    "    pr[\"RefMean\"] = pr[\"val_r\"]\n",
    "    return pr[[\"Seed\",\"Iteration\",\"Delta\",\"RefMean\"]]\n",
    "\n",
    "def _collapse_by_seed(deltas: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Average Delta and RefMean within each seed (if multiple iterations exist).\"\"\"\n",
    "    if deltas.empty:\n",
    "        return deltas\n",
    "    g = deltas.groupby(\"Seed\", as_index=False).agg(Delta=(\"Delta\",\"mean\"),\n",
    "                                                  RefMean=(\"RefMean\",\"mean\"))\n",
    "    return g\n",
    "\n",
    "def paired_vs_random(df_slice: pd.DataFrame, method: str, metric: str):\n",
    "    \"\"\"\n",
    "    Paired test of (method - Random) = 0.\n",
    "    If CLUSTER_BY_SEED is True and the window has multiple iterations per seed,\n",
    "    first average within seed, then test across seeds.\n",
    "    Returns: n_pairs, mean_diff, t_stat, p_raw, cohen_d (dz), percent_change_vs_random, ci_lo, ci_hi.\n",
    "    \"\"\"\n",
    "    deltas = _delta_table(df_slice, method, metric)\n",
    "    if deltas.empty:\n",
    "        return 0, np.nan, np.nan, 1.0, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    if CLUSTER_BY_SEED:\n",
    "        use = _collapse_by_seed(deltas)\n",
    "        d = use[\"Delta\"].to_numpy()\n",
    "        rbar = use[\"RefMean\"].to_numpy()\n",
    "        n = len(use)\n",
    "    else:\n",
    "        d = deltas[\"Delta\"].to_numpy()\n",
    "        rbar = deltas[\"RefMean\"].to_numpy()\n",
    "        n = len(deltas)\n",
    "\n",
    "    if n < MIN_SAMPLE_SIZE:\n",
    "        return n, np.nan, np.nan, 1.0, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    mean_diff = float(np.mean(d))\n",
    "    sd_diff   = float(np.std(d, ddof=1))\n",
    "    r_mean    = float(np.mean(rbar))\n",
    "    t_stat, p_raw, (ci_lo, ci_hi) = _t_and_ci(d, mu0=0.0, tail=TAIL_VSR, ci_level=CI_LEVEL)\n",
    "    cohend = (mean_diff / sd_diff) if sd_diff != 0 else np.nan\n",
    "    pct_change = (mean_diff / abs(r_mean) * 100) if abs(r_mean) > 1e-12 else np.nan\n",
    "    return n, mean_diff, float(t_stat), float(p_raw), float(cohend), float(pct_change), float(ci_lo), float(ci_hi)\n",
    "\n",
    "def diff_in_diff(df_mf: pd.DataFrame, df_emf: pd.DataFrame, method: str, metric: str):\n",
    "    \"\"\"\n",
    "    Test (MF−Random) − (EMF−Random) = 0.\n",
    "    If CLUSTER_BY_SEED is True, collapse within seed first, then test across seeds.\n",
    "    Returns: n_pairs, mean_diff, t_stat, p_raw, cohen_d, ci_lo, ci_hi.\n",
    "    \"\"\"\n",
    "    mf = _delta_table(df_mf, method, metric)\n",
    "    emf = _delta_table(df_emf, method, metric)\n",
    "    if mf.empty or emf.empty:\n",
    "        return 0, np.nan, np.nan, 1.0, np.nan, np.nan, np.nan\n",
    "\n",
    "    paired = pd.merge(mf[[\"Seed\",\"Iteration\",\"Delta\"]],\n",
    "                      emf[[\"Seed\",\"Iteration\",\"Delta\"]],\n",
    "                      on=[\"Seed\",\"Iteration\"], suffixes=(\"_mf\",\"_emf\"))\n",
    "    if paired.empty:\n",
    "        return 0, np.nan, np.nan, 1.0, np.nan, np.nan, np.nan\n",
    "\n",
    "    if CLUSTER_BY_SEED:\n",
    "        mf_s = paired.groupby(\"Seed\", as_index=False)[\"Delta_mf\"].mean()\n",
    "        emf_s= paired.groupby(\"Seed\", as_index=False)[\"Delta_emf\"].mean()\n",
    "        dtab = pd.merge(mf_s, emf_s, on=\"Seed\")\n",
    "        diff = dtab[\"Delta_mf\"].to_numpy() - dtab[\"Delta_emf\"].to_numpy()\n",
    "        n = len(dtab)\n",
    "    else:\n",
    "        diff = paired[\"Delta_mf\"].to_numpy() - paired[\"Delta_emf\"].to_numpy()\n",
    "        n = diff.size\n",
    "\n",
    "    if n < MIN_SAMPLE_SIZE:\n",
    "        return n, np.nan, np.nan, 1.0, np.nan, np.nan, np.nan\n",
    "\n",
    "    mean_diff = float(np.mean(diff))\n",
    "    sd_diff   = float(np.std(diff, ddof=1))\n",
    "    t_stat, p_raw, (ci_lo, ci_hi) = _t_and_ci(diff, mu0=0.0, tail=TAIL_DID, ci_level=CI_LEVEL)\n",
    "    cohend = (mean_diff / sd_diff) if sd_diff != 0 else np.nan\n",
    "    return n, mean_diff, float(t_stat), float(p_raw), float(cohend), float(ci_lo), float(ci_hi)\n",
    "\n",
    "# =========================\n",
    "# Multiple-comparison adjustment\n",
    "# =========================\n",
    "def _adjust_series(p: pd.Series, method: str) -> pd.Series:\n",
    "    \"\"\"Adjust a vector of p-values using 'bonferroni' or 'holm'. Returns aligned Series.\"\"\"\n",
    "    p = pd.Series(p, dtype=float)\n",
    "    m = len(p)\n",
    "    if m == 0:\n",
    "        return p\n",
    "\n",
    "    mth = method.lower()\n",
    "    if mth == \"bonferroni\":\n",
    "        return np.minimum(p * m, 1.0)\n",
    "\n",
    "    if mth == \"holm\":\n",
    "        order = np.argsort(p.values)            # ascending p\n",
    "        adj   = np.empty_like(p.values, float)\n",
    "        running_max = 0.0\n",
    "        for rank, idx in enumerate(order):\n",
    "            factor = m - rank                   # m, m-1, ..., 1\n",
    "            val = min(p.values[idx] * factor, 1.0)\n",
    "            running_max = max(running_max, val)\n",
    "            adj[idx] = running_max\n",
    "        return pd.Series(adj, index=p.index)\n",
    "\n",
    "    return p  # no adjustment\n",
    "\n",
    "def adjust_table(df: pd.DataFrame, alpha: float, family: str, method: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply multiple-comparison correction to MF/EMF vs Random rows.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "\n",
    "    if family == \"by_row\":\n",
    "        out[\"Metric_Tag\"] = out[\"Metric\"].astype(str) + \"@(\" + out[\"Condition\"].astype(str) + \")\"\n",
    "        out[\"P_Adj\"] = np.nan\n",
    "        for _, idx in out.groupby(\"Metric_Tag\", sort=False).groups.items():\n",
    "            out.loc[idx, \"P_Adj\"] = _adjust_series(out.loc[idx, \"P_Raw\"], method).values\n",
    "    else:\n",
    "        out[\"P_Adj\"] = _adjust_series(out[\"P_Raw\"], method).values\n",
    "\n",
    "    out[\"Is_Significant\"] = out[\"P_Adj\"] < alpha\n",
    "    out[\"Direction\"] = np.where(out[\"Mean_Diff\"] > 0, \"Better\",\n",
    "                         np.where(out[\"Mean_Diff\"] < 0, \"Worse\", \"No Change\"))\n",
    "    out[\"Performance\"] = np.where(out[\"Is_Significant\"], out[\"Direction\"], \"No Difference\")\n",
    "    return out\n",
    "\n",
    "def adjust_did(df: pd.DataFrame, alpha: float, method: str) -> pd.DataFrame:\n",
    "    \"\"\"Adjust DiD p-values across all DiD rows as a single family (Holm or Bonferroni).\"\"\"\n",
    "    if df.empty:\n",
    "        return df.assign(P_Adj=[], Is_Significant=[])\n",
    "    out = df.copy()\n",
    "    out[\"P_Adj\"] = _adjust_series(out[\"P_Raw\"], method).values\n",
    "    out[\"Is_Significant\"] = out[\"P_Adj\"] < alpha\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Heatmap (fixed 4 rows) with REF\n",
    "# =========================\n",
    "def star_for(p_adj: float) -> str:\n",
    "    if not np.isfinite(p_adj): return \"ns\"\n",
    "    if p_adj < 1e-3: return \"***\"\n",
    "    if p_adj < 1e-2: return \"**\"\n",
    "    if p_adj < 5e-2: return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def p_to_strength(p_adj: float, cap: float = 6.0) -> float:\n",
    "    if not np.isfinite(p_adj) or p_adj <= 0:\n",
    "        return 1.0\n",
    "    return min(-np.log10(p_adj), cap)/cap\n",
    "\n",
    "def color_value(row, mode: str) -> float:\n",
    "    sign = 0.0\n",
    "    if row[\"Mean_Diff\"] > 0: sign = 1.0\n",
    "    elif row[\"Mean_Diff\"] < 0: sign = -1.0\n",
    "\n",
    "    if mode == \"cohen_d\":\n",
    "        d = row[\"Cohen_d\"]\n",
    "        if not np.isfinite(d): return 0.0\n",
    "        d = max(min(d, D_CAP), -D_CAP)  # clamp\n",
    "        return d / D_CAP\n",
    "    if mode == \"percent\":\n",
    "        pc = row[\"Percent_Change\"]\n",
    "        if not np.isfinite(pc): return 0.0\n",
    "        pc = max(min(pc, PCT_CAP), -PCT_CAP)\n",
    "        return pc / PCT_CAP\n",
    "    return sign * p_to_strength(row[\"P_Adj\"])  # \"p\" mode\n",
    "\n",
    "def make_heatmap(\n",
    "    df_dual: pd.DataFrame,\n",
    "    strategy: str,\n",
    "    out_dir: str,\n",
    "    dataset_tag: str,\n",
    "    methods_order: List[str],\n",
    "    show: bool = True,\n",
    "    title_override: Optional[str] = None\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    ordered_rows = [\"MEP@(MF)\",\"MEP@(EMF)\",\"MAP@(MF)\",\"MAP@(EMF)\"]\n",
    "    methods_present = [m for m in methods_order if m in df_dual[\"Method\"].unique()]\n",
    "    non_ref = [m for m in methods_present if m != REFERENCE_METHOD]\n",
    "    methods = non_ref + [REFERENCE_METHOD]\n",
    "\n",
    "    df = df_dual.copy()\n",
    "    df[\"Metric_Tag\"] = df[\"Metric\"].astype(str) + \"@(\" + df[\"Condition\"].astype(str) + \")\"\n",
    "    # NEW: choose one n to display in title (mode → fallback to min)\n",
    "    n_vals = df[\"Sample_Size\"] if \"Sample_Size\" in df.columns else None\n",
    "    if n_vals is not None and np.isfinite(n_vals).any():\n",
    "        n_for_title = int(np.median(n_vals))\n",
    "    else:\n",
    "        n_for_title = None\n",
    "        \n",
    "    H = np.full((len(ordered_rows), len(methods)), np.nan)\n",
    "    ANNO = np.full((len(ordered_rows), len(methods)), \"\", dtype=object)\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        tag = f\"{r['Metric']}@({r['Condition']})\"\n",
    "        if tag not in ordered_rows or r[\"Method\"] == REFERENCE_METHOD:\n",
    "            continue\n",
    "        i = ordered_rows.index(tag)\n",
    "        j = methods.index(r[\"Method\"])\n",
    "        H[i, j] = color_value(r, HEATMAP_MODE)\n",
    "        pct = r.get(\"Percent_Change\", np.nan)\n",
    "        pct_text = \"NA\" if not np.isfinite(pct) else f\"{pct:+.1f}%\"\n",
    "        ANNO[i, j] = f\"{star_for(r['P_Adj'])}\\n{pct_text}\"\n",
    "        # annotate with star + % and n\n",
    "        #n_pairs = int(r.get(\"Sample_Size\", np.nan)) if np.isfinite(r.get(\"Sample_Size\", np.nan)) else None\n",
    "        #n_text = f\"\\n(n={n_pairs})\" if n_pairs else \"\"\n",
    "        #ANNO[i, j] = f\"{star_for(r['P_Adj'])}\\n{pct_text}{n_text}\"\n",
    "\n",
    "    # REF column (last)\n",
    "    j_ref = methods.index(REFERENCE_METHOD)\n",
    "    for i in range(len(ordered_rows)):\n",
    "        H[i, j_ref] = 0.0\n",
    "        ANNO[i, j_ref] = \"REF\"\n",
    "\n",
    "    # Plot\n",
    "    plt.style.use(\"default\")\n",
    "    fig, ax = plt.subplots(figsize=(13.5, 7.6))\n",
    "    cmap = plt.get_cmap(\"RdYlGn\")\n",
    "    im = ax.imshow(H, cmap=cmap, norm=TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1))\n",
    "\n",
    "    # annotations\n",
    "    for i in range(len(ordered_rows)):\n",
    "        for j in range(len(methods)):\n",
    "            txt = ANNO[i, j]\n",
    "            val = H[i, j]\n",
    "            color = \"white\" if np.isfinite(val) and abs(val) > 0.5 else \"black\"\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\", fontsize=18, fontweight=\"bold\", color=color)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(methods)))\n",
    "    ax.set_xticklabels(methods, rotation=30, ha=\"right\",fontsize=12,fontweight=\"bold\")\n",
    "    ax.set_yticks(np.arange(len(ordered_rows)))\n",
    "    ax.set_yticklabels(ordered_rows, fontsize=12,fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Active Learning Methods (Reference = Random)\",fontsize=14)\n",
    "    ax.set_ylabel(\"Metric@(Condition)\")\n",
    "\n",
    "    # Blue border around REF\n",
    "    ax.add_patch(Rectangle((j_ref-0.5, -0.5), 1, len(ordered_rows), fill=False, edgecolor=\"blue\", linewidth=3))\n",
    "\n",
    "    mode_detail_map = {\n",
    "        \"p\":       rf\"(color = signed $-\\log_{{10}}$ adjusted p; correction = {ADJUST_METHOD.title()})\",\n",
    "        \"cohen_d\": \"(color = signed paired Cohen’s d)\",\n",
    "        \"percent\": \"(color = signed %Δ vs Random; capped at ±25%)\",\n",
    "    }\n",
    "    mode_label = mode_detail_map.get(HEATMAP_MODE, \"\")\n",
    "    lt_mf, ls_mf   = MF_EXPECTED\n",
    "    lt_emf, ls_emf = EMF_EXPECTED\n",
    "    if _float_equal(ls_mf, ls_emf):\n",
    "        cond_label = (\n",
    "            rf\"$\\lambda_{{select}}={ls_mf}$\"\n",
    "            r\"  |  \"\n",
    "            rf\"MF: $\\lambda_{{train}}={lt_mf}$\"\n",
    "            r\"  |  \"\n",
    "            rf\"EMF: $\\lambda_{{train}}={lt_emf}$\"\n",
    "        )\n",
    "    else:\n",
    "        cond_label = (\n",
    "            rf\"MF: $\\lambda_{{train}}={lt_mf},\\,\\lambda_{{select}}={ls_mf}$\"\n",
    "            r\"  |  \"\n",
    "            rf\"EMF: $\\lambda_{{train}}={lt_emf},\\,\\lambda_{{select}}={ls_emf}$\"\n",
    "        )\n",
    "    title_main = title_override or STRATEGY_LABELS.get(strategy, strategy)\n",
    "    n_suffix = f\"  (paired samples: n={n_for_title})\"\n",
    "\n",
    "    ax.set_title(title_main + n_suffix + \"\\n\" + mode_label + \"\\n\" + cond_label,\n",
    "                 fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.02)\n",
    "    cbar.set_label(\"Heatmap scale (−1 worse · 0 no diff · +1 better)\")\n",
    "\n",
    "    legend_text = (\n",
    "        r\"$\\mathbf{GREEN}$ = Better than Random | \"\n",
    "        r\"$\\mathbf{RED}$ = Worse than Random | \"\n",
    "        r\"$\\mathbf{YELLOW}$ is No difference\" \"\\n\"\n",
    "        rf\"Stars use $\\mathbf{{{ADJUST_METHOD.title()}}}$-adjusted p-values: \"  \n",
    "        r\"$\\mathbf{(***)}$<0.001, $\\mathbf{(**)}$<0.01, $\\mathbf{(*)}$<0.05, and $\\mathbf{ns}$ = not significant.\"\n",
    "    )\n",
    "    plt.figtext(0.5, -0.02, legend_text, ha=\"center\", fontsize=15.5,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.85))\n",
    "    plt.tight_layout(rect=[0,0.05,1,0.98])\n",
    "\n",
    "    base = os.path.join(out_dir, f\"heatmap_dual_FIXED4_{HEATMAP_MODE}_{strategy}_neighbor_{NEIGHBOR}_dataset_{dataset_tag}\")\n",
    "    for ext in [\"png\",\"pdf\",\"svg\"]:\n",
    "        plt.savefig(f\"{base}.{ext}\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"[PLOT] Saved heatmap: {base}.[png|pdf|svg]\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# =========================\n",
    "# Reporting\n",
    "# =========================\n",
    "def write_reports(df: pd.DataFrame, df_did: pd.DataFrame, strategy: str, out_dir: str):\n",
    "    \"\"\"Save detailed per-test rows, DiD table, per-condition summaries, and a compact pivot.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    det_path = os.path.join(out_dir, f\"detailed_dual_{strategy}.csv\")\n",
    "    df.to_csv(det_path, index=False)\n",
    "    print(f\"[CSV] Detailed (MF & EMF vs Random): {det_path}\")\n",
    "\n",
    "    did_path = os.path.join(out_dir, f\"did_dual_{strategy}.csv\")\n",
    "    df_did.to_csv(did_path, index=False)\n",
    "    print(f\"[CSV] Difference-in-Differences: {did_path}\")\n",
    "\n",
    "    for cond in [\"MF\",\"EMF\"]:\n",
    "        sub = df[df[\"Condition\"] == cond]\n",
    "        rows = []\n",
    "        for m in [mm for mm in METHODS_UNIVERSE if mm in sub[\"Method\"].unique() and mm != REFERENCE_METHOD]:\n",
    "            mdf = sub[sub[\"Method\"] == m]\n",
    "            if mdf.empty: continue\n",
    "            better = int((mdf[\"Performance\"] == \"Better\").sum())\n",
    "            worse  = int((mdf[\"Performance\"] == \"Worse\").sum())\n",
    "            nodif  = int((mdf[\"Performance\"] == \"No Difference\").sum())\n",
    "            tot = len(mdf)\n",
    "            avg_abs_pct = float(mdf[\"Percent_Change\"].abs().replace([np.inf,-np.inf], np.nan).dropna().mean()) if tot else np.nan\n",
    "            rows.append(dict(\n",
    "                Method=m,\n",
    "                Better=f\"{better}/{tot} ({(100*better/tot if tot else 0):.1f}%)\",\n",
    "                Worse=f\"{worse}/{tot} ({(100*worse/tot if tot else 0):.1f}%)\",\n",
    "                No_Diff=f\"{nodif}/{tot} ({(100*nodif/tot if tot else 0):.1f}%)\",\n",
    "                Avg_abs_pct=f\"{0 if np.isnan(avg_abs_pct) else avg_abs_pct:.2f}%\"\n",
    "            ))\n",
    "        path = os.path.join(out_dir, f\"summary_{cond}_{strategy}.csv\")\n",
    "        pd.DataFrame(rows).to_csv(path, index=False)\n",
    "        print(f\"[CSV] {cond} summary: {path}\")\n",
    "\n",
    "    show = df.copy()\n",
    "    show[\"Metric_Tag\"] = show[\"Metric\"].astype(str) + \"@(\" + show[\"Condition\"].astype(str) + \")\"\n",
    "\n",
    "    def fmt_cell(r):\n",
    "        s = star_for(r[\"P_Adj\"])\n",
    "        pct = \"NA\" if not np.isfinite(r[\"Percent_Change\"]) else f\"{r['Percent_Change']:+.1f}%\"\n",
    "        d   = \"NA\" if not np.isfinite(r[\"Cohen_d\"]) else f\"{r['Cohen_d']:.2f}\"\n",
    "        return f\"{r['Performance']} | {pct} | {s} | d={d}\"\n",
    "\n",
    "    show[\"Cell\"] = show.apply(fmt_cell, axis=1)\n",
    "    pivot = show.pivot_table(index=\"Method\", columns=\"Metric_Tag\", values=\"Cell\",\n",
    "                             aggfunc=lambda x: x.iloc[0])\n",
    "    pivot = pivot.loc[:, ~pivot.columns.duplicated()]  # safety\n",
    "    want_cols = [\"MEP@(MF)\",\"MEP@(EMF)\",\"MAP@(MF)\",\"MAP@(EMF)\"]\n",
    "    pivot = pivot.reindex(columns=[c for c in want_cols if c in pivot.columns])\n",
    "    p_path = os.path.join(out_dir, f\"comparison_pivot_{strategy}.csv\")\n",
    "    pivot.to_csv(p_path)\n",
    "    print(f\"[CSV] Pivot: {p_path}\")\n",
    "\n",
    "# =========================\n",
    "# Console analysis helpers\n",
    "# =========================\n",
    "def _pretty(sig: bool) -> str:\n",
    "    return \"✓\" if sig else \"–\"\n",
    "\n",
    "def _stars(p_adj: float) -> str:\n",
    "    if not np.isfinite(p_adj): return \"ns\"\n",
    "    if p_adj < 1e-3: return \"***\"\n",
    "    if p_adj < 1e-2: return \"**\"\n",
    "    if p_adj < 5e-2: return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def analyze_outputs(out_dir: str, strategy: str):\n",
    "    \"\"\"\n",
    "    Reload CSVs we just wrote and print:\n",
    "      (1) For MF and EMF, per metric: a table comparing each baseline vs Random.\n",
    "      (2) A concise DiD (MF−R)−(EMF−R) summary.\n",
    "    \"\"\"\n",
    "    det = os.path.join(out_dir, f\"detailed_dual_{strategy}.csv\")\n",
    "    did = os.path.join(out_dir, f\"did_dual_{strategy}.csv\")\n",
    "    print(\"\\n[ANALYZE]\", STRATEGY_LABELS.get(strategy, strategy))\n",
    "    if not os.path.exists(det):\n",
    "        print(\"  (No detailed CSV found.)\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(det)\n",
    "\n",
    "    # ----- A) Per-condition baseline vs Random tables -----\n",
    "    if df.empty:\n",
    "        print(\"  (Detailed table is empty.)\")\n",
    "    else:\n",
    "        df = df.sort_values([\"Condition\", \"Metric\", \"Method\"]).reset_index(drop=True)\n",
    "        for cond in [\"MF\", \"EMF\"]:\n",
    "            sub = df[(df[\"Condition\"] == cond)].copy()\n",
    "            if sub.empty: continue\n",
    "            print(f\"\\n  -- {cond} vs Random --\")\n",
    "            for metric in ACTIVE_METRICS:\n",
    "                msub = sub[sub[\"Metric\"] == metric].copy()\n",
    "                if msub.empty: continue\n",
    "                msub = msub[msub[\"Method\"] != REFERENCE_METHOD]\n",
    "                def _fmt_num(x, fmt):\n",
    "                    return \"\" if not np.isfinite(x) else format(x, fmt)\n",
    "                out_rows = []\n",
    "                for _, r in msub.iterrows():\n",
    "                    out_rows.append(dict(\n",
    "                        Method   = r[\"Method\"],\n",
    "                        n        = int(r[\"Sample_Size\"]),\n",
    "                        MeanDiff = _fmt_num(r[\"Mean_Diff\"], \"+.4f\"),\n",
    "                        dz       = _fmt_num(r[\"Cohen_d\"], \".2f\"),\n",
    "                        pctDelta = (_fmt_num(r[\"Percent_Change\"], \"+.1f\") + \"%\") if np.isfinite(r[\"Percent_Change\"]) else \"\",\n",
    "                        p_adj    = _fmt_num(r.get(\"P_Adj\", np.nan), \".3g\"),\n",
    "                        Sig      = _stars(r.get(\"P_Adj\", np.nan)),\n",
    "                        Perf     = r.get(\"Performance\", \"\")\n",
    "                    ))\n",
    "                tab = pd.DataFrame(out_rows, columns=[\"Method\",\"n\",\"MeanDiff\",\"dz\",\"pctDelta\",\"p_adj\",\"Sig\",\"Perf\"])\n",
    "                if tab.empty: continue\n",
    "                def _rank(row):\n",
    "                    if row[\"Perf\"] == \"Better\": return (0, -abs(float(row[\"dz\"] or 0)))\n",
    "                    if row[\"Perf\"] == \"Worse\":  return (1, -abs(float(row[\"dz\"] or 0)))\n",
    "                    return (2, 0)\n",
    "                tab[\"_order\"] = tab.apply(_rank, axis=1)\n",
    "                tab = tab.sort_values(\"_order\").drop(columns=\"_order\")\n",
    "                print(f\"     {metric}:\")\n",
    "                print(tab.to_string(index=False))\n",
    "\n",
    "    # ----- B) DiD summary -----\n",
    "    if os.path.exists(did):\n",
    "        df_did = pd.read_csv(did)\n",
    "        print(\"\\n  -- DiD: (MF−R) − (EMF−R) --\")\n",
    "        if df_did.empty:\n",
    "            print(\"     No DiD rows.\")\n",
    "        else:\n",
    "            sig = df_did[df_did[\"Is_Significant\"] == True].copy()\n",
    "            if sig.empty:\n",
    "                print(\"     No significant DiD.\")\n",
    "            else:\n",
    "                for metric in ACTIVE_METRICS:\n",
    "                    ss = sig[sig[\"Metric\"] == metric]\n",
    "                    if ss.empty: continue\n",
    "                    pos = ss[ss[\"Mean_Diff\"] > 0.0].sort_values(\"Cohen_d\", ascending=False)\n",
    "                    neg = ss[ss[\"Mean_Diff\"] < 0.0].sort_values(\"Cohen_d\")\n",
    "                    if not pos.empty:\n",
    "                        names = \", \".join(f\"{r.Method} (dz={r.Cohen_d:.2f}, p_adj={r.P_Adj:.3g})\" for _, r in pos.iterrows())\n",
    "                        print(f\"     {metric}: MF > EMF for {names}\")\n",
    "                    if not neg.empty:\n",
    "                        names = \", \".join(f\"{r.Method} (dz={r.Cohen_d:.2f}, p_adj={r.P_Adj:.3g})\" for _, r in neg.iterrows())\n",
    "                        print(f\"     {metric}: EMF > MF for {names}\")\n",
    "    else:\n",
    "        print(\"\\n  -- DiD: (MF−R) − (EMF−R) --\")\n",
    "        print(\"     (No DiD CSV found.)\")\n",
    "\n",
    "# =========================\n",
    "# Overview table helpers (new)\n",
    "# =========================\n",
    "def _count_raw_pairs_for_window(mf_slice: pd.DataFrame, emf_slice: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Raw n for the window = number of unique (Seed, Iteration) pairs present in Random.\n",
    "    We take the min across MF and EMF slices to reflect aligned availability.\n",
    "    \"\"\"\n",
    "    def _count(df):\n",
    "        if df is None or df.empty: return 0\n",
    "        r = df[df[\"Method\"] == REFERENCE_METHOD]\n",
    "        if r.empty: return 0\n",
    "        return r.drop_duplicates([\"Seed\",\"Iteration\"]).shape[0]\n",
    "    n_mf  = _count(mf_slice)\n",
    "    n_emf = _count(emf_slice)\n",
    "    if n_mf and n_emf:\n",
    "        return min(n_mf, n_emf)\n",
    "    return n_mf or n_emf or 0\n",
    "\n",
    "def _overview_row(strategy_key: str, paired_df: pd.DataFrame, n_raw: int) -> dict:\n",
    "    \"\"\"\n",
    "    Build one summary row for the overview table from a window's paired_df.\n",
    "    \"\"\"\n",
    "    if paired_df is None or paired_df.empty:\n",
    "        return dict(Strategy=strategy_key, n=0, SigTests=\"0/0\", SigPct=\"0.0%\", Better=0, Worse=0, AvgAbsPct=\"0.00\")\n",
    "\n",
    "    total_tests = int(paired_df.shape[0])\n",
    "    sig_tests   = int((paired_df[\"Is_Significant\"] == True).sum())\n",
    "    better      = int((paired_df[\"Performance\"] == \"Better\").sum())\n",
    "    worse       = int((paired_df[\"Performance\"] == \"Worse\").sum())\n",
    "    avg_abs_pct = paired_df[\"Percent_Change\"].abs().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    avg_abs_pct = 0.0 if avg_abs_pct.empty else float(avg_abs_pct.mean())\n",
    "\n",
    "    return dict(\n",
    "        Strategy   = STRATEGY_LABELS.get(strategy_key, strategy_key),\n",
    "        n          = n_raw,\n",
    "        SigTests   = f\"{sig_tests}/{total_tests}\",\n",
    "        SigPct     = f\"{(100.0*sig_tests/total_tests if total_tests else 0.0):.1f}%\",\n",
    "        Better     = better,\n",
    "        Worse      = worse,\n",
    "        AvgAbsPct  = f\"{avg_abs_pct:.2f}\"\n",
    "    )\n",
    "\n",
    "def _write_overview_table(rows: list, out_dir: str, filename_base: str = \"overview_dual_windows\"):\n",
    "    \"\"\"\n",
    "    Save overview as CSV and a LaTeX tabular snippet that matches paper style.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df = pd.DataFrame(rows, columns=[\"Strategy\",\"n\",\"SigTests\",\"SigPct\",\"Better\",\"Worse\",\"AvgAbsPct\"])\n",
    "    csv_path = os.path.join(out_dir, f\"{filename_base}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    tex_lines = [\n",
    "        r\"\\begin{tabular}{l r r r r r r}\",\n",
    "        r\"\\toprule\",\n",
    "        r\"Strategy & $n$ & Sig.\\ Tests & Sig.\\ (\\%) & Better & Worse & Avg.\\ $|\\%\\Delta|$ \\\\\",\n",
    "        r\"\\midrule\",\n",
    "    ]\n",
    "    for _, r in df.iterrows():\n",
    "        tex_lines.append(\n",
    "            f\"{r['Strategy']} & {r['n']} & {r['SigTests']} & {r['SigPct']} & \"\n",
    "            f\"{int(r['Better'])} & {int(r['Worse'])} & {float(r['AvgAbsPct']):.2f} \\\\\\\\\"\n",
    "        )\n",
    "    tex_lines += [r\"\\bottomrule\", r\"\\end{tabular}\"]\n",
    "    tex_str = \"\\n\".join(tex_lines)\n",
    "    tex_path = os.path.join(out_dir, f\"{filename_base}.tex\")\n",
    "    with open(tex_path, \"w\") as f:\n",
    "        f.write(tex_str)\n",
    "\n",
    "    print(f\"[CSV] Overview table: {csv_path}\")\n",
    "    print(f\"[TEX] LaTeX table:    {tex_path}\")\n",
    "\n",
    "# =========================\n",
    "# Orchestrator\n",
    "# =========================\n",
    "def run_dual_analysis(\n",
    "    base_dirs: Iterable[str],\n",
    "    dataset_filter: Optional[str] = DEFAULT_DATASET,\n",
    "    strategies = None,\n",
    "    out_dir: str = OUT_DIR,\n",
    "    show_plots: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Main entry: discover files, run tests, write CSVs, plot heatmaps, print analysis,\n",
    "    and produce an overview summary table across windows.\n",
    "    \"\"\"\n",
    "    # Default windows: first, iter5, final, last5>=5\n",
    "    if strategies is None:\n",
    "        strategies = [\n",
    "            (\"first_only\", {}),\n",
    "            (\"exact_iter\", {\"at_iter\": 5}),\n",
    "            (\"final_only\", {}),\n",
    "            (\"last_n\",     {\"num_iterations\": 5}),\n",
    "        ]\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    roots = list_seeds_dirs(base_dirs)\n",
    "    if not roots:\n",
    "        print(f\"[AUTO] No 'Results_*/seeds_results' folders under: {list(base_dirs)}\")\n",
    "        return\n",
    "\n",
    "    ledger = discover_files(roots)\n",
    "    if ledger.empty:\n",
    "        print(\"[AUTO] No CSV files discovered.\")\n",
    "        return\n",
    "    led_path = os.path.join(out_dir, \"discovery_ledger.csv\")\n",
    "    ledger.to_csv(led_path, index=False)\n",
    "    print(f\"[DISCOVERY] Ledger saved: {led_path}\")\n",
    "\n",
    "    mf_files, emf_files = classify(ledger, dataset_filter)\n",
    "    if not mf_files and not emf_files:\n",
    "        print(\"[INFO] No MF/EMF matches for given dataset filter.\")\n",
    "        return\n",
    "\n",
    "    mf_long  = to_long(load_seed_csvs(mf_files),  \"MF\")  if mf_files  else pd.DataFrame()\n",
    "    emf_long = to_long(load_seed_csvs(emf_files), \"EMF\") if emf_files else pd.DataFrame()\n",
    "\n",
    "    if mf_long.empty:  print(\"[WARN] No usable seeded MF files (missing Iteration or no matches).\")\n",
    "    if emf_long.empty: print(\"[WARN] No usable seeded EMF files (missing Iteration or no matches).\")\n",
    "\n",
    "    if not mf_long.empty:\n",
    "        mf_long = mf_long[mf_long[\"Method\"].isin(METHODS_UNIVERSE)]\n",
    "    if not emf_long.empty:\n",
    "        emf_long = emf_long[emf_long[\"Method\"].isin(METHODS_UNIVERSE)]\n",
    "\n",
    "    # accumulate overview rows\n",
    "    overview_rows = []\n",
    "\n",
    "    for strategy, kwargs in strategies:\n",
    "        # Pretty name (special-case exact_iter->iter5 label)\n",
    "        title_override = None\n",
    "        strategy_key_for_table = strategy\n",
    "        if strategy == \"exact_iter\" and kwargs.get(\"at_iter\", None) == 5:\n",
    "            strategy_key_for_table = \"iter5\"\n",
    "            title_override = STRATEGY_LABELS[\"iter5\"]\n",
    "\n",
    "        print(f\"\\n===== {STRATEGY_LABELS.get(strategy_key_for_table, strategy)} =====\")\n",
    "        mf_slice  = select_rows(mf_long,  strategy=strategy, **kwargs) if not mf_long.empty else pd.DataFrame()\n",
    "        emf_slice = select_rows(emf_long, strategy=strategy, **kwargs) if not emf_long.empty else pd.DataFrame()\n",
    "\n",
    "        # A) MF/EMF vs Random\n",
    "        rows = []\n",
    "        for cond, dfc in ((\"MF\", mf_slice), (\"EMF\", emf_slice)):\n",
    "            if dfc.empty:\n",
    "                continue\n",
    "            for method in [m for m in METHODS_UNIVERSE if m != REFERENCE_METHOD and m in dfc[\"Method\"].unique()]:\n",
    "                for metric in ACTIVE_METRICS:\n",
    "                    n, mean_diff, t_stat, p_raw, d, pct, ci_lo, ci_hi = paired_vs_random(dfc, method, metric)\n",
    "                    if n < MIN_SAMPLE_SIZE:\n",
    "                        continue\n",
    "                    rows.append(dict(\n",
    "                        Condition=cond, Method=method, Metric=metric,\n",
    "                        Sample_Size=n, Mean_Diff=mean_diff, T_Stat=t_stat,\n",
    "                        P_Raw=p_raw, Cohen_d=d, Percent_Change=pct,\n",
    "                        CI_Lo=ci_lo, CI_Hi=ci_hi, Tail=TAIL_VSR\n",
    "                    ))\n",
    "        paired_df = pd.DataFrame(rows)\n",
    "        if paired_df.empty:\n",
    "            print(\"[INFO] No aligned pairs under this window (check Random presence & seed overlap).\")\n",
    "            continue\n",
    "        paired_df = adjust_table(paired_df, alpha=ALPHA, family=FAMILY_SCOPE, method=ADJUST_METHOD)\n",
    "\n",
    "        # B) DiD: (MF−R) − (EMF−R)\n",
    "        did_rows = []\n",
    "        if not mf_slice.empty and not emf_slice.empty:\n",
    "            common_methods = sorted(set(METHODS_UNIVERSE) & set(mf_slice[\"Method\"].unique()) & set(emf_slice[\"Method\"].unique()))\n",
    "            common_methods = [m for m in common_methods if m != REFERENCE_METHOD]\n",
    "            for method in common_methods:\n",
    "                for metric in ACTIVE_METRICS:\n",
    "                    n, mean_diff, t_stat, p_raw, d, ci_lo, ci_hi = diff_in_diff(mf_slice, emf_slice, method, metric)\n",
    "                    if n < MIN_SAMPLE_SIZE:\n",
    "                        continue\n",
    "                    did_rows.append(dict(\n",
    "                        Method=method, Metric=metric, Sample_Size=n,\n",
    "                        Mean_Diff=mean_diff, T_Stat=t_stat, P_Raw=p_raw, Cohen_d=d,\n",
    "                        CI_Lo=ci_lo, CI_Hi=ci_hi, Tail=TAIL_DID\n",
    "                    ))\n",
    "        did_df = pd.DataFrame(did_rows)\n",
    "        did_df = adjust_did(did_df, alpha=ALPHA, method=ADJUST_METHOD)\n",
    "\n",
    "        # Write + Plot + Analyze\n",
    "        write_reports(paired_df, did_df, strategy_key_for_table, out_dir)\n",
    "        make_heatmap(\n",
    "            paired_df,\n",
    "            strategy=strategy_key_for_table,\n",
    "            out_dir=out_dir,\n",
    "            dataset_tag=(dataset_filter or \"mixed\"),\n",
    "            methods_order=METHODS_UNIVERSE,\n",
    "            show=show_plots,\n",
    "            title_override=title_override\n",
    "        )\n",
    "        analyze_outputs(out_dir, strategy_key_for_table)\n",
    "\n",
    "        # Overview row\n",
    "        n_raw = _count_raw_pairs_for_window(mf_slice, emf_slice)\n",
    "        overview_rows.append(_overview_row(strategy_key_for_table, paired_df, n_raw))\n",
    "\n",
    "    # Save a single overview across all processed windows\n",
    "    _write_overview_table(overview_rows, out_dir, filename_base=\"overview_dual_windows\")\n",
    "\n",
    "# =========================\n",
    "# Example call\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    run_dual_analysis(\n",
    "        base_dirs=[\n",
    "            \".\",  # current tree\n",
    "            #\"LINK_TO_ANOTHER_ROOT\",\n",
    "        ],\n",
    "        dataset_filter=\"100k\",\n",
    "        show_plots=True,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
