{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82285a92",
   "metadata": {},
   "source": [
    "# A New Explainable Active Learning Approach for Recommender Systems (ExAL)\n",
    "\n",
    "**Authors and Contact Information:**\n",
    "\n",
    "** Anonymous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cfbf1-e067-4866-9329-6bf224e7d055",
   "metadata": {},
   "source": [
    "# Adding Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d210430-af10-4148-a12c-185bb69322c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import glob, io, logging, os, random, shutil, zipfile\n",
    "import numba, numpy as np, pandas as pd, requests, tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Logger Setup \n",
    "# This logger will log to both a file and the console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"run_logs.log\"),    \n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365a25e",
   "metadata": {},
   "source": [
    "# Global Hyperparameters for Explainable Active Learning (ExAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950692a-be41-434f-b209-688656c463bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_iter      = 10     # Number of active learning iterations (outer loop)\n",
    "SWITCH        = 5      # Iteration to switch from ExAL-Max to ExAL-Min (for ExAL-Min-Max)\n",
    "\n",
    "ALPHA_INIT    = 0.01   # Learning rate for initial EMF (matrix factorization) training\n",
    "ALPHA_RETRAIN = 0.001  # Learning rate for online retraining (per AL iteration)\n",
    "\n",
    "LAMDA         = 0.1    # L1 regularization strength for EMF\n",
    "NEIGHBOR      = 20     # Number of neighbors (k) for explainability matrix W\n",
    "BETA          = 0.15   # L2 regularization strength for EMF\n",
    "\n",
    "INIT_STEPS    = 300    # Number of EMF training epochs before AL begins\n",
    "ONLINE_STEP   = 10     # Number of SGD steps per user per AL iteration\n",
    "\n",
    "TopN          = 25     # Top-N cutoff for recommendation evaluation (MAP, xP, xR, etc)\n",
    "K             = 10     # Dimension of latent feature vectors in MF/EMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6347acb-60a1-48ab-8ba6-d544f6068ca3",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fedf8-4d2e-4395-a512-bbec420d6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movielens(dataset='100k'):\n",
    "    \"\"\"\n",
    "    Load MovieLens dataset (100k or 1M).\n",
    "    Returns:\n",
    "      data_M: user-item rating matrix (users x items, unrated = 0)\n",
    "      movies: movie info (100k only)\n",
    "    \"\"\"\n",
    "    if dataset == '100k':\n",
    "        if not os.path.exists('ml-100k'):\n",
    "            # Download and extract ML-100k\n",
    "            url_100k = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "            r = requests.get(url_100k)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(path='./')\n",
    "\n",
    "        # Load ratings and build rating matrix\n",
    "        data = pd.read_table('ml-100k/u.data', names=['UserID', 'movieID', 'Rating', 'Timestamp'])\n",
    "        data_M = data.pivot(index='UserID', columns='movieID', values='Rating').fillna(0)\n",
    "\n",
    "        # Load movie genres\n",
    "        genre_columns = [\n",
    "            'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',\n",
    "            'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical',\n",
    "            'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n",
    "        ]\n",
    "        movies = pd.read_csv(\n",
    "            'ml-100k/u.item',\n",
    "            sep='|',\n",
    "            header=None,\n",
    "            encoding='ISO-8859-1',\n",
    "            usecols=[0, 1, *range(5, 24)],\n",
    "            names=['movieID', 'movie name', 'release date', 'video release date', 'IMDb URL'] + genre_columns\n",
    "        )\n",
    "        movies['genre'] = movies[genre_columns].dot(pd.Index(genre_columns) + ',').str.rstrip(',')\n",
    "        movies = movies[['movieID', 'movie name', 'genre']]\n",
    "\n",
    "    elif dataset == '1m':\n",
    "        if not os.path.exists('ml-1m'):\n",
    "            # Download and extract ML-1M\n",
    "            url_1m = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "            r = requests.get(url_1m)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(path='./')\n",
    "\n",
    "        # Load ratings and build rating matrix\n",
    "        data = pd.read_csv('ml-1m/ratings.dat', sep='::', engine='python',\n",
    "                           names=['UserID', 'movieID', 'Rating', 'Timestamp'])\n",
    "        data_M = data.pivot(index='UserID', columns='movieID', values='Rating').fillna(0)\n",
    "        movies = None  # No movie info for 1M\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "    \n",
    "    # Reindex users and items to consecutive integers\n",
    "    data_M = data_M.reset_index(drop=True)\n",
    "    data_M.columns = range(data_M.shape[1])\n",
    "\n",
    "    return data_M, movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13ceca-8566-4784-b131-b07ab6c9e511",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382f4c7-b244-4547-9733-50f443a9cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(\n",
    "    data_M,\n",
    "    num_test_users=None,      \n",
    "    num_train_ratings=3,         \n",
    "    num_test_ratings=20, \n",
    "    min_pool_size=10,\n",
    "    rng=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits user-item rating matrix into train, test, and pool sets.\n",
    "    Aligns with paper methodology - test set used for AL selection.\n",
    "    \"\"\"\n",
    "    X = data_M.values\n",
    "    U, I = X.shape\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "    logger.info(\"=== SPLIT FUNCTION DEBUG ===\")\n",
    "    logger.info(f\"Dataset shape: {U} users x {I} items\")\n",
    "    logger.info(f\"Split sizes: train={num_train_ratings}, test={num_test_ratings}, min_pool={min_pool_size}\")\n",
    "\n",
    "    # --- 1. Identify eligible test users ---\n",
    "    req = num_train_ratings + num_test_ratings + min_pool_size  \n",
    "    candidates = [u for u in range(U) if np.count_nonzero(X[u]) >= req]\n",
    "    logger.info(f\"Found {len(candidates)} users with >= {req} ratings\")\n",
    "\n",
    "    # --- 2. Select test users for AL scenario ---\n",
    "    if num_test_users is None:\n",
    "        num_test_users = max(1, int(0.1 * len(candidates)))\n",
    "    rng.shuffle(candidates)\n",
    "    test_users = np.array(candidates[:num_test_users])\n",
    "    logger.info(f\"Selected {len(test_users)} test users\")\n",
    "    test_set = set(test_users)\n",
    "\n",
    "    # --- 3. Allocate ratings into train, test, and pool matrices ---\n",
    "    train = np.zeros((U, I), float)\n",
    "    test  = np.zeros((U, I), float)\n",
    "    pool  = np.zeros((U, I), float)\n",
    "    \n",
    "    split_stats = {\"train\": 0, \"test\": 0, \"pool\": 0}  \n",
    "    \n",
    "    for u in range(U):\n",
    "        items = np.flatnonzero(X[u])\n",
    "        if u in test_set and len(items) >= req:\n",
    "            # Select ratings for train/test\n",
    "            n_select = num_train_ratings + num_test_ratings  \n",
    "            sel = rng.choice(items, size=n_select, replace=False)\n",
    "            \n",
    "            train_end = num_train_ratings\n",
    "            \n",
    "            # Train set: small set for initialization\n",
    "            train[u, sel[:train_end]] = X[u, sel[:train_end]]\n",
    "            # Test set: used for AL query selection AND final evaluation\n",
    "            test[u, sel[train_end:]] = X[u, sel[train_end:]]\n",
    "            # Pool: candidate ratings for AL queries\n",
    "            pool_items = [i for i in items if i not in sel]\n",
    "            pool[u, pool_items] = X[u, pool_items]\n",
    "            \n",
    "            split_stats[\"train\"] += train_end\n",
    "            split_stats[\"test\"] += num_test_ratings\n",
    "            split_stats[\"pool\"] += len(pool_items)\n",
    "        else:\n",
    "            # For non-test users, keep all available ratings for training\n",
    "            train[u, items] = X[u, items]\n",
    "            split_stats[\"train\"] += len(items)\n",
    "            \n",
    "    logger.info(f\"Split statistics: {split_stats}\")\n",
    "    logger.info(\"=========================\")\n",
    "    \n",
    "    return train, test, pool, test_users  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237317d-c1e4-4a5c-bdae-9eb8d9000773",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd30b7-2c7f-4e5b-b022-5c7a4b15706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(train, test, rng, lamda, steps=INIT_STEPS,\n",
    "                     alpha=ALPHA_INIT, beta=BETA, K=K,\n",
    "                     neighbor=NEIGHBOR, theta=0.0):\n",
    "    \"\"\"\n",
    "    Initializes Explainable Matrix Factorization (EMF) for ExAL experiments.\n",
    "\n",
    "    \"\"\"\n",
    "    U, I = train.shape\n",
    "    P = rng.random((U, K))\n",
    "    Q = rng.random((I, K))\n",
    "    W = calc_exp(train, neighbor=neighbor, theta=theta)\n",
    "    P, Q, train_mae = EMF_with_explainability(\n",
    "        train, P, Q, K, W=W, lamda=lamda, steps=steps, alpha=alpha, beta=beta\n",
    "    )\n",
    "    pred = P.dot(Q.T)\n",
    "    mask = test != 0\n",
    "    test_mae = np.abs(pred[mask] - test[mask]).mean() if np.any(mask) else np.nan\n",
    "    return P, Q, train_mae, test_mae, W\n",
    "\n",
    "def calc_exp(rate, neighbor=20, theta=0.0):\n",
    "    U, I = rate.shape\n",
    "    k = min(neighbor, U-1)\n",
    "    if k <= 0:\n",
    "        return np.zeros((U, I), float)\n",
    "\n",
    "    dist = pairwise_distances(rate, metric='cosine')\n",
    "    nn = np.argsort(dist, axis=1)[:, 1:k+1]          \n",
    "    expl = (rate[nn, :] > 0).sum(axis=1) / float(k)  \n",
    "    if theta > 0.0:\n",
    "        expl[expl < theta] = 0.0\n",
    "    return expl\n",
    "\n",
    "def calc_exp_row(rate, u, neighbor=20, theta=0.0):\n",
    "    U, I = rate.shape\n",
    "    k = min(neighbor, U-1)\n",
    "    if k <= 0:\n",
    "        return np.zeros(I, float)\n",
    "\n",
    "    dist_u = pairwise_distances(rate[u][None, :], rate, metric='cosine')[0]\n",
    "    nn_idx = np.argsort(dist_u)[1:k+1]              \n",
    "    expl_u = (rate[nn_idx, :] > 0).sum(axis=0) / float(k)\n",
    "    if theta > 0.0:\n",
    "        expl_u[expl_u < theta] = 0.0\n",
    "    return expl_u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def EMF_with_explainability(R, P, Q, K, W, lamda, steps, alpha, beta):\n",
    "    \"\"\"\n",
    "    EMF (Explainable Matrix Factorization) optimizer.\n",
    "\n",
    "    Returns:\n",
    "      - Updated P, Q, and train MAE.\n",
    "    \"\"\"\n",
    "    Q = Q.T\n",
    "    U, I = R.shape\n",
    "    for _ in range(steps):\n",
    "        for u in range(U):\n",
    "            for i in range(I):\n",
    "                if R[u, i] > 0:\n",
    "                    e = R[u, i] - np.dot(P[u], Q[:, i])\n",
    "                    for f in range(K):\n",
    "                        diff = P[u, f] - Q[f, i]\n",
    "                        grad_p = 2 * e * Q[f, i] - beta * P[u, f] - lamda * W[u, i] * diff\n",
    "                        grad_q = 2 * e * P[u, f] - beta * Q[f, i] + lamda * W[u, i] * diff\n",
    "                        P[u, f] += alpha * grad_p\n",
    "                        Q[f, i] += alpha * grad_q\n",
    "    # Compute MAE on training set\n",
    "    total_error = 0.0\n",
    "    count = 0\n",
    "    for u in range(U):\n",
    "        for i in range(I):\n",
    "            if R[u, i] > 0:\n",
    "                total_error += abs(R[u, i] - np.dot(P[u], Q[:, i]))\n",
    "                count += 1\n",
    "    train_mae = total_error / count if count > 0 else 0.0\n",
    "    return P, Q.T, train_mae\n",
    "\n",
    "@numba.njit\n",
    "def retrain_online_exp(u, train, P_init, Q, W, alpha, beta, K, steps, lamda):\n",
    "    \"\"\"\n",
    "    Per-user online update of latent vector P_u after adding new training item(s).\n",
    "    Optimizes the same loss as EMF, but only for user u and their observed ratings.\n",
    "    Returns the updated latent vector for user u.\n",
    "    \"\"\"\n",
    "    P_u = P_init[u].copy()\n",
    "    Q_t = Q.T\n",
    "    I = train.shape[1]\n",
    "    for _ in range(steps):\n",
    "        for i in range(I):\n",
    "            if train[u, i] != 0:\n",
    "                e = train[u, i] - np.dot(P_u, Q_t[:, i])\n",
    "                for f in range(K):\n",
    "                    diff = P_u[f] - Q_t[f, i]\n",
    "                    grad = 2.0 * e * Q_t[f, i] - beta * P_u[f] - lamda * W[u, i] * diff\n",
    "                    P_u[f] += alpha * grad\n",
    "    return P_u\n",
    "\n",
    "def calc_avg(train):\n",
    "    \"\"\"\n",
    "    Computes the per-item average rating (ignoring zeros).\n",
    "    \"\"\"\n",
    "    sums   = np.sum(train, axis=0)\n",
    "    counts = np.count_nonzero(train, axis=0)\n",
    "    avg    = np.zeros_like(sums, dtype=float)\n",
    "    np.divide(sums, counts, out=avg, where=counts > 0)\n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0c7cf-32d9-4220-ab3b-2228cc685096",
   "metadata": {},
   "source": [
    "# ExAL selections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac6f3b-5808-4061-beb3-9fdafc1434dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def active_selection_exal_min(\n",
    "    u: int,\n",
    "    test_items: np.ndarray,   \n",
    "    pool: np.ndarray,\n",
    "    eR: np.ndarray,\n",
    "    lR: np.ndarray,\n",
    "    Q_dot: np.ndarray,\n",
    "    expl: np.ndarray,\n",
    "    alpha: float,\n",
    "    lamda: float\n",
    ") -> int:\n",
    "\n",
    "    best_score = np.inf\n",
    "    best_m = -1\n",
    "    num_items = pool.shape[1]\n",
    "\n",
    "    for m in range(num_items):\n",
    "        if pool[u, m] == 0:\n",
    "            continue\n",
    "        Rum = eR[u, m]\n",
    "        s = 0.0\n",
    "        for idx in range(test_items.shape[0]):  \n",
    "            j = test_items[idx]  \n",
    "            dp = Q_dot[m, j]\n",
    "            inside = (\n",
    "                1.0 - eR[u, j]\n",
    "                + 2.0 * alpha * (\n",
    "                    (Rum - lR[m]) * dp\n",
    "                    + lamda * expl[u, m] * (eR[u, j] - dp)\n",
    "                )\n",
    "            )\n",
    "            s += abs(inside)\n",
    "        if s < best_score:\n",
    "            best_score = s\n",
    "            best_m = m\n",
    "    return best_m\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_exal_max(\n",
    "    u: int,\n",
    "    test_items: np.ndarray,  \n",
    "    pool: np.ndarray,\n",
    "    eR: np.ndarray,\n",
    "    lR: np.ndarray,\n",
    "    Q_dot: np.ndarray,\n",
    "    expl: np.ndarray,\n",
    "    alpha: float,\n",
    "    lamda: float\n",
    ") -> int:\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_m = -1\n",
    "    num_items = pool.shape[1]\n",
    "\n",
    "    for m in range(num_items):\n",
    "        if pool[u, m] == 0:\n",
    "            continue\n",
    "        Rum = eR[u, m]\n",
    "        s = 0.0\n",
    "        for idx in range(test_items.shape[0]):  \n",
    "            j = test_items[idx]  \n",
    "            dp = Q_dot[m, j]\n",
    "            inside = (\n",
    "                1.0 - eR[u, j]\n",
    "                + 2.0 * alpha * (\n",
    "                    (Rum - lR[m]) * dp\n",
    "                    + lamda * expl[u, m] * (eR[u, j] - dp)\n",
    "                )\n",
    "            )\n",
    "            s += abs(inside)\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_m = m\n",
    "    return best_m\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_exal_max_min(\n",
    "    u: int,\n",
    "    test_items: np.ndarray,   \n",
    "    pool: np.ndarray,\n",
    "    eR: np.ndarray,\n",
    "    lR: np.ndarray,\n",
    "    Q_dot: np.ndarray,\n",
    "    expl: np.ndarray,\n",
    "    alpha: float,\n",
    "    lamda: float,\n",
    "    iteration: int,\n",
    "    switch_point: int = 5\n",
    ") -> int:\n",
    "\n",
    "    if iteration < switch_point:\n",
    "        return active_selection_exal_max(\n",
    "            u, test_items, pool, eR, lR, Q_dot, expl, alpha, lamda  \n",
    "        )\n",
    "    else:\n",
    "        return active_selection_exal_min(\n",
    "            u, test_items, pool, eR, lR, Q_dot, expl, alpha, lamda \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738efd38-0289-4b02-8bb2-bd5406abe419",
   "metadata": {},
   "source": [
    "# Active Learning Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2350a7-7037-4a2d-b6be-ad2856173bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def active_selection_karimi(\n",
    "   u: int,\n",
    "   test_items: np.ndarray,   \n",
    "   pool: np.ndarray,\n",
    "   eR: np.ndarray,\n",
    "   lR: np.ndarray,\n",
    "   Q_dot: np.ndarray,\n",
    "   alpha: float\n",
    ") -> int:\n",
    "\n",
    "   best_m = -1\n",
    "   best_score = 1e18\n",
    "   \n",
    "   for m in range(pool.shape[1]):\n",
    "       if pool[u, m] == 0:\n",
    "           continue\n",
    "       Rum = eR[u, m]\n",
    "       s = 0.0\n",
    "       \n",
    "       for idx in range(test_items.shape[0]): \n",
    "           j = test_items[idx]  \n",
    "           inside = (\n",
    "               1.0\n",
    "               - eR[u, j]  \n",
    "               + 2.0 * alpha * (Rum - lR[m]) * Q_dot[m, j]\n",
    "           )\n",
    "           s += abs(inside)\n",
    "           \n",
    "       if s < best_score:\n",
    "           best_score = s\n",
    "           best_m = m\n",
    "   return best_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804dafa-a0dc-4cb0-930d-ace84dc36f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def select_random(u, pool, rand_val):\n",
    "    \"\"\"\n",
    "    Random selection from user u's pool using rand_val in [0,1).\n",
    "    Ensures reproducibility. Returns −1 if pool is empty.\n",
    "    \"\"\"\n",
    "    # 1) Gather all valid item indices in the pool for user u\n",
    "    valid = []\n",
    "    for i in range(pool.shape[1]):\n",
    "        if pool[u, i] != 0:\n",
    "            valid.append(i)\n",
    "    # 2) If none, bail out\n",
    "    if len(valid) == 0:\n",
    "        return -1\n",
    "    # 3) Map rand_val to one of those indices\n",
    "    idx = int(rand_val * len(valid))\n",
    "    if idx >= len(valid):\n",
    "        idx = len(valid) - 1\n",
    "    return valid[idx]\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_uncertainty(u, pool, eR, midpoint=3.0):\n",
    "    \"\"\"\n",
    "    Uncertainty-based selection:\n",
    "    Select item with predicted rating closest to midpoint (default = 3.0).\n",
    "    \"\"\"\n",
    "    best_idx = -1\n",
    "    best_dist = np.inf\n",
    "    for i in range(pool.shape[1]):\n",
    "        if pool[u, i] != 0:\n",
    "            d = abs(eR[u, i] - midpoint)\n",
    "            if d < best_dist:\n",
    "                best_dist = d\n",
    "                best_idx = i\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_highest_pred(u, pool, eR):\n",
    "    best_idx = -1\n",
    "    best_score = -np.inf\n",
    "    I = pool.shape[1]\n",
    "    for i in range(I):\n",
    "        if pool[u, i] != 0:\n",
    "            s = eR[u, i]\n",
    "            if s > best_score:\n",
    "                best_score = s\n",
    "                best_idx = i\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def active_selection_highest_variance(u, pool, eR):\n",
    "    \"\"\"\n",
    "    Highest global variance across users for each item in u's pool.\n",
    "    \"\"\"\n",
    "    U, I = eR.shape\n",
    "    best_idx = -1\n",
    "    best_var = -1.0\n",
    "\n",
    "    # Precompute: mean and mean of squares for each item\n",
    "    item_mean = np.empty(I)\n",
    "    item_msq  = np.empty(I)\n",
    "    for i in range(I):\n",
    "        s = 0.0\n",
    "        ss = 0.0\n",
    "        for uu in range(U):\n",
    "            x = eR[uu, i]\n",
    "            s  += x\n",
    "            ss += x*x\n",
    "        item_mean[i] = s / U\n",
    "        item_msq[i]  = ss / U\n",
    "\n",
    "    for i in range(I):\n",
    "        if pool[u, i] == 0:\n",
    "            continue\n",
    "        var = item_msq[i] - item_mean[i]*item_mean[i]\n",
    "        if var > best_var:\n",
    "            best_var = var\n",
    "            best_idx = i\n",
    "\n",
    "    return best_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9ebb7-1188-44ff-8657-919bb3dc36db",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bd976-23b8-453c-b67e-b8536f6ddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def topn(eR, n, u):\n",
    "    \"\"\"\n",
    "    Return indices of top-n items for user u by predicted score.\n",
    "    \"\"\"\n",
    "    idx = np.argsort(-eR[u])\n",
    "    return idx[:n]\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def calculate_MER(eR, W, users, n):\n",
    "    \"\"\"\n",
    "    Mean Explainable Recall @n (MER):\n",
    "    Fraction of explainable items retrieved in top-n.\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    count_u = 0\n",
    "    U = users.shape[0]\n",
    "    I = W.shape[1]\n",
    "    for ui in range(U):\n",
    "        u = users[ui]\n",
    "        top = topn(eR, n, u)\n",
    "\n",
    "        # Count explainable items for user u\n",
    "        expl_total = 0\n",
    "        for j in range(I):\n",
    "            if W[u, j] > 0:\n",
    "                expl_total += 1\n",
    "        if expl_total == 0:\n",
    "            continue\n",
    "\n",
    "        # Count explainable items in top-n\n",
    "        cnt = 0\n",
    "        for idx in range(n):\n",
    "            k = top[idx]\n",
    "            if W[u, k] > 0:\n",
    "                cnt += 1\n",
    "\n",
    "        total += cnt / expl_total\n",
    "        count_u += 1\n",
    "\n",
    "    return total / count_u if count_u > 0 else 0.0\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def calculate_MEP(eR, W, users, n):\n",
    "    \"\"\"\n",
    "    Mean Explainable Precision @n (MEP):\n",
    "    Fraction of top-n items that are explainable.\n",
    "    \"\"\"\n",
    "    MEP = 0.0\n",
    "    total_expl = 0\n",
    "    total_n = 0\n",
    "    U = users.shape[0]\n",
    "\n",
    "    for ui in range(U):\n",
    "        u = users[ui]\n",
    "        top = topn(eR, n, u)\n",
    "\n",
    "        # Count explainable items in top-n\n",
    "        cnt = 0\n",
    "        for idx in range(n):\n",
    "            k = top[idx]\n",
    "            if W[u, k] > 0:\n",
    "                cnt += 1\n",
    "\n",
    "        MEP += cnt / n\n",
    "        total_expl += cnt\n",
    "        total_n += n\n",
    "\n",
    "    return (MEP / U if U > 0 else 0.0), total_expl, total_n\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def calculate_MAP(eR, test, users, n):\n",
    "    \"\"\"\n",
    "    Mean Average Precision @n (MAP):\n",
    "    Measures ranking quality for known test items.\n",
    "    \"\"\"\n",
    "    total_ap = 0.0\n",
    "    U = len(users)\n",
    "    I = test.shape[1]\n",
    "\n",
    "    for ui in range(U):\n",
    "        u = users[ui]\n",
    "\n",
    "        # Count relevant items for user u\n",
    "        rel = 0\n",
    "        for j in range(I):\n",
    "            if test[u, j] != 0:\n",
    "                rel += 1\n",
    "        if rel == 0:\n",
    "            continue\n",
    "\n",
    "        top = topn(eR, n, u)\n",
    "        hits = 0.0\n",
    "        sum_prec = 0.0\n",
    "\n",
    "        for rank in range(n):\n",
    "            j = top[rank]\n",
    "            if test[u, j] != 0:\n",
    "                hits += 1\n",
    "                sum_prec += hits / (rank + 1)\n",
    "\n",
    "        denom = n if rel >= n else rel\n",
    "        total_ap += sum_prec / denom\n",
    "\n",
    "    return total_ap / U if U > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_ndcg(eR, test, users, n):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain @n (NDCG):\n",
    "    Measures rank-sensitive relevance using ideal order baseline.\n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "    \"\"\"\n",
    "    ndcg_total = 0.0\n",
    "    valid_users = 0\n",
    "\n",
    "    for u in users:\n",
    "        rel = test[u]\n",
    "        top_n_pred = np.argsort(-eR[u])[:n]\n",
    "        dcg = 0.0\n",
    "        idcg = 0.0\n",
    "\n",
    "        ranked_rels = rel[top_n_pred]\n",
    "        for i in range(len(ranked_rels)):\n",
    "            if ranked_rels[i] > 0:\n",
    "                dcg += 1.0 / np.log2(i + 2)\n",
    "\n",
    "        ideal_rels = np.sort(rel)[::-1][:n]\n",
    "        for i in range(len(ideal_rels)):\n",
    "            if ideal_rels[i] > 0:\n",
    "                idcg += 1.0 / np.log2(i + 2)\n",
    "\n",
    "        if idcg > 0:\n",
    "            ndcg_total += dcg / idcg\n",
    "            valid_users += 1\n",
    "\n",
    "    return ndcg_total / valid_users if valid_users > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_novelty(topN_items_per_user, item_popularity):\n",
    "    \"\"\"\n",
    "    Novelty: −log2(popularity) averaged over top-N items for all users.\n",
    "    Lower popularity = higher novelty.\n",
    "    \n",
    "    This metric measures how much the recommender system promotes niche/rare items\n",
    "    versus popular/mainstream items. Higher novelty indicates better diversity.\n",
    "    \n",
    "    Args:\n",
    "        topN_items_per_user: List of arrays, each containing recommended item indices for one user\n",
    "        item_popularity: Array where item_popularity[i] = number of users who rated item i\n",
    "    \n",
    "    Returns:\n",
    "        float: Average novelty score across all recommendations (higher = more novel/diverse)\n",
    "    \n",
    "    Reference: https://castells.github.io/papers/recsys2011.pdf\n",
    "    \"\"\"\n",
    "    # Step 1: Add small epsilon to avoid log(0) mathematical error\n",
    "    # Items with 0 ratings would cause log(0) = undefined\n",
    "    # Adding 1e-6 (0.000001) ensures all items get a valid novelty score\n",
    "    popularity_safe = item_popularity + 1e-6  # Numerical stability\n",
    "    \n",
    "    # Step 2: Compute log2 of popularity for all items\n",
    "    # Why log2? It transforms multiplicative popularity differences into additive novelty differences\n",
    "    log_popularity = np.log2(popularity_safe)\n",
    "    \n",
    "    # Step 3: Initialize accumulators for averaging\n",
    "    total_novelty = 0.0  # Sum of all novelty scores\n",
    "    count = 0           # Total number of recommendations across all users\n",
    "\n",
    "    # Step 4: Iterate through each user's top-N recommendations\n",
    "    for top_items in topN_items_per_user:\n",
    "        # Step 4a: Compute novelty for this user's recommendations\n",
    "        # Novelty = -log2(popularity), so we negate the log values\n",
    "        # Why negative? Popular items (high log) should have low novelty\n",
    "        # Rare items (low log) should have high novelty\n",
    "        user_novelty = -np.sum(log_popularity[top_items])\n",
    "        \n",
    "        # Step 4b: Add to running total\n",
    "        total_novelty += user_novelty\n",
    "        \n",
    "        # Step 4c: Count number of items recommended to this user\n",
    "        count += len(top_items)\n",
    "\n",
    "    # Step 5: Return average novelty across all recommendations\n",
    "    # Dividing by count gives us the mean novelty per recommended item\n",
    "    return total_novelty / count if count > 0 else 0.0\n",
    "\n",
    "def calculate_novelty_EFD(topN_items_per_user, item_popularity):\n",
    "    \"\"\"\n",
    "    Expected Free Discovery (EFD): Novelty measure based on inverse collection frequency.\n",
    "    Higher EFD = recommending more rare/novel items (better for discovery).\n",
    "    \n",
    "    This differs from your existing novelty metric by using inverse frequency\n",
    "    rather than negative log popularity, making it more sensitive to rare items.\n",
    "    \n",
    "    Args:\n",
    "        topN_items_per_user: List of arrays, each containing recommended item indices for one user\n",
    "        item_popularity: Array where item_popularity[i] = number of users who rated item i\n",
    "    \n",
    "    Returns:\n",
    "        float: Average EFD score (higher = more novel recommendations)\n",
    "        \n",
    "    Reference: Vargas & Castells, RecSys 2011\n",
    "    \"\"\"\n",
    "    # Total number of users (for inverse frequency calculation)\n",
    "    total_users = np.sum(item_popularity > 0).astype(float)\n",
    "    if total_users == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate inverse frequency for each item\n",
    "    # IDF(i) = 1 / fraction of users who rated item i\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    item_freq = (item_popularity + 1e-6) / total_users\n",
    "    inverse_freq = 1.0 / item_freq\n",
    "    \n",
    "    # Calculate EFD\n",
    "    total_efd = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for top_items in topN_items_per_user:\n",
    "        # Sum of inverse frequencies for this user's recommendations\n",
    "        user_efd = np.sum(inverse_freq[top_items])\n",
    "        total_efd += user_efd\n",
    "        count += len(top_items)\n",
    "    \n",
    "    # Return average EFD per recommended item\n",
    "    return total_efd / count if count > 0 else 0.0\n",
    "\n",
    "def calculate_item_coverage(topN_items_all_users, num_items):\n",
    "    \"\"\"\n",
    "    Item Coverage: Fraction of catalog items that appear in recommendations.\n",
    "    Critical for ExAL - shows if explainability constraint causes filter bubbles.\n",
    "    \n",
    "    Args:\n",
    "        topN_items_all_users: List of arrays, each with top-N items for a user\n",
    "        num_items: Total number of items in catalog\n",
    "    \n",
    "    Returns:\n",
    "        float: Coverage ratio (0-1, higher is better)\n",
    "    \"\"\"\n",
    "    unique_items = set()\n",
    "    for user_items in topN_items_all_users:\n",
    "        unique_items.update(user_items)\n",
    "    return len(unique_items) / num_items\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gini_index(topN_items_all_users, num_items):\n",
    "    \"\"\"\n",
    "    Gini Index: Measures recommendation concentration (0=uniform, 1=concentrated).\n",
    "    Essential for ExAL - high Gini means recommending same explainable items to everyone.\n",
    "    \n",
    "    Args:\n",
    "        topN_items_all_users: List of recommendation arrays\n",
    "        num_items: Total catalog size\n",
    "        \n",
    "    Returns:\n",
    "        float: Gini coefficient (0-1, lower is better for diversity)\n",
    "    \"\"\"\n",
    "    # Count how many times each item is recommended\n",
    "    item_counts = np.zeros(num_items)\n",
    "    for user_items in topN_items_all_users:\n",
    "        for item in user_items:\n",
    "            item_counts[item] += 1\n",
    "    \n",
    "    # Sort counts for Gini calculation\n",
    "    item_counts = np.sort(item_counts)\n",
    "    n = len(item_counts)\n",
    "    index = np.arange(1, n + 1)\n",
    "    \n",
    "    # Gini coefficient formula\n",
    "    gini = (2 * np.sum(index * item_counts)) / (n * np.sum(item_counts)) - (n + 1) / n\n",
    "    return gini\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ARP(topN_items_all_users, item_popularity):\n",
    "    \"\"\"\n",
    "    Average Recommendation Popularity: Mean popularity of recommended items.\n",
    "    Crucial for ExAL - explainable items tend to be popular (many neighbors rated them).\n",
    "    \n",
    "    Args:\n",
    "        topN_items_all_users: List of recommendation arrays\n",
    "        item_popularity: Array where item_popularity[i] = number of users who rated item i\n",
    "        \n",
    "    Returns:\n",
    "        float: Average popularity (lower indicates more diverse/novel recommendations)\n",
    "    \"\"\"\n",
    "    total_popularity = 0\n",
    "    count = 0\n",
    "    \n",
    "    for user_items in topN_items_all_users:\n",
    "        for item in user_items:\n",
    "            total_popularity += item_popularity[item]\n",
    "            count += 1\n",
    "            \n",
    "    return total_popularity / count if count > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_item_popularity(rating_matrix):\n",
    "    \"\"\"\n",
    "    Reference: https://dl.acm.org/doi/abs/10.1145/3109859.3109912\n",
    "    \n",
    "    Compute item popularity as the count of nonzero ratings per item.\n",
    "    Popularity is defined as the number of users who rated each item.\n",
    "    \n",
    "    Args:\n",
    "        rating_matrix (np.ndarray): User-item rating matrix [num_users x num_items]\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 1D array of item popularity counts [num_items]\n",
    "    \"\"\"\n",
    "    # Count, for each item (column), how many users gave a nonzero rating.\n",
    "    # This results in an array where each entry is the popularity count for one item.\n",
    "    return np.count_nonzero(rating_matrix, axis=0)\n",
    "\n",
    "\n",
    "def assign_popularity_buckets(item_popularity):\n",
    "    \"\"\"\n",
    "    Assign items to 'low', 'medium', and 'high' popularity buckets (equal thirds).\n",
    "    This is used for popularity bias analysis.\n",
    "    \n",
    "    Args:\n",
    "        item_popularity (np.ndarray): Array of item popularity counts [num_items]\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 1D array with bucket assignments per item: 0=low, 1=medium, 2=high\n",
    "    \"\"\"\n",
    "    # Compute the popularity values at the 33.33% and 66.66% percentiles.\n",
    "    quantiles = np.percentile(item_popularity, [33.33, 66.66])\n",
    "    # Create array to hold bucket assignments (default 0 = low popularity).\n",
    "    buckets = np.zeros_like(item_popularity, dtype=int)\n",
    "    # Assign bucket label 2 (high popularity) to items above 66.66% quantile.\n",
    "    buckets[item_popularity > quantiles[1]] = 2\n",
    "    # Assign bucket label 1 (medium popularity) to items between 33.33% and 66.66% quantiles.\n",
    "    buckets[(item_popularity > quantiles[0]) & (item_popularity <= quantiles[1])] = 1\n",
    "    # Items at or below 33.33% quantile remain bucket 0 (low popularity).\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def fraction_by_popularity_bucket(topN_items, popularity_buckets):\n",
    "    \"\"\"\n",
    "    Given a set of recommended item indices (topN_items) and each item's popularity bucket,\n",
    "    compute the fraction of recommendations in each bucket (low/medium/high).\n",
    "    \n",
    "    Args:\n",
    "        topN_items (np.ndarray or list): Indices of recommended items\n",
    "        popularity_buckets (np.ndarray): Array of item bucket assignments (output of assign_popularity_buckets)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 1D array with fraction of items in each bucket [frac_low, frac_medium, frac_high]\n",
    "    \"\"\"\n",
    "    # Initialize counters for each bucket (0: low, 1: medium, 2: high).\n",
    "    bucket_counts = np.zeros(3)\n",
    "    # Iterate through each recommended item.\n",
    "    for idx in topN_items:\n",
    "        # Lookup the item's bucket and increment corresponding counter.\n",
    "        bucket = popularity_buckets[idx]\n",
    "        bucket_counts[bucket] += 1\n",
    "    # Normalize to obtain the fraction for each bucket.\n",
    "    return bucket_counts / len(topN_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fffbd28-67ff-4a98-89f7-4a3efb5bbd65",
   "metadata": {},
   "source": [
    "# Main experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033749f-bc0e-467d-9df6-ffccec63dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame utility: concatenate with dropping all-NaN columns\n",
    "def safe_concat(df_list, ignore_index=True):\n",
    "    \"\"\"\n",
    "    Concatenate DataFrames, dropping any columns that are all-NaN.\n",
    "    \"\"\"\n",
    "    cleaned = [df.dropna(axis=1, how='all') for df in df_list]\n",
    "    return pd.concat(cleaned, ignore_index=ignore_index)\n",
    "\n",
    "\n",
    "'''def get_lambda_for_strategy(strategy, lambda_value):\n",
    "    \"\"\"\n",
    "    Only return the real lambda_value for the three ExAL methods.\n",
    "    Otherwise (all other strategies) return 0 to disable the explainability term.\n",
    "    \"\"\"\n",
    "    return lambda_value if strategy in ('EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max') else 0.0'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da710b-55c3-4b08-8d4e-2bc2527045ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(lambda_value, strategy_input=None, return_results=False, seed=None, n_iter=num_iter, results_folder=None, dataset='100k', freeze_Q=True, theta=0.0, neighbor=20,RECOMPUTE_W_EACH_ITER=True):\n",
    "    global logger\n",
    "\n",
    "    # 1. Set output folders for results and popularity buckets\n",
    "    if results_folder is None:\n",
    "        results_folder = f\"Results_{neighbor}/seeds_results\"\n",
    "    pop_folder = f\"Results_{neighbor}/Popularity_Buckets\"\n",
    "\n",
    "    # 2. Initialize RNG for reproducibility\n",
    "    rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n",
    "\n",
    "    # 3. Ensure output directories exist\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    os.makedirs(pop_folder, exist_ok=True)\n",
    "\n",
    "    # 4. Load dataset and prepare rating matrix\n",
    "    data_M, _ = load_movielens(dataset)\n",
    "    rate = pd.DataFrame(data_M)\n",
    "\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"STARTING EXAL EXPERIMENT with freeze_Q=\"\n",
    "                f\"{freeze_Q}, lambda={lambda_value}, strategy={strategy_input}, \"\n",
    "                f\"dataset={dataset}, seed={seed}, n_iter={n_iter}, \"\n",
    "                f\"neighbor={neighbor}, theta={theta}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\n",
    "        f\"freeze_Q={freeze_Q} → \"\n",
    "        f\"{'fixed Q during AL' if freeze_Q else 'EMF updates per iteration'}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    fixed_test = None\n",
    "\n",
    "    train_init, test_init, pool_init, test_user = split(\n",
    "        rate,\n",
    "        num_test_users=fixed_test,\n",
    "        num_train_ratings=3,\n",
    "        num_test_ratings=20,   \n",
    "        min_pool_size=10,\n",
    "        rng=rng\n",
    "    )\n",
    "    # Enhanced logging for first 5 test users\n",
    "    logger.info(\"==== Data Split Verification (First 5 Test Users) ====\")\n",
    "\n",
    "    for i, u in enumerate(test_user[:5]):\n",
    "        train_idx = np.where(train_init[u] != 0)[0]\n",
    "        test_idx = np.where(test_init[u] != 0)[0]\n",
    "        pool_idx = np.where(pool_init[u] != 0)[0]\n",
    "        \n",
    "        logger.info(f\"User {u:3d}:\")\n",
    "        logger.info(f\"  - Train: {len(train_idx):2d} items - {train_idx.tolist()[:5]}{'...' if len(train_idx) > 5 else ''}\")\n",
    "        logger.info(f\"  - Test:  {len(test_idx):2d} items - {test_idx.tolist()[:5]}{'...' if len(test_idx) > 5 else ''} \")\n",
    "        logger.info(f\"  - Pool:  {len(pool_idx):2d} items - {pool_idx.tolist()[:5]}{'...' if len(pool_idx) > 5 else ''}\")\n",
    "    logger.info(\"=\"*55)\n",
    "    \n",
    "    \n",
    "    # 6. Log split statistics for the first 5 test users\n",
    "    logger.info(\"==== Data Split Check for First 5 Test Users ====\")\n",
    "    for u in test_user[:5]:\n",
    "        train_idx = np.where(train_init[u] != 0)[0]\n",
    "        test_idx = np.where(test_init[u] != 0)[0]\n",
    "        pool_idx = np.where(pool_init[u] != 0)[0]\n",
    "        logger.info(f\"User {u:3d}: train={len(train_idx)}, test={len(test_idx)}, pool={len(pool_idx)}\")\n",
    "    logger.info(\"==============================================\")\n",
    "\n",
    "    # 7. Compute per-item average rating, item popularity, and assign popularity buckets\n",
    "    lR = calc_avg(train_init)\n",
    "\n",
    "    item_popularity = compute_item_popularity(train_init)\n",
    "    popularity_buckets = assign_popularity_buckets(item_popularity)\n",
    "\n",
    "    # 8. Select strategies to run (user-defined or default list)\n",
    "    if strategy_input:\n",
    "        strategies = [strategy_input]\n",
    "    else:\n",
    "        strategies = [\n",
    "            'EXAL-Min', \n",
    "            'EXAL-Max',\n",
    "            'EXAL-Min-Max',\n",
    "            'KARIMI', \n",
    "            'Uncertainty', \n",
    "            'Random', \n",
    "            'HighestPred', \n",
    "            'HighestVar'\n",
    "        ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 9. Loop through each active learning strategy\n",
    "    for strategy in strategies:\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"STRATEGY: {strategy}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "\n",
    "        # Reset splits and RNG for fair comparison\n",
    "        rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n",
    "        train = np.copy(train_init)\n",
    "        test = np.copy(test_init)\n",
    "        pool = np.copy(pool_init)\n",
    "\n",
    "        # Initialize per-iteration metrics storage\n",
    "        evolution = pd.DataFrame(columns=[\"Iteration\", \"MAE\", \"MEP\", \"MER\", \"F-Score\", \"MAP\"])\n",
    "        \n",
    "        #lamda_used = get_lambda_for_strategy(strategy, LAMDA)\n",
    "\n",
    "        lamda_used = float(lambda_value)\n",
    "\n",
    "        logger.info(f\"Running {strategy} with lamda_used={lamda_used}\")\n",
    "\n",
    "        # 10. Initialize latent factors; compute initial explainability matrix\n",
    "        P_init, Q_init, _, _, expl = initialize_model(\n",
    "            train_init, test_init, rng,\n",
    "            steps=INIT_STEPS, alpha=ALPHA_INIT, beta=BETA,\n",
    "            K=K, neighbor=neighbor, theta=theta, lamda=lamda_used \n",
    "        )\n",
    "\n",
    "        # Log initial explainability matrix sparsity\n",
    "        logger.info(\"\\n[Explainability Matrix Sparsity Check]\")\n",
    "        logger.info(\"Initial explainable items per user (first 10 users):\")\n",
    "        for u in range(min(10, expl.shape[0])):\n",
    "            num_expl = int(np.sum(expl[u] > 0))\n",
    "            total_items = expl.shape[1]\n",
    "            perc = 100.0 * num_expl / total_items\n",
    "            logger.info(f\"  User {u:2d}: {num_expl:4d} / {total_items} items explainable ({perc:.2f}%)\")\n",
    "        logger.info(\"-\" * 55)\n",
    "\n",
    "        # Deep copy latent factors for use in AL loop\n",
    "        P, Q = np.copy(P_init), np.copy(Q_init)\n",
    "\n",
    "        # Initialize MAE tracking\n",
    "        train_mae_list = []\n",
    "        test_mae_list = []\n",
    "\n",
    "        # 11. Active Learning loop  (single loop)\n",
    "        for iteration in tqdm.tqdm(range(n_iter), desc=f\"[{strategy}] AL Iter\"):\n",
    "            logger.info(f\"\\n--- Iteration {iteration} ---\")\n",
    "\n",
    "            # Update per-item averages\n",
    "            lR = calc_avg(train)\n",
    "            if lamda_used > 0 and RECOMPUTE_W_EACH_ITER:\n",
    "                expl = calc_exp(train, neighbor=neighbor, theta=theta)\n",
    "\n",
    "\n",
    "            # Predictions for selection + item–item dots\n",
    "            eR = P.dot(Q.T)\n",
    "            Q_dot = Q.dot(Q.T)\n",
    "\n",
    "            # Per-user selection + online update\n",
    "            for u in tqdm.tqdm(test_user, desc=\" Users\", leave=False):\n",
    "                test_items = np.where(test[u, :] != 0)[0]\n",
    "\n",
    "                if strategy == 'EXAL-Min':\n",
    "                    j = active_selection_exal_min(u, test_items, pool, eR, lR, Q_dot, expl, ALPHA_RETRAIN, lamda_used)\n",
    "                elif strategy == 'EXAL-Max':\n",
    "                    j = active_selection_exal_max(u, test_items, pool, eR, lR, Q_dot, expl, ALPHA_RETRAIN, lamda_used)\n",
    "                elif strategy == 'EXAL-Min-Max':\n",
    "                    j = active_selection_exal_max_min(u, test_items, pool, eR, lR, Q_dot, expl,\n",
    "                                                    ALPHA_RETRAIN, lamda_used, iteration, switch_point=SWITCH)\n",
    "                elif strategy == 'KARIMI':\n",
    "                    j = active_selection_karimi(u, test_items, pool, eR, lR, Q_dot, ALPHA_RETRAIN)\n",
    "                elif strategy == 'Random':\n",
    "                    j = select_random(u, pool, rng.random())\n",
    "                elif strategy == 'Uncertainty':\n",
    "                    j = active_selection_uncertainty(u, pool, eR, midpoint=3.0)\n",
    "                elif strategy == 'HighestPred':\n",
    "                    j = active_selection_highest_pred(u, pool, eR)\n",
    "                elif strategy == 'HighestVar':\n",
    "                    j = active_selection_highest_variance(u, pool, eR)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "                if j >= 0:\n",
    "                    # move picked rating to train\n",
    "                    train[u, j] = pool[u, j]\n",
    "                    pool[u, j]  = 0\n",
    "                    lR = calc_avg(train)\n",
    "\n",
    "                    # refresh explainability row for this user (fast mode only)\n",
    "                    if lamda_used > 0 and not RECOMPUTE_W_EACH_ITER:\n",
    "                        expl[u, :] = calc_exp_row(train, u, neighbor=neighbor, theta=theta)\n",
    "\n",
    "\n",
    "                    # online update of P_u\n",
    "                    P[u] = retrain_online_exp(u, train, P, Q, expl, ALPHA_RETRAIN, BETA, K, ONLINE_STEP, lamda_used)\n",
    "\n",
    "            # optional EMF pass to update Q (and P) set False to activate online updates for both P and Q\n",
    "            if not freeze_Q:\n",
    "                P, Q, _ = EMF_with_explainability(train, P, Q, K,\n",
    "                                                expl, lamda_used,\n",
    "                                                steps=ONLINE_STEP,\n",
    "                                                alpha=ALPHA_RETRAIN,\n",
    "                                                beta=BETA)\n",
    "                logger.info(f\"[Iter {iteration}] Q updated (mean={Q.mean():.4f}, std={Q.std():.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "            # === recompute predictions AFTER updates (for metrics) ===\n",
    "            eR = P.dot(Q.T)\n",
    "\n",
    "            # --- MAE ---\n",
    "            mae_train = np.nanmean(np.abs(eR[train != 0] - train[train != 0]))\n",
    "            mae_test  = np.nanmean(np.abs(eR[test  != 0] - test [test  != 0]))\n",
    "            train_mae_list.append(mae_train)\n",
    "            test_mae_list.append(mae_test)\n",
    "            logger.info(f\"[{strategy}] Iter {iteration}: TRAIN MAE={mae_train:.4f}, TEST MAE={mae_test:.4f}\")\n",
    "\n",
    "            # --- W stats ---\n",
    "            num_expl_nonzero = np.sum(expl > 0)\n",
    "            total_entries    = expl.shape[0] * expl.shape[1]\n",
    "            percent_nonzero  = 100 * num_expl_nonzero / total_entries\n",
    "            logger.info(f\"[W Sparsity] Non-zero W entries: {num_expl_nonzero}/{total_entries} ({percent_nonzero:.4f}%)\")\n",
    "\n",
    "            percent_exp_user = np.sum(expl > 0, axis=1) / expl.shape[1]\n",
    "            mean_cov = np.mean(percent_exp_user) * 100\n",
    "            top5     = np.sort(percent_exp_user)[-5:] * 100\n",
    "            bot5     = np.sort(percent_exp_user)[:5] * 100\n",
    "            logger.info(f\"[W Coverage] Mean explainable items/user: {mean_cov:.2f}%\")\n",
    "            logger.info(f\"Top 5 users w/ most explainable items: {top5}\")\n",
    "            logger.info(f\"Bottom 5 users w/ least explainable items: {bot5}\")\n",
    "\n",
    "            # --- Metrics ---\n",
    "            mask = (train != 0) | (pool != 0)\n",
    "            eR_masked = eR.copy()\n",
    "            eR_masked[mask] = -np.inf\n",
    "\n",
    "            MEP, total_expl, total_n = calculate_MEP(eR_masked, expl, test_user, TopN)\n",
    "            MER = calculate_MER(eR_masked, expl, test_user, TopN)\n",
    "            F   = 2*(MEP*MER)/(MEP+MER) if (MEP+MER)>0 else 0.0\n",
    "            MAPv= calculate_MAP(eR_masked, test, test_user, TopN)\n",
    "\n",
    "            topN_items, all_top = [], []\n",
    "            for u in test_user:\n",
    "                s = eR[u].copy()\n",
    "                s[mask[u]] = -np.inf\n",
    "                top = np.argsort(s)[-TopN:][::-1]\n",
    "                topN_items.append(top)\n",
    "                all_top.extend(top)\n",
    "\n",
    "            coverage = calculate_item_coverage(topN_items, train.shape[1])\n",
    "            gini = calculate_gini_index(topN_items, train.shape[1])\n",
    "            arp  = calculate_ARP(topN_items, item_popularity)\n",
    "            gini_diversity = 1 - gini\n",
    "            novelty     = calculate_novelty(topN_items, item_popularity)\n",
    "            novelty_efd = calculate_novelty_EFD(topN_items, item_popularity)\n",
    "            ndcg        = calculate_ndcg(eR_masked, test, test_user, TopN)\n",
    "            frac_pop    = fraction_by_popularity_bucket(np.array(all_top), popularity_buckets)\n",
    "            frac_bias   = frac_pop[0] - frac_pop[2]\n",
    "\n",
    "            mae_high, mae_low = [], []\n",
    "            for u in test_user:\n",
    "                recs = topN_items[test_user.tolist().index(u)]\n",
    "                high = [i for i in recs if popularity_buckets[i]==2 and test[u,i]!=0]\n",
    "                low  = [i for i in recs if popularity_buckets[i]==0 and test[u,i]!=0]\n",
    "                if high: mae_high.append(np.mean(np.abs(eR[u,high] - test[u,high])))\n",
    "                if low:  mae_low .append(np.mean(np.abs(eR[u,low]  - test[u,low])))\n",
    "            mean_high = np.nan if not mae_high else np.mean(mae_high)\n",
    "            mean_low  = np.nan if not mae_low  else np.mean(mae_low)\n",
    "            mae_bias  = mean_low - mean_high\n",
    "\n",
    "            pop_df = pd.DataFrame({\n",
    "                \"Iteration\":[iteration],\n",
    "                \"Frac_Low\": [frac_pop[0]],\n",
    "                \"Frac_Med\": [frac_pop[1]],\n",
    "                \"Frac_High\":[frac_pop[2]],\n",
    "            })\n",
    "            pop_key = f\"{strategy}_lambda_{lambda_value}_{dataset}\"\n",
    "            pop_path= os.path.join(pop_folder, f\"{pop_key}{'_seed_'+str(seed) if seed else ''}.csv\")\n",
    "            if os.path.exists(pop_path):\n",
    "                old = pd.read_csv(pop_path)\n",
    "                pop_df = pd.concat([old, pop_df], ignore_index=True)\n",
    "            pop_df.to_csv(pop_path, index=False)\n",
    "\n",
    "            iteration_df = pd.DataFrame({\n",
    "                \"Iteration\":       [iteration],\n",
    "                \"MAE\":             [mae_test],\n",
    "                \"Train_MAE\":       [mae_train],\n",
    "                \"Overfit_Gap\":     [mae_test-mae_train],\n",
    "                \"MEP\":             [MEP],\n",
    "                \"MER\":             [MER],\n",
    "                \"F-Score\":         [F],\n",
    "                \"MAP\":             [MAPv],\n",
    "                \"NDCG\":            [ndcg],\n",
    "                \"Novelty\":         [novelty],\n",
    "                \"Novelty_EFD\":     [novelty_efd],\n",
    "                \"Gini\":            [gini_diversity],\n",
    "                \"Coverage\":        [coverage],\n",
    "                \"ARP\":             [arp],\n",
    "                \"MAE_HighPop\":     [mean_high],\n",
    "                \"MAE_LowPop\":      [mean_low],\n",
    "                \"MAE_Pop_Bias\":    [mae_bias],\n",
    "                \"Frac_Bias\":       [frac_bias],\n",
    "                \"Total_Explained\": [total_expl],\n",
    "                \"Total_Candidates\":[total_n]\n",
    "            })\n",
    "            evolution = safe_concat([evolution, iteration_df], ignore_index=True)\n",
    "            logger.info(\"Iteration stats:\\n\" + str(iteration_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # === Save strategy results to disk ===\n",
    "        key = f\"{strategy}_lambda_{lambda_value}_{dataset}\"\n",
    "        results[key] = evolution\n",
    "        if not return_results:\n",
    "            path = os.path.join(results_folder, f\"{key}{'_seed_'+str(seed) if seed else ''}.csv\")\n",
    "            evolution.to_csv(path, index=False)\n",
    "            logger.info(f\"Saved results to {path}\")\n",
    "\n",
    "            # Log MAE progress\n",
    "            logger.info(f\"Iteration {iteration} complete:\")\n",
    "            logger.info(f\"  - Train MAE: {mae_train:.4f}\")\n",
    "            logger.info(f\"  - Test MAE: {mae_test:.4f}\")\n",
    "            logger.info(f\"  - MEP: {MEP:.4f}, MER: {MER:.4f}, F-Score: {F:.4f}\")\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"EXPERIMENT COMPLETE\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "    return results if return_results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c4d36-729d-4aec-af6e-bf750edd6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_seed_experiment(lambda_value, strategy_input=None, seeds=None, n_iter=num_iter,\n",
    "                          results_folder=None, dataset='100k', freeze_Q=True,\n",
    "                          theta=0.0, neighbor=20, recompute_w_each_iter=True):    \n",
    "    \"\"\"\n",
    "    Run active learning experiments for multiple seeds and average results.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Set default folders for results\n",
    "    if results_folder is None:\n",
    "        results_folder = f\"Results_{neighbor}\"\n",
    "    seeds_folder = os.path.join(results_folder, \"seeds_results\")\n",
    "    pop_folder = os.path.join(results_folder, \"Popularity_Buckets\")\n",
    "\n",
    "    # 2. Default seeds if not provided\n",
    "    if seeds is None:\n",
    "        seeds = [42, 101, 202, 303, 404, 505, 606, 707, 808, 909]\n",
    "        #seeds = [42]  # (for debugging)\n",
    "\n",
    "    # 3. Ensure output directories exist\n",
    "    os.makedirs(seeds_folder, exist_ok=True)\n",
    "    os.makedirs(pop_folder, exist_ok=True)\n",
    "    logger.info(f\"Running multi-seed experiment with seeds: {seeds}\")\n",
    "\n",
    "    # 4. Prepare result storage\n",
    "    results_all = {}  \n",
    "\n",
    "    # 5. Run experiment for each seed\n",
    "    for seed in seeds:\n",
    "        # Ensure RNG state is controlled\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        logger.info(f\"==== Seed {seed} ====\")\n",
    "\n",
    "        # Run single-seed experiment\n",
    "        results = main(\n",
    "            lambda_value,\n",
    "            strategy_input=strategy_input,\n",
    "            return_results=True,\n",
    "            seed=seed,\n",
    "            n_iter=n_iter,\n",
    "            results_folder=seeds_folder,\n",
    "            dataset=dataset,\n",
    "            theta=theta,\n",
    "            neighbor=neighbor,\n",
    "            freeze_Q=freeze_Q,\n",
    "            RECOMPUTE_W_EACH_ITER=recompute_w_each_iter   \n",
    "        )\n",
    "\n",
    "        # Save and collect results\n",
    "        for strategy_key, df in results.items():\n",
    "            out_path = os.path.join(seeds_folder, f\"{strategy_key}_seed_{seed}.csv\")\n",
    "            df.to_csv(out_path, index=False)\n",
    "            logger.info(f\"Saved: {out_path}\")\n",
    "\n",
    "            if strategy_key not in results_all:\n",
    "                results_all[strategy_key] = []\n",
    "            results_all[strategy_key].append(df.copy())\n",
    "\n",
    "    # 6. Average results across seeds for each strategy\n",
    "    for strategy_key, dfs in results_all.items():\n",
    "        # Create MultiIndex DataFrame from list of runs\n",
    "        concat_df = pd.concat(dfs, keys=range(len(dfs)), names=['Seed', 'Row'])\n",
    "\n",
    "        # Group by iteration index and average across seeds\n",
    "        avg_df = concat_df.groupby('Iteration').mean(numeric_only=True).reset_index()\n",
    "\n",
    "        avg_csv = os.path.join(seeds_folder, f\"AVG_{strategy_key}.csv\")\n",
    "        avg_df.to_csv(avg_csv, index=False)\n",
    "        logger.info(f\"Averaged results for {strategy_key} saved to {avg_csv}\")\n",
    "\n",
    "    logger.info(f\"Multi-seed experiment completed. Averaged results in {seeds_folder}/.\")\n",
    "\n",
    "    # 7. Post-process and average popularity results for each strategy\n",
    "    strategies = [\n",
    "        'EXAL-Min', \n",
    "        'EXAL-Max',\n",
    "        'EXAL-Min-Max',\n",
    "        'KARIMI', \n",
    "        'Uncertainty', \n",
    "        'Random', \n",
    "        'HighestPred', \n",
    "        'HighestVar'\n",
    "    ]\n",
    "    for strat in strategies:\n",
    "        # Pattern: grab all CSVs with per-seed Popularity bucket results\n",
    "        pattern = os.path.join(pop_folder, f\"{strat}_lambda_{lambda_value}_{dataset}_seed_*.csv\")\n",
    "        files = [f for f in glob.glob(pattern) if not os.path.basename(f).startswith('AVG_')]\n",
    "\n",
    "        avg_file = os.path.join(pop_folder, f\"AVG_{strat}_lambda_{lambda_value}_{dataset}.csv\")\n",
    "\n",
    "        if len(files) == 1:\n",
    "            # Only one file → copy it\n",
    "            shutil.copyfile(files[0], avg_file)\n",
    "            logger.info(f\"[INFO] Only one Popularity file for {strat}: copied {files[0]} → {avg_file}\")\n",
    "\n",
    "        elif len(files) > 1:\n",
    "            # Multiple seeds → average them\n",
    "            dfs = [pd.read_csv(f) for f in files]\n",
    "            concat = pd.concat(dfs, keys=range(len(dfs)), names=['Seed', 'Row'])\n",
    "            avg_df = concat.groupby('Iteration').mean(numeric_only=True).reset_index()\n",
    "            avg_df.to_csv(avg_file, index=False)\n",
    "            logger.info(f\"[INFO] Averaged Popularity results for {strat} saved to {avg_file}\")\n",
    "\n",
    "        else:\n",
    "            # No files found\n",
    "            logger.warning(f\"[WARN] No Popularity files found for {strat}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0827762-6716-41cd-9bd8-88fac49bc5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    import argparse\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        force=True\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if 'ipykernel' in sys.argv[0]:\n",
    "        sys.argv = [sys.argv[0]]\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Active Learning Experiment for MovieLens'\n",
    "    )\n",
    "    parser.add_argument('--lambda_value', type=float, default=LAMDA,\n",
    "                        help='Lambda regularization parameter')\n",
    "    parser.add_argument('--strategy', type=str, default=None,\n",
    "                        help='AL strategy (e.g., \"EXAL-Min\")')\n",
    "    parser.add_argument('--dataset', type=str, default='100k',\n",
    "                        choices=['100k', '1m'],\n",
    "                        help='MovieLens dataset: 100k or 1m')\n",
    "    parser.add_argument('--theta', type=float, default=0.0,\n",
    "                        help='Explainability threshold (theta) for W_{ui}')\n",
    "    parser.add_argument('--neighbor', type=int, default=NEIGHBOR,\n",
    "                        help='Number of neighbors for explainability matrix W')\n",
    "    parser.add_argument('--freeze_Q', dest='freeze_Q', action='store_true',\n",
    "                        help='Freeze item factors Q during AL iterations (paper-faithful).')\n",
    "    parser.add_argument('--no-freeze_Q', dest='freeze_Q', action='store_false',\n",
    "                        help='Allow iteration-end EMF updates to Q (deviation; experimental).')\n",
    "    parser.set_defaults(freeze_Q=False)\n",
    "    parser.add_argument('--recompute_w_each_iter', action='store_true', default=True) # True by default if we want to recompute W each iteration False for fast mode\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    lambda_v = args.lambda_value\n",
    "    strategy = args.strategy\n",
    "    dataset = args.dataset\n",
    "    theta = args.theta\n",
    "    freeze_Q = args.freeze_Q\n",
    "    neighbor = args.neighbor\n",
    "\n",
    "    multi_seed_experiment(\n",
    "        lambda_v,\n",
    "        strategy_input=strategy,\n",
    "        seeds=None,\n",
    "        n_iter=num_iter,\n",
    "        results_folder=None,\n",
    "        dataset=dataset,\n",
    "        freeze_Q=freeze_Q,\n",
    "        theta=theta,\n",
    "        neighbor=neighbor,\n",
    "        recompute_w_each_iter=args.recompute_w_each_iter)   \n",
    "'''\n",
    "\n",
    "    # Loop over each lambda value\n",
    "    for lambda_v in LAMDAs:\n",
    "        logger.info(f\"\\n=== Running experiments for LAMDA = {lambda_v} ===\")\n",
    "        multi_seed_experiment(\n",
    "            lambda_v,\n",
    "            strategy_input=strategy,\n",
    "            seeds=None,\n",
    "            n_iter=num_iter,\n",
    "            results_folder=None,  \n",
    "            dataset=dataset,\n",
    "            freeze_Q=freeze_Q,\n",
    "            theta=theta,\n",
    "            neighbor=neighbor,\n",
    "            recompute_w_each_iter=args.recompute_w_each_iter\n",
    "        )'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921c016-97ab-499c-844f-a5bf9f2417ae",
   "metadata": {},
   "source": [
    "# PLOT the Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ff188",
   "metadata": {},
   "source": [
    "### Metrics Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc89483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Setup logging for experiment traceability\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ======= Experiment global constants (set these as needed) =======\n",
    "LAMDA      = LAMDA        # Explainability regularization strength\n",
    "TopN       = TopN         # Number of recommendations per user\n",
    "dataset    = dataset      # Dataset name: '100k' or '1m'\n",
    "STEPint    = INIT_STEPS   # Steps for MF model initialization\n",
    "STEPonl    = ONLINE_STEP  # Steps for online MF updates\n",
    "num_iter   = num_iter     # Active learning iterations\n",
    "SWITCH     = SWITCH       # Switch point for EXAL-Min-Max hybrid\n",
    "NEIGHBOR   = NEIGHBOR     # k for k-nearest neighbor explainability\n",
    "\n",
    "# ======= Define plot styles for each strategy =======\n",
    "styles = {\n",
    "    'KARIMI':               {'color': 'blue',      'marker': 'o', 'linestyle': '-'},\n",
    "    'Random':               {'color': 'green',     'marker': 's', 'linestyle': '-'},\n",
    "    'HighestVar':           {'color': 'cyan',      'marker': '^', 'linestyle': '-'},\n",
    "    'HighestPred':          {'color': 'magenta',   'marker': 'v', 'linestyle': '-'},\n",
    "    'Uncertainty':          {'color': 'orange',    'marker': 'x', 'linestyle': '-'},\n",
    "    'EXAL-Min':             {'color': 'gold',      'marker': 'D', 'linestyle': '-'},\n",
    "    'EXAL-Max':             {'color': 'black',     'marker': '*', 'linestyle': '--'},  \n",
    "    'EXAL-Min-Max':         {'color': 'red',       'marker': 'X', 'linestyle': '--'}  \n",
    "}\n",
    "\n",
    "# ======= Metric labels and directions for visualization =======\n",
    "labels = {\n",
    "    'MAE':          'Mean Absolute Error (MAE) ↓ better',\n",
    "    'MEP':          'Mean Explainable Precision (MEP) ↑ better', \n",
    "    'MER':          'Mean Explainable Recall (MER) ↑ better',\n",
    "    'F-Score':      'Explainable F-Score ↑ better',\n",
    "    'MAP':          'Mean Average Precision (MAP) ↑ better'\n",
    "}\n",
    "direction_info = {\n",
    "    'MAE':          {'better': 'lower', 'arrow': '↓', 'color': 'green'},\n",
    "    'MEP':          {'better': 'higher', 'arrow': '↑', 'color': 'green'}, \n",
    "    'MER':          {'better': 'higher', 'arrow': '↑', 'color': 'green'},\n",
    "    'F-Score':      {'better': 'higher', 'arrow': '↑', 'color': 'green'},\n",
    "    'MAP':          {'better': 'higher', 'arrow': '↑', 'color': 'green'}\n",
    "}\n",
    "method_categories = {\n",
    "    'Baselines': ['KARIMI', 'Random', 'HighestVar', 'HighestPred','Uncertainty'],\n",
    "    'Original ExAL': ['EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max']\n",
    "}\n",
    "\n",
    "neighbor = NEIGHBOR\n",
    "logger.info(f\"Generating enhanced plots for NEIGHBOR = {neighbor}\")\n",
    "\n",
    "results_folder = f\"Results_{neighbor}/seeds_results\"\n",
    "plot_folder    = f\"Results_{neighbor}/Plots_Enhanced_Metrics\"\n",
    "os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "#  Map each method to its results CSV path \n",
    "csv_files = {\n",
    "    method: os.path.join(\n",
    "        results_folder,\n",
    "        f\"AVG_{method}_lambda_{LAMDA}_{dataset}.csv\"\n",
    "    )\n",
    "    for method in styles\n",
    "}\n",
    "def ensure_zero_row(df, metric):\n",
    "    \"\"\"Guarantee an Iteration==0 row; if missing, copy the first row and set Iteration=0.\"\"\"\n",
    "    if (df['Iteration'] == 0).any():\n",
    "        return df\n",
    "    first = df.iloc[0].copy()\n",
    "    first['Iteration'] = 0\n",
    "    return (pd.concat([pd.DataFrame([first]), df], ignore_index=True)\n",
    "              .sort_values('Iteration'))\n",
    "    \n",
    "    \n",
    "def create_organized_legend(ax, dfs):\n",
    "    \"\"\"Organize legend by strategy category for clearer comparison.\"\"\"\n",
    "    legend_elements = []\n",
    "    for category, methods in method_categories.items():\n",
    "        category_methods = [m for m in methods if m in dfs and m in styles]\n",
    "        if category_methods:\n",
    "            legend_elements.append(plt.Line2D([0], [0], color='none', label=f'─── {category} ───'))\n",
    "            for method in category_methods:\n",
    "                style = styles[method]\n",
    "                legend_elements.append(\n",
    "                    plt.Line2D([0], [0], \n",
    "                              color=style['color'], \n",
    "                              marker=style['marker'],\n",
    "                              linestyle=style['linestyle'],\n",
    "                              linewidth=2, \n",
    "                              markersize=7,\n",
    "                              label=method)\n",
    "                )\n",
    "    return ax.legend(handles=legend_elements, fontsize=12, \n",
    "                    loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "def highlight_best_performers(ax, dfs, metric):\n",
    "    \"\"\"Highlight the top-performing method in the final iteration.\"\"\"\n",
    "    if not dfs:\n",
    "        return\n",
    "    final_values = {m: df[metric].iloc[-1] for m, df in dfs.items() if not df.empty}\n",
    "    if not final_values:\n",
    "        return\n",
    "    is_lower_better = direction_info[metric]['better'] == 'lower'\n",
    "    best_method = min(final_values, key=final_values.get) if is_lower_better else max(final_values, key=final_values.get)\n",
    "    best_value = final_values[best_method]\n",
    "    if best_method in dfs:\n",
    "        df = dfs[best_method]\n",
    "        final_iter = df['Iteration'].iloc[-1]\n",
    "        ax.scatter(final_iter, best_value, \n",
    "                  s=150, \n",
    "                  facecolors='none', \n",
    "                  edgecolors='red', \n",
    "                  linewidths=3,\n",
    "                  label=f'Best: {best_method}')\n",
    "\n",
    "def add_direction_indicator(ax, metric, ylow, yhigh):\n",
    "    \"\"\"Add a text/arrow to indicate if higher/lower values are preferred for the metric.\"\"\"\n",
    "    # DISABLED: Direction indicator removed for cleaner plots\n",
    "    pass\n",
    "\n",
    "def analyze_trends(dfs, metric):\n",
    "    \"\"\"Compute improvement trend from first to last AL iteration for each method.\"\"\"\n",
    "    trends = {}\n",
    "    direction = direction_info[metric]\n",
    "    is_lower_better = direction['better'] == 'lower'\n",
    "    for method, df in dfs.items():\n",
    "        if len(df) < 2:\n",
    "            continue\n",
    "        first_val = df[metric].iloc[0]\n",
    "        last_val = df[metric].iloc[-1]\n",
    "        improvement = (first_val - last_val) if is_lower_better else (last_val - first_val)\n",
    "        trend = 'improving' if improvement > 0 else 'declining'\n",
    "        trends[method] = {\n",
    "            'improvement': improvement,\n",
    "            'trend': trend,\n",
    "            'first': first_val,\n",
    "            'last': last_val\n",
    "        }\n",
    "    return trends\n",
    "\n",
    "#  Plot metrics across all AL strategies \n",
    "for metric, ylabel in labels.items():\n",
    "    plt.figure(figsize=(16, 10))  # Large, clear plots for paper-quality figures\n",
    "    dfs = {}\n",
    "    min_val, max_val = np.inf, -np.inf\n",
    "\n",
    "    # Load results from CSVs\n",
    "    for method, path in csv_files.items():\n",
    "        if not os.path.isfile(path):\n",
    "            logger.debug(f\"Missing file for {method}: {path}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if metric not in df.columns:\n",
    "                logger.debug(f\"Metric {metric} not in file for {method}: {path}\")\n",
    "                continue\n",
    "            df_clean = df[df[metric].notna() & np.isfinite(df[metric])]\n",
    "            if df_clean.empty:\n",
    "                logger.warning(f\"No valid data for {metric} in {method}\")\n",
    "                continue\n",
    "            dfs[method] = df_clean\n",
    "            min_val = min(min_val, df_clean[metric].min())\n",
    "            max_val = max(max_val, df_clean[metric].max())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading {method}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not dfs:\n",
    "        logger.warning(f\"No data for metric {metric} at NEIGHBOR={neighbor}\")\n",
    "        plt.close()\n",
    "        continue\n",
    "\n",
    "    margin = 0.05 * (max_val - min_val) if max_val > min_val else 0.01\n",
    "    ylow  = min_val - margin\n",
    "    yhigh = max_val + margin\n",
    "\n",
    "    # Plot each strategy's learning curve\n",
    "    for method, df in dfs.items():\n",
    "        style = styles[method]\n",
    "        linewidth = 3.5 if 'EXAL' in method else 3.0\n",
    "        markersize = 7 if 'EXAL' in method else 7\n",
    "        alpha = 1.0 if 'EXAL' in method else 0.8\n",
    "        plt.plot(\n",
    "            df['Iteration'], df[metric],\n",
    "            label=method,\n",
    "            linewidth=linewidth,\n",
    "            marker=style['marker'],\n",
    "            color=style['color'],\n",
    "            linestyle=style['linestyle'],\n",
    "            markersize=markersize,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "    # Highlight top performer and add better direction\n",
    "    highlight_best_performers(plt.gca(), dfs, metric)\n",
    "    add_direction_indicator(plt.gca(), metric, ylow, yhigh)\n",
    "\n",
    "    # Plot formatting for clarity and publication\n",
    "    plt.xlabel('Active Learning Iteration', fontsize=20, fontweight='bold')\n",
    "    plt.ylabel(ylabel, fontsize=20, fontweight='bold')\n",
    "    direction = direction_info[metric]\n",
    "    plt.title(\n",
    "        f\"{ylabel.split('(')[0].strip()} Evolution Across AL Iterations\\n\"\n",
    "        f\"Dataset: MovieLens-{dataset.upper()} | λ={LAMDA} | Top-N={TopN} | \"\n",
    "        f\"Neighbors={neighbor} |{direction['arrow']} Better\",\n",
    "        fontsize=17, fontweight='bold', pad=25\n",
    "    )\n",
    "    plt.grid(True, alpha=0.7, linestyle='-', linewidth=0.9)\n",
    "    plt.ylim(ylow, yhigh)\n",
    "    max_iter = max(df['Iteration'].max() for df in dfs.values())\n",
    "    plt.xticks(range(0, int(max_iter) + 1), fontsize=20)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    # Legend and layout\n",
    "    create_organized_legend(plt.gca(), dfs)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ----- Save plots in all desired formats -----\n",
    "    save_path_png = os.path.join(plot_folder, f\"Enhanced_{metric}_Comparison_LAMDA_({LAMDA}).png\")\n",
    "    save_path_pdf = save_path_png.replace('.png', '.pdf')\n",
    "    save_path_svg = save_path_png.replace('.png', '.svg')\n",
    "\n",
    "    plt.savefig(save_path_png, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.savefig(save_path_pdf, bbox_inches='tight', facecolor='white')\n",
    "    plt.savefig(save_path_svg, bbox_inches='tight', facecolor='white')\n",
    "    logger.info(f\"Saved PNG plot:  {save_path_png}\")\n",
    "    logger.info(f\"Saved PDF plot:  {save_path_pdf}\")\n",
    "    logger.info(f\"Saved SVG plot:  {save_path_svg}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Trend analysis for report or log\n",
    "    trends = analyze_trends(dfs, metric)\n",
    "    logger.info(f\"\\n=== TRENDS ANALYSIS FOR {metric} ===\")\n",
    "    for method, trend_info in sorted(trends.items(), key=lambda x: x[1]['improvement'], reverse=True):\n",
    "        logger.info(f\"{method:20}: {trend_info['trend']:10} \"\n",
    "                   f\"({trend_info['first']:.4f} → {trend_info['last']:.4f}, \"\n",
    "                   f\"Δ={trend_info['improvement']:+.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef13e2",
   "metadata": {},
   "source": [
    "## t-test significance vs. Random baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, shapiro, wilcoxon, normaltest\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy import stats\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # These need to be set with actual values before running\n",
    "        self.lambda_value = LAMDA  # Replace with actual lambda value\n",
    "        self.neighbor = NEIGHBOR   # Replace with actual neighbor value  \n",
    "        self.dataset = dataset     # Replace with actual dataset name\n",
    "        \n",
    "        self.all_methods = [\n",
    "            'EXAL-Min', 'EXAL-Max', 'EXAL-Min-Max', \n",
    "            'KARIMI', 'Random', 'HighestPred', 'HighestVar', 'Uncertainty'\n",
    "        ]\n",
    "        self.reference_method = 'Random'\n",
    "        self.metrics = [\"MAE\", \"MEP\", \"MER\", \"F-Score\", \"MAP\"]\n",
    "        \n",
    "        # Metric information with correct names\n",
    "        self.metric_info = {\n",
    "            \"MAE\": {\n",
    "                \"direction\": False,  # Lower is better\n",
    "                \"full_name\": \"Mean Absolute Error\",\n",
    "                \"description\": \"Average prediction error\",\n",
    "                \"expected_improvement\": \"decrease\"\n",
    "            },\n",
    "            \"MEP\": {\n",
    "                \"direction\": True,   # Higher is better\n",
    "                \"full_name\": \"Mean Explainable Precision\",\n",
    "                \"description\": \"Fraction of top-N items that are explainable\",\n",
    "                \"expected_improvement\": \"increase\"\n",
    "            },\n",
    "            \"MER\": {\n",
    "                \"direction\": True,   # Higher is better\n",
    "                \"full_name\": \"Mean Explainable Recall\",\n",
    "                \"description\": \"Fraction of explainable items retrieved in top-N\",\n",
    "                \"expected_improvement\": \"increase\"\n",
    "            },\n",
    "            \"F-Score\": {\n",
    "                \"direction\": True,   # Higher is better\n",
    "                \"full_name\": \"Explainable F-Score\",\n",
    "                \"description\": \"Harmonic mean of MEP and MER\",\n",
    "                \"expected_improvement\": \"increase\"\n",
    "            },\n",
    "            \"MAP\": {\n",
    "                \"direction\": True,   # Higher is better\n",
    "                \"full_name\": \"Mean Average Precision\",\n",
    "                \"description\": \"Ranking quality for known test items\",\n",
    "                \"expected_improvement\": \"increase\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.metric_direction = {k: v[\"direction\"] for k, v in self.metric_info.items()}\n",
    "        self.alpha = 0.05  # Significance level\n",
    "        self.use_two_sided = True  # Use two-sided tests for exploratory analysis\n",
    "        self.min_sample_size = 3\n",
    "        self.result_dir = f\"Results_{self.neighbor}/seeds_results\"\n",
    "        self.stat_dir = \"stat_results\"\n",
    "        os.makedirs(self.stat_dir, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === STATISTICAL HELPER FUNCTIONS ===\n",
    "\n",
    "def cohens_d_paired(x, y):\n",
    "    \"\"\"Calculate Cohen's d effect size for paired samples.\"\"\"\n",
    "    differences = np.array(x) - np.array(y)\n",
    "    mean_diff = np.mean(differences)\n",
    "    std_diff = np.std(differences, ddof=1)  # Sample standard deviation\n",
    "    \n",
    "    if std_diff == 0:\n",
    "        return 0.0 if mean_diff == 0 else np.inf\n",
    "    \n",
    "    return mean_diff / std_diff\n",
    "\n",
    "def interpret_effect_size(d):\n",
    "    \"\"\"Interpret Cohen's d effect size according to conventional guidelines.\"\"\"\n",
    "    abs_d = abs(d)\n",
    "    if abs_d < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif abs_d < 0.5:\n",
    "        return \"small\"\n",
    "    elif abs_d < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def check_normality(data, alpha=0.05):\n",
    "    \"\"\"Check normality of data using multiple tests.\"\"\"\n",
    "    data = np.array(data)\n",
    "    n = len(data)\n",
    "    \n",
    "    results = {\n",
    "        'sample_size': n,\n",
    "        'is_normal': False,\n",
    "        'shapiro_stat': np.nan,\n",
    "        'shapiro_p': np.nan,\n",
    "        'dagostino_stat': np.nan,\n",
    "        'dagostino_p': np.nan,\n",
    "        'test_used': 'None'\n",
    "    }\n",
    "    \n",
    "    if n < 3:\n",
    "        results['test_used'] = 'Insufficient data'\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        # Shapiro-Wilk test (preferred for small samples)\n",
    "        if n <= 50:\n",
    "            stat, p_val = shapiro(data)\n",
    "            results.update({\n",
    "                'shapiro_stat': stat,\n",
    "                'shapiro_p': p_val,\n",
    "                'is_normal': p_val > alpha,\n",
    "                'test_used': 'Shapiro-Wilk'\n",
    "            })\n",
    "        else:\n",
    "            # D'Agostino-Pearson test for larger samples\n",
    "            stat, p_val = normaltest(data)\n",
    "            results.update({\n",
    "                'dagostino_stat': stat,\n",
    "                'dagostino_p': p_val,\n",
    "                'is_normal': p_val > alpha,\n",
    "                'test_used': 'D\\'Agostino-Pearson'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        results['test_used'] = f'Error: {str(e)}'\n",
    "    \n",
    "    return results\n",
    "\n",
    "# === DATA LOADING ===\n",
    "def load_data(config, strategy='final_only', num_iterations=3, min_iteration=5):\n",
    "    \"\"\"Load data for all methods from individual seed files with multiple strategies.\"\"\"\n",
    "    print(f\"\\n===== LOADING DATA: NEIGHBOR = {config.neighbor} =====\")\n",
    "    print(f\"Data extraction strategy: {strategy}\")\n",
    "    \n",
    "    # Initialize storage with metadata\n",
    "    method_results = {m: {metric: [] for metric in config.metrics} \n",
    "                     for m in config.all_methods}\n",
    "    \n",
    "    # Also store metadata for advanced analysis\n",
    "    method_metadata = {m: {'iterations': [], 'seeds': [], 'temporal_order': []} \n",
    "                      for m in config.all_methods}\n",
    "    \n",
    "    for method in config.all_methods:\n",
    "        # Find all seed files for this method\n",
    "        pattern = os.path.join(\n",
    "            config.result_dir, \n",
    "            f\"{method}_lambda_{config.lambda_value}_{config.dataset}_seed_*.csv\"\n",
    "        )\n",
    "        files = sorted(glob.glob(pattern))\n",
    "        \n",
    "        print(f\"  Found {len(files)} files for {method}\")\n",
    "        \n",
    "        for file_idx, file_path in enumerate(files):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if not df.empty and 'Iteration' in df.columns:\n",
    "                    \n",
    "                    # Extract seed number from filename for tracking\n",
    "                    seed_num = file_idx  # Fallback\n",
    "                    if '_seed_' in file_path:\n",
    "                        try:\n",
    "                            seed_num = int(file_path.split('_seed_')[1].split('.')[0])\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Select rows based on strategy\n",
    "                    if strategy == 'final_only':\n",
    "                        rows_to_use = [len(df)-1]  # Last row index\n",
    "                        \n",
    "                    elif strategy == 'last_n':\n",
    "                        start_idx = max(0, len(df) - num_iterations)\n",
    "                        rows_to_use = list(range(start_idx, len(df)))\n",
    "                        \n",
    "                    elif strategy == 'all_iterations':\n",
    "                        rows_to_use = list(range(len(df)))\n",
    "                        \n",
    "                    elif strategy == 'convergence_period':\n",
    "                        rows_to_use = [i for i in range(len(df)) if df.iloc[i]['Iteration'] >= min_iteration]\n",
    "                        \n",
    "                    elif strategy == 'early_late':\n",
    "                        early_rows = list(range(min(3, len(df))))\n",
    "                        late_rows = list(range(max(0, len(df)-3), len(df)))\n",
    "                        rows_to_use = early_rows + late_rows\n",
    "                    \n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "                    \n",
    "                    # Store each metric value from selected rows\n",
    "                    for row_idx in rows_to_use:\n",
    "                        row = df.iloc[row_idx]\n",
    "                        iteration = row.get('Iteration', row_idx)\n",
    "                        \n",
    "                        for metric in config.metrics:\n",
    "                            if metric in df.columns:\n",
    "                                value = row.get(metric, np.nan)\n",
    "                                # Only store valid numeric values\n",
    "                                if not pd.isna(value) and np.isfinite(value):\n",
    "                                    method_results[method][metric].append(value)\n",
    "                                    \n",
    "                                    # Store metadata for advanced analysis\n",
    "                                    method_metadata[method]['iterations'].append(iteration)\n",
    "                                    method_metadata[method]['seeds'].append(seed_num)\n",
    "                                    method_metadata[method]['temporal_order'].append(len(method_results[method][metric])-1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    # Print comprehensive sample information\n",
    "    print(f\"\\nSample sizes per method (strategy: {strategy}):\")\n",
    "    for method in config.all_methods:\n",
    "        sample_size = len(method_results[method]['MAE'])\n",
    "        unique_seeds = len(set(method_metadata[method]['seeds'])) if method_metadata[method]['seeds'] else 0\n",
    "        avg_iterations_per_seed = sample_size / unique_seeds if unique_seeds > 0 else 0\n",
    "        print(f\"  {method}: {sample_size} observations from {unique_seeds} seeds \"\n",
    "              f\"({avg_iterations_per_seed:.1f} iterations/seed)\")\n",
    "    \n",
    "    return method_results, method_metadata\n",
    "\n",
    "# === STATISTICAL ANALYSIS ===\n",
    "def perform_comprehensive_statistical_tests(config, method_results):\n",
    "    \"\"\"Perform comprehensive statistical tests comparing each method to the baseline.\"\"\"\n",
    "    print(f\"\\n===== COMPREHENSIVE STATISTICAL ANALYSIS vs {config.reference_method} =====\")\n",
    "    print(f\"Test type: {'Two-sided' if config.use_two_sided else 'One-sided'}\")\n",
    "    print(f\"Significance level: α = {config.alpha}\")\n",
    "    print(f\"Multiple testing correction: Bonferroni\")\n",
    "    \n",
    "    records = []\n",
    "    ref_data = method_results[config.reference_method]\n",
    "    \n",
    "    # STEP 1: Perform all pairwise comparisons\n",
    "    for method in config.all_methods:\n",
    "        if method == config.reference_method:\n",
    "            continue  # Skip self-comparison\n",
    "            \n",
    "        method_data = method_results[method]\n",
    "        \n",
    "        for metric in config.metrics:\n",
    "            print(f\"\\nAnalyzing {method} vs {config.reference_method} on {metric}...\")\n",
    "            \n",
    "            # Extract paired values (same seeds for both methods)\n",
    "            ref_vals = np.array(ref_data[metric])\n",
    "            method_vals = np.array(method_data[metric])\n",
    "            \n",
    "            # STEP 2: Data validation and cleaning\n",
    "            valid_idx = ~(pd.isna(ref_vals) | pd.isna(method_vals) | \n",
    "                         np.isinf(ref_vals) | np.isinf(method_vals))\n",
    "            ref_vals_clean = ref_vals[valid_idx]\n",
    "            method_vals_clean = method_vals[valid_idx]\n",
    "            \n",
    "            # Check minimum sample size\n",
    "            n_pairs = len(ref_vals_clean)\n",
    "            if n_pairs < config.min_sample_size:\n",
    "                print(f\"  Skipped: Insufficient data (n={n_pairs} < {config.min_sample_size})\")\n",
    "                continue\n",
    "            \n",
    "            # STEP 3: Descriptive statistics\n",
    "            ref_mean = np.mean(ref_vals_clean)\n",
    "            ref_std = np.std(ref_vals_clean, ddof=1)\n",
    "            method_mean = np.mean(method_vals_clean)\n",
    "            method_std = np.std(method_vals_clean, ddof=1)\n",
    "            \n",
    "            # Calculate differences for paired analysis\n",
    "            differences = method_vals_clean - ref_vals_clean\n",
    "            mean_diff = np.mean(differences)\n",
    "            std_diff = np.std(differences, ddof=1)\n",
    "            \n",
    "            # Calculate descriptive statistics for differences\n",
    "            diff_min = np.min(differences)\n",
    "            diff_max = np.max(differences)\n",
    "            diff_median = np.median(differences)\n",
    "            \n",
    "            # Relative change calculation\n",
    "            if abs(ref_mean) > 1e-10:  # Avoid division by zero\n",
    "                percent_change = (mean_diff / abs(ref_mean)) * 100\n",
    "            else:\n",
    "                percent_change = np.nan\n",
    "            \n",
    "            # STEP 4: Check normality of differences\n",
    "            normality_results = check_normality(differences, config.alpha)\n",
    "            is_normal = normality_results['is_normal']\n",
    "            \n",
    "            print(f\"  Sample size: n={n_pairs}\")\n",
    "            print(f\"  Mean difference: {mean_diff:.6f} ({percent_change:+.2f}%)\")\n",
    "            print(f\"  Normality test: {normality_results['test_used']}, p={normality_results.get('shapiro_p', normality_results.get('dagostino_p', 'N/A')):.4f}\")\n",
    "            print(f\"  Distribution: {'Normal' if is_normal else 'Non-normal'}\")\n",
    "            \n",
    "            # STEP 5: Determine test direction\n",
    "            if config.use_two_sided:\n",
    "                alternative = 'two-sided'\n",
    "                print(f\"  Test direction: Two-sided (exploratory)\")\n",
    "            else:\n",
    "                # One-sided test based on expected improvement direction\n",
    "                is_higher_better = config.metric_direction[metric]\n",
    "                if is_higher_better:\n",
    "                    alternative = 'greater'  # H1: method > baseline\n",
    "                    print(f\"  Test direction: One-sided (expect method > baseline)\")\n",
    "                else:\n",
    "                    alternative = 'less'     # H1: method < baseline  \n",
    "                    print(f\"  Test direction: One-sided (expect method < baseline)\")\n",
    "            \n",
    "            # STEP 6: Choose and perform appropriate statistical test\n",
    "            try:\n",
    "                if std_diff == 0:\n",
    "                    # No variance in differences\n",
    "                    p_value = 1.0 if mean_diff == 0 else 0.0\n",
    "                    test_used = 'No variance in differences'\n",
    "                    test_statistic = np.nan\n",
    "                    print(f\"  Test: {test_used}\")\n",
    "                    \n",
    "                elif is_normal and n_pairs >= 5:\n",
    "                    # PAIRED T-TEST (parametric)\n",
    "                    t_stat, p_value = ttest_rel(method_vals_clean, ref_vals_clean, \n",
    "                                               alternative=alternative)\n",
    "                    test_used = 'Paired t-test'\n",
    "                    test_statistic = t_stat\n",
    "                    degrees_freedom = n_pairs - 1\n",
    "                    print(f\"  Test: Paired t-test, t({degrees_freedom})={t_stat:.4f}, p={p_value:.6f}\")\n",
    "                    \n",
    "                else:\n",
    "                    # WILCOXON SIGNED-RANK TEST (non-parametric)\n",
    "                    non_zero_diff = differences[differences != 0]\n",
    "                    if len(non_zero_diff) == 0:\n",
    "                        p_value = 1.0\n",
    "                        test_used = 'All differences are zero'\n",
    "                        test_statistic = np.nan\n",
    "                        print(f\"  Test: All differences are zero\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            w_stat, p_value = wilcoxon(differences, \n",
    "                                                     alternative=alternative,\n",
    "                                                     zero_method='wilcox',\n",
    "                                                     mode='auto')\n",
    "                            test_used = 'Wilcoxon signed-rank test'\n",
    "                            test_statistic = w_stat\n",
    "                            print(f\"  Test: Wilcoxon signed-rank, W={w_stat:.4f}, p={p_value:.6f}\")\n",
    "                        except Exception as e:\n",
    "                            # Fallback for edge cases\n",
    "                            p_value = np.nan\n",
    "                            test_used = f'Wilcoxon failed: {str(e)}'\n",
    "                            test_statistic = np.nan\n",
    "                            print(f\"  Test: Wilcoxon failed - {str(e)}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                p_value = np.nan\n",
    "                test_used = f'Test failed: {str(e)}'\n",
    "                test_statistic = np.nan\n",
    "                print(f\"  Test: Failed - {str(e)}\")\n",
    "            \n",
    "            # STEP 7: Calculate effect size\n",
    "            effect_size = cohens_d_paired(method_vals_clean, ref_vals_clean)\n",
    "            effect_interpretation = interpret_effect_size(effect_size)\n",
    "            print(f\"  Effect size: Cohen's d = {effect_size:.4f} ({effect_interpretation})\")\n",
    "            \n",
    "            # STEP 8: Determine significance and performance\n",
    "            is_significant = not pd.isna(p_value) and p_value < config.alpha\n",
    "            \n",
    "            # Determine performance direction\n",
    "            if pd.isna(p_value):\n",
    "                performance = 'Test Failed'\n",
    "            elif not is_significant:\n",
    "                performance = 'No Difference'\n",
    "            else:\n",
    "                # Significant - determine if better or worse\n",
    "                is_higher_better = config.metric_direction[metric]\n",
    "                method_is_better = (mean_diff > 0) if is_higher_better else (mean_diff < 0)\n",
    "                performance = 'Better' if method_is_better else 'Worse'\n",
    "            \n",
    "            print(f\"  Result: {performance} (p={p_value:.6f}, {'significant' if is_significant else 'not significant'})\")\n",
    "            \n",
    "            # STEP 9: Store comprehensive results\n",
    "            record = {\n",
    "                # Method and metric info\n",
    "                'Method': method,\n",
    "                'Metric': metric,\n",
    "                'Metric_Direction': 'Higher better' if config.metric_direction[metric] else 'Lower better',\n",
    "                \n",
    "                # Sample information\n",
    "                'Sample_Size': n_pairs,\n",
    "                'Valid_Pairs': n_pairs,\n",
    "                \n",
    "                # Descriptive statistics\n",
    "                'Method_Mean': method_mean,\n",
    "                'Method_Std': method_std,\n",
    "                'Ref_Mean': ref_mean,\n",
    "                'Ref_Std': ref_std,\n",
    "                \n",
    "                # Difference statistics\n",
    "                'Mean_Diff': mean_diff,\n",
    "                'Std_Diff': std_diff,\n",
    "                'Diff_Min': diff_min,\n",
    "                'Diff_Max': diff_max,\n",
    "                'Diff_Median': diff_median,\n",
    "                'Percent_Change': percent_change,\n",
    "                \n",
    "                # Normality testing\n",
    "                'Normality_Test': normality_results['test_used'],\n",
    "                'Normality_Statistic': normality_results.get('shapiro_stat', \n",
    "                                                           normality_results.get('dagostino_stat', np.nan)),\n",
    "                'Normality_P': normality_results.get('shapiro_p', \n",
    "                                                    normality_results.get('dagostino_p', np.nan)),\n",
    "                'Is_Normal': is_normal,\n",
    "                \n",
    "                # Statistical test results\n",
    "                'Test_Used': test_used,\n",
    "                'Test_Statistic': test_statistic,\n",
    "                'Alternative_Hypothesis': alternative,\n",
    "                'P_Value': p_value,\n",
    "                'Is_Significant': is_significant,\n",
    "                \n",
    "                # Effect size\n",
    "                'Effect_Size': effect_size,\n",
    "                'Effect_Interpretation': effect_interpretation,\n",
    "                \n",
    "                # Performance assessment\n",
    "                'Performance': performance\n",
    "            }\n",
    "            \n",
    "            records.append(record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(records)\n",
    "    \n",
    "    # STEP 10: Apply multiple testing correction\n",
    "    if len(results_df) > 0:\n",
    "        print(f\"\\n===== MULTIPLE TESTING CORRECTION =====\")\n",
    "        \n",
    "        # Extract valid p-values for correction\n",
    "        p_values = results_df['P_Value'].values\n",
    "        valid_p_mask = ~pd.isna(p_values)\n",
    "        \n",
    "        if np.sum(valid_p_mask) > 0:\n",
    "            # Apply Bonferroni correction\n",
    "            valid_p = p_values[valid_p_mask]\n",
    "            n_tests = len(valid_p)\n",
    "            \n",
    "            print(f\"Number of tests performed: {n_tests}\")\n",
    "            print(f\"Bonferroni correction: α_corrected = {config.alpha}/{n_tests} = {config.alpha/n_tests:.6f}\")\n",
    "            \n",
    "            # Perform correction\n",
    "            reject, corrected_p, alpha_sidak, alpha_bonf = multipletests(\n",
    "                valid_p, \n",
    "                alpha=config.alpha, \n",
    "                method='bonferroni'\n",
    "            )\n",
    "            \n",
    "            # Initialize corrected arrays\n",
    "            corrected_p_values = np.full_like(p_values, np.nan)\n",
    "            significant_after_correction = np.zeros(len(p_values), dtype=bool)\n",
    "            \n",
    "            # Store corrected values\n",
    "            corrected_p_values[valid_p_mask] = corrected_p\n",
    "            significant_after_correction[valid_p_mask] = reject\n",
    "            \n",
    "            # Add to results DataFrame\n",
    "            results_df['P_Value_Corrected'] = corrected_p_values\n",
    "            results_df['Significant_After_Correction'] = significant_after_correction\n",
    "            \n",
    "            # Update performance based on corrected significance\n",
    "            def update_performance(row):\n",
    "                if pd.isna(row['P_Value']):\n",
    "                    return 'Test Failed'\n",
    "                elif not row['Significant_After_Correction']:\n",
    "                    return 'No Difference'\n",
    "                else:\n",
    "                    return row['Performance']  # Keep original Better/Worse designation\n",
    "            \n",
    "            results_df['Performance_Corrected'] = results_df.apply(update_performance, axis=1)\n",
    "            \n",
    "            # Summary of correction impact\n",
    "            n_significant_before = results_df['Is_Significant'].sum()\n",
    "            n_significant_after = results_df['Significant_After_Correction'].sum()\n",
    "            print(f\"Significant results before correction: {n_significant_before}\")\n",
    "            print(f\"Significant results after correction: {n_significant_after}\")\n",
    "            print(f\"Results lost to multiple testing: {n_significant_before - n_significant_after}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def aggregate_by_seed(values, seeds):\n",
    "    \"\"\"Aggregate values by seed (helper function for data alignment).\"\"\"\n",
    "    d = {}\n",
    "    for v, s in zip(values, seeds):\n",
    "        d.setdefault(s, []).append(float(v))\n",
    "    return {s: np.mean(vs) for s, vs in d.items()}\n",
    "\n",
    "def align_by_seed(method_vals, method_seeds, ref_vals, ref_seeds):\n",
    "    \"\"\"Align method and reference values by matching seeds.\"\"\"\n",
    "    m = aggregate_by_seed(method_vals, method_seeds)\n",
    "    r = aggregate_by_seed(ref_vals, ref_seeds)\n",
    "    common = sorted(set(m) & set(r))\n",
    "    return np.array([m[s] for s in common]), np.array([r[s] for s in common])\n",
    "\n",
    "# === CLEAN VISUALIZATION ===\n",
    "def create_clean_heatmap(config, df, strategy='unknown'):\n",
    "    \"\"\"Create a clean, professional heatmap for statistical comparisons.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for heatmap\")\n",
    "        return\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure with better dimensions\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    \n",
    "    # Prepare data - exclude reference method from display\n",
    "    methods = [m for m in config.all_methods if m != config.reference_method]\n",
    "    metrics = config.metrics\n",
    "    \n",
    "    # Create data matrix\n",
    "    heat_data = np.zeros((len(metrics), len(methods)))\n",
    "    annotations = np.full((len(metrics), len(methods)), \"\", dtype=object)\n",
    "    \n",
    "    # Fill data\n",
    "    for _, row in df.iterrows():\n",
    "        if row['Method'] not in methods:\n",
    "            continue\n",
    "            \n",
    "        method_idx = methods.index(row['Method'])\n",
    "        metric_idx = metrics.index(row['Metric'])\n",
    "        \n",
    "        p_val = row['P_Value']\n",
    "        performance = row['Performance']\n",
    "        percent_change = row['Percent_Change']\n",
    "        effect_size = row['Effect_Size']\n",
    "        \n",
    "        # Color coding: -1 to 1 scale\n",
    "        if pd.isna(p_val) or performance == 'No Difference':\n",
    "            heat_data[metric_idx, method_idx] = 0\n",
    "            annotations[metric_idx, method_idx] = f\"ns\\n{percent_change:+.1f}%\"\n",
    "        elif performance == 'Better':\n",
    "            if p_val < 0.001:\n",
    "                heat_data[metric_idx, method_idx] = 1.0\n",
    "                annotations[metric_idx, method_idx] = f\"***\\n{percent_change:+.1f}%\"\n",
    "            elif p_val < 0.01:\n",
    "                heat_data[metric_idx, method_idx] = 0.75\n",
    "                annotations[metric_idx, method_idx] = f\"**\\n{percent_change:+.1f}%\"\n",
    "            else:\n",
    "                heat_data[metric_idx, method_idx] = 0.5\n",
    "                annotations[metric_idx, method_idx] = f\"*\\n{percent_change:+.1f}%\"\n",
    "        else:  # Worse\n",
    "            if p_val < 0.001:\n",
    "                heat_data[metric_idx, method_idx] = -1.0\n",
    "                annotations[metric_idx, method_idx] = f\"***\\n{percent_change:+.1f}%\"\n",
    "            elif p_val < 0.01:\n",
    "                heat_data[metric_idx, method_idx] = -0.75\n",
    "                annotations[metric_idx, method_idx] = f\"**\\n{percent_change:+.1f}%\"\n",
    "            else:\n",
    "                heat_data[metric_idx, method_idx] = -0.5\n",
    "                annotations[metric_idx, method_idx] = f\"*\\n{percent_change:+.1f}%\"\n",
    "    \n",
    "    # Create enhanced metric labels\n",
    "    metric_labels = []\n",
    "    for metric in metrics:\n",
    "        direction = \"↓\" if not config.metric_info[metric][\"direction\"] else \"↑\"\n",
    "        metric_labels.append(f\"{metric} {direction}\")\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(heat_data, cmap='RdYlGn', aspect='auto', vmin=-1, vmax=1)\n",
    "    \n",
    "    # Add annotations with proper sizing\n",
    "    for i in range(len(metrics)):\n",
    "        for j in range(len(methods)):\n",
    "            text = annotations[i, j]\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=17, fontweight='bold')\n",
    "    \n",
    "    # Set ticks and labels with proper spacing\n",
    "    ax.set_xticks(np.arange(len(methods)))\n",
    "    ax.set_yticks(np.arange(len(metrics)))\n",
    "    \n",
    "    # Fix overlapping method names\n",
    "    ax.set_xticklabels(methods, rotation=0, ha='center', fontsize=13)\n",
    "    ax.set_yticklabels(metric_labels, fontsize=12)\n",
    "    \n",
    "    # Add clean grid\n",
    "    ax.set_xticks(np.arange(len(methods)+1)-0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(metrics)+1)-0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", size=0)\n",
    "    \n",
    "    # Remove major ticks\n",
    "    ax.tick_params(which=\"major\", length=0)\n",
    "    \n",
    "    # Strategy display mapping\n",
    "    strategy_display = {\n",
    "        'final_only': 'Final Iteration Only',\n",
    "        'all_iterations': 'All Iterations', \n",
    "        'convergence_period': 'Convergence Period (Iter 5+)',\n",
    "        'last_n': 'Last N Iterations',\n",
    "        'early_late': 'Early + Late Iterations'\n",
    "    }.get(strategy, strategy)\n",
    "    \n",
    "    # Summarize which statistical test was used\n",
    "    if 'Test_Used' in df.columns:\n",
    "        test_counts = df['Test_Used'].value_counts()\n",
    "        if len(test_counts) == 1:\n",
    "            test_summary = test_counts.index[0]\n",
    "        else:\n",
    "            test_summary = f\"Mixed ({test_counts.index[0]}: {test_counts.iloc[0]})\"\n",
    "    else:\n",
    "        test_summary = \"Unknown\"\n",
    "    \n",
    "    # Alternative hypothesis summary\n",
    "    if 'Alternative_Hypothesis' in df.columns:\n",
    "        alt_counts = df['Alternative_Hypothesis'].value_counts()\n",
    "        alt_summary = alt_counts.index[0] if len(alt_counts) == 1 else \"Mixed\"\n",
    "    else:\n",
    "        alt_summary = \"Unknown\"\n",
    "    \n",
    "    # Titles and labels\n",
    "    ax.set_title(\n",
    "        f\"Statistical Comparison vs {config.reference_method} Baseline \"\n",
    "        f\"Strategy: {strategy_display} | λ={config.lambda_value} | Neighbors={config.neighbor}\\n\"\n",
    "        f\"Test: {test_summary} | Alternative: {alt_summary}\",\n",
    "        fontsize=15, fontweight='bold', pad=25\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Active Learning Methods', fontsize=15, labelpad=15)\n",
    "    ax.set_ylabel('Evaluation Metrics', fontsize=15, labelpad=15)\n",
    "    \n",
    "    # Add colorbar with better positioning\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.6, aspect=20, pad=0.02)\n",
    "    cbar.set_label('Performance vs Random', fontsize=15, labelpad=15)\n",
    "    cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "    cbar.set_ticklabels(['Much Worse', 'Worse', 'No Diff', 'Better', 'Much Better'], fontsize=12)\n",
    "    \n",
    "    # Calculate Bonferroni-corrected alpha\n",
    "    num_tests = len(df)\n",
    "    alpha_corrected = config.alpha / num_tests if num_tests > 0 else np.nan\n",
    "\n",
    "    legend_text = (\n",
    "        f\"GREEN = Better than Random | RED = Worse than Random | YELLOW = No significant difference\\n\"\n",
    "        f\"*** p<0.001 | ** p<0.01 | * p<0.05 | ns = not significant | ↑ = Higher is better | ↓ = Lower is better\\n\"\n",
    "        f\"Bonferroni-corrected α = {alpha_corrected:.6f} (Original α = {config.alpha}) for {num_tests} tests\"\n",
    "    )\n",
    "    \n",
    "    plt.figtext(0.5, 0.08, legend_text, ha='center', fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.18, top=0.90, left=0.12, right=0.95)\n",
    "    \n",
    "    # Save with strategy in filename\n",
    "    base_path = os.path.join(config.stat_dir, f\"heatmap_{strategy}_neighbor_{config.neighbor}_lambda_{config.lambda_value}\")\n",
    "    for ext in ['png', 'pdf', 'svg']:\n",
    "        plt.savefig(f\"{base_path}.{ext}\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"\\nSaved heatmap for {strategy_display}: {base_path}.[png|pdf|svg]\")\n",
    "    plt.show()\n",
    "\n",
    "# === REPORTING ===\n",
    "def generate_summary_report(config, df, strategy='unknown'):\n",
    "    \"\"\"Generate a comprehensive summary report of the statistical analysis.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data available for reporting\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall performance summary\n",
    "    print(\"\\n=== PERFORMANCE SUMMARY vs RANDOM ===\")\n",
    "    summary_data = []\n",
    "    for method in df['Method'].unique():\n",
    "        method_df = df[df['Method'] == method]\n",
    "        better_count = sum(method_df['Performance'] == 'Better')\n",
    "        worse_count = sum(method_df['Performance'] == 'Worse')\n",
    "        no_diff_count = sum(method_df['Performance'] == 'No Difference')\n",
    "        total = len(method_df)\n",
    "        \n",
    "        # Calculate average effect size\n",
    "        avg_effect = method_df['Effect_Size'].mean()\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Method': method,\n",
    "            'Better': f\"{better_count}/{total} ({better_count/total*100:.1f}%)\",\n",
    "            'Worse': f\"{worse_count}/{total} ({worse_count/total*100:.1f}%)\",\n",
    "            'No_Diff': f\"{no_diff_count}/{total} ({no_diff_count/total*100:.1f}%)\",\n",
    "            'Avg_Effect_Size': f\"{avg_effect:.3f}\"\n",
    "        })\n",
    "    \n",
    "    print(tabulate(summary_data, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    # Best performers per metric\n",
    "    print(\"\\n=== BEST PERFORMERS PER METRIC ===\")\n",
    "    for metric in config.metrics:\n",
    "        metric_df = df[df['Metric'] == metric]\n",
    "        if not metric_df.empty:\n",
    "            # Find best performer\n",
    "            if config.metric_direction[metric]:  # Higher is better\n",
    "                best_row = metric_df.loc[metric_df['Method_Mean'].idxmax()]\n",
    "            else:  # Lower is better\n",
    "                best_row = metric_df.loc[metric_df['Method_Mean'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n{metric} ({config.metric_info[metric]['full_name']}):\")\n",
    "            print(f\"  Best: {best_row['Method']} (Mean: {best_row['Method_Mean']:.4f})\")\n",
    "            print(f\"  vs Random: {best_row['Percent_Change']:+.2f}% change, p={best_row['P_Value']:.4f}\")\n",
    "            print(f\"  Effect size: {best_row['Effect_Size']:.3f} ({best_row['Effect_Interpretation']})\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    output_path = os.path.join(config.stat_dir, \n",
    "                              f\"detailed_results_{strategy}_neighbor_{config.neighbor}_lambda_{config.lambda_value}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nDetailed results saved to: {output_path}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_path = os.path.join(config.stat_dir, \n",
    "                                f\"summary_{strategy}_neighbor_{config.neighbor}_lambda_{config.lambda_value}.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Summary saved to: {summary_path}\")\n",
    "\n",
    "def create_strategy_comparison_plot(all_results, config):\n",
    "    \"\"\"Create visualization comparing different data extraction strategies.\"\"\"\n",
    "    if len(all_results) < 2:\n",
    "        print(\"Need at least 2 strategies for comparison\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Strategy Comparison: Statistical Power Analysis\\nλ={config.lambda_value}, Neighbors={config.neighbor}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Sample sizes comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    strategies = list(all_results.keys())\n",
    "    sample_sizes = []\n",
    "    for strategy in strategies:\n",
    "        if len(all_results[strategy]) > 0:\n",
    "            sample_sizes.append(all_results[strategy]['Sample_Size'].mean())\n",
    "        else:\n",
    "            sample_sizes.append(0)\n",
    "    \n",
    "    bars1 = ax1.bar(strategies, sample_sizes, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Average Sample Size per Strategy')\n",
    "    ax1.set_ylabel('Sample Size (n)')\n",
    "    ax1.tick_params(axis='x', rotation=0)\n",
    "    for bar, size in zip(bars1, sample_sizes):\n",
    "        if size > 0:\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                    f'{size:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Significance rates comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    sig_rates = []\n",
    "    for strategy in strategies:\n",
    "        if len(all_results[strategy]) > 0:\n",
    "            sig_rate = all_results[strategy]['Is_Significant'].mean() * 100\n",
    "        else:\n",
    "            sig_rate = 0\n",
    "        sig_rates.append(sig_rate)\n",
    "    \n",
    "    bars2 = ax2.bar(strategies, sig_rates, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Statistical Significance Rate')\n",
    "    ax2.set_ylabel('% Tests Significant')\n",
    "    ax2.tick_params(axis='x', rotation=0)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    for bar, rate in zip(bars2, sig_rates):\n",
    "        if rate > 0:\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                    f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Effect sizes comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    effect_sizes = []\n",
    "    for strategy in strategies:\n",
    "        if len(all_results[strategy]) > 0:\n",
    "            avg_effect = all_results[strategy]['Effect_Size'].abs().mean()\n",
    "        else:\n",
    "            avg_effect = 0\n",
    "        effect_sizes.append(avg_effect)\n",
    "    \n",
    "    bars3 = ax3.bar(strategies, effect_sizes, color='lightgreen', alpha=0.7)\n",
    "    ax3.set_title('Average |Effect Size| (Cohen\\'s d)')\n",
    "    ax3.set_ylabel('|Cohen\\'s d|')\n",
    "    ax3.tick_params(axis='x', rotation=0)\n",
    "    for bar, effect in zip(bars3, effect_sizes):\n",
    "        if effect > 0:\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{effect:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Performance outcomes\n",
    "    ax4 = axes[1, 1]\n",
    "    performance_data = []\n",
    "    for strategy in strategies:\n",
    "        if len(all_results[strategy]) > 0:\n",
    "            df = all_results[strategy]\n",
    "            better = (df['Performance'] == 'Better').sum()\n",
    "            worse = (df['Performance'] == 'Worse').sum()\n",
    "            no_diff = (df['Performance'] == 'No Difference').sum()\n",
    "            total = len(df)\n",
    "            \n",
    "            performance_data.append({\n",
    "                'Strategy': strategy,\n",
    "                'Better': better/total*100 if total > 0 else 0,\n",
    "                'Worse': worse/total*100 if total > 0 else 0,\n",
    "                'No_Diff': no_diff/total*100 if total > 0 else 0\n",
    "            })\n",
    "        else:\n",
    "            performance_data.append({\n",
    "                'Strategy': strategy,\n",
    "                'Better': 0,\n",
    "                'Worse': 0,\n",
    "                'No_Diff': 0\n",
    "            })\n",
    "    \n",
    "    if performance_data:\n",
    "        perf_df = pd.DataFrame(performance_data)\n",
    "        x = np.arange(len(strategies))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax4.bar(x - width, perf_df['Better'], width, label='Better', color='green', alpha=0.7)\n",
    "        ax4.bar(x, perf_df['No_Diff'], width, label='No Difference', color='yellow', alpha=0.7)\n",
    "        ax4.bar(x + width, perf_df['Worse'], width, label='Worse', color='red', alpha=0.7)\n",
    "        \n",
    "        ax4.set_title('Performance Outcomes Distribution')\n",
    "        ax4.set_ylabel('% of Tests')\n",
    "        ax4.set_xlabel('Strategy')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(strategies)\n",
    "        ax4.legend()\n",
    "        ax4.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the comparison plot\n",
    "    comparison_path = os.path.join(config.stat_dir, \n",
    "                                  f\"strategy_comparison_plot_neighbor_{config.neighbor}_lambda_{config.lambda_value}\")\n",
    "    for ext in ['png', 'pdf']:\n",
    "        plt.savefig(f\"{comparison_path}.{ext}\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"Strategy comparison plot saved: {comparison_path}.[png|pdf]\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function with enhanced statistical rigor and full plotting.\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"COMPREHENSIVE STATISTICAL ANALYSIS FOR EXAL - ENHANCED VERSION WITH PLOTTING\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Lambda: {config.lambda_value}\")\n",
    "    print(f\"  - Neighbors: {config.neighbor}\")\n",
    "    print(f\"  - Dataset: {config.dataset}\")\n",
    "    print(f\"  - Reference Method: {config.reference_method}\")\n",
    "    print(f\"  - Significance Level: {config.alpha}\")\n",
    "    print(f\"  - Test Type: {'Two-sided' if config.use_two_sided else 'One-sided'}\")\n",
    "    \n",
    "    # Check configuration\n",
    "    if config.lambda_value == \"LAMDA\" or config.neighbor == \"NEIGHBOR\" or config.dataset == \"dataset\":\n",
    "        print(\"\\nWARNING: Please set actual values for lambda_value, neighbor, and dataset in the Config class!\")\n",
    "        print(\"Current values are placeholders and will cause file loading to fail.\")\n",
    "        return\n",
    "    \n",
    "    # CHOOSE DATA EXTRACTION STRATEGY:\n",
    "    strategies_to_compare = [\n",
    "        ('final_only', 'Traditional: Final iteration only', {}),\n",
    "        ('all_iterations', 'Enhanced: All iterations for maximum power', {}),\n",
    "        ('convergence_period', 'Stable: Iterations 5+ (convergence period)', {'min_iteration': 5})\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for strategy, description, kwargs in strategies_to_compare:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ANALYSIS WITH STRATEGY: {strategy.upper()}\")\n",
    "        print(f\"Description: {description}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Load data with current strategy\n",
    "            method_results, method_metadata = load_data(config, strategy=strategy, **kwargs)\n",
    "            \n",
    "            # Check if we have sufficient data\n",
    "            total_samples = sum(len(method_results[m]['MAE']) for m in config.all_methods)\n",
    "            if total_samples == 0:\n",
    "                print(f\"Error: No data loaded for strategy {strategy}. Check file paths and configuration.\")\n",
    "                continue\n",
    "            \n",
    "            # Perform statistical analysis\n",
    "            results_df = perform_comprehensive_statistical_tests(config, method_results)\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                # Store results\n",
    "                all_results[strategy] = results_df\n",
    "                \n",
    "                # Create visualizations for this strategy\n",
    "                print(f\"\\n=== CREATING VISUALIZATIONS FOR {strategy.upper()} ===\")\n",
    "                create_clean_heatmap(config, results_df, strategy)\n",
    "                \n",
    "                # Generate detailed report\n",
    "                generate_summary_report(config, results_df, strategy)\n",
    "                \n",
    "                # Save strategy-specific results\n",
    "                output_path = os.path.join(config.stat_dir, \n",
    "                                          f\"analysis_{strategy}_neighbor_{config.neighbor}_lambda_{config.lambda_value}.csv\")\n",
    "                results_df.to_csv(output_path, index=False)\n",
    "                print(f\"Results saved to: {output_path}\")\n",
    "                \n",
    "                # Quick summary for this strategy\n",
    "                print(f\"\\n=== QUICK SUMMARY FOR {strategy.upper()} ===\")\n",
    "                significant_count = results_df['Is_Significant'].sum()\n",
    "                total_tests = len(results_df)\n",
    "                print(f\"Significant results: {significant_count}/{total_tests} ({significant_count/total_tests*100:.1f}%)\")\n",
    "                \n",
    "                # Show sample sizes achieved\n",
    "                sample_sizes = sorted(results_df['Sample_Size'].unique())\n",
    "                print(f\"Sample sizes achieved: {sample_sizes}\")\n",
    "                \n",
    "                # Show effect sizes for key comparisons\n",
    "                exal_methods = results_df[results_df['Method'].str.contains('EXAL')]\n",
    "                if not exal_methods.empty:\n",
    "                    avg_effect_size = exal_methods['Effect_Size'].abs().mean()\n",
    "                    print(f\"Average |effect size| for EXAL methods: {avg_effect_size:.3f}\")\n",
    "            else:\n",
    "                print(f\"No statistical results generated for strategy {strategy}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing strategy {strategy}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # COMPARISON OF STRATEGIES\n",
    "    if len(all_results) > 1:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"STRATEGY COMPARISON SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create strategy comparison visualization\n",
    "        create_strategy_comparison_plot(all_results, config)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for strategy, results_df in all_results.items():\n",
    "            significant_count = results_df['Is_Significant'].sum()\n",
    "            total_tests = len(results_df)\n",
    "            avg_sample_size = results_df['Sample_Size'].mean()\n",
    "            avg_effect_size = results_df['Effect_Size'].abs().mean()\n",
    "            \n",
    "            # Count Better vs Worse performance\n",
    "            better_count = (results_df['Performance'] == 'Better').sum()\n",
    "            worse_count = (results_df['Performance'] == 'Worse').sum()\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Strategy': strategy,\n",
    "                'Avg_Sample_Size': f\"{avg_sample_size:.1f}\",\n",
    "                'Significant_Tests': f\"{significant_count}/{total_tests}\",\n",
    "                'Sig_Percentage': f\"{significant_count/total_tests*100:.1f}%\",\n",
    "                'Better_Performance': f\"{better_count}\",\n",
    "                'Worse_Performance': f\"{worse_count}\",\n",
    "                'Avg_Effect_Size': f\"{avg_effect_size:.3f}\"\n",
    "            })  \n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(tabulate(comparison_df, headers='keys', tablefmt='grid'))\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_path = os.path.join(config.stat_dir, \n",
    "                                      f\"strategy_comparison_neighbor_{config.neighbor}_lambda_{config.lambda_value}.csv\")\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        print(f\"\\nStrategy comparison saved to: {comparison_path}\")\n",
    "    \n",
    "    elif len(all_results) == 1:\n",
    "        print(f\"\\nOnly one strategy completed successfully: {list(all_results.keys())[0]}\")\n",
    "    else:\n",
    "        print(f\"\\nNo strategies completed successfully. Check your configuration and data files.\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"COMPREHENSIVE STATISTICAL ANALYSIS WITH FULL PLOTTING COMPLETE!\")\n",
    "    print(f\"All results, visualizations, and reports saved in: {config.stat_dir}/\")\n",
    "    print(f\"{'='*100}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
